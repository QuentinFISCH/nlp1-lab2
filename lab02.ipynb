{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP LAB 02\n",
    "Théo Ripoll - Quentin Fish - Nicolas Fidel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import get_dataset_split_names\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/Users/quentinfisch/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f45dfb7647744f3adfe792a0631e738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dataset_split_names(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    12500\n",
      "1    12500\n",
      "Name: label, dtype: int64\n",
      "label\n",
      "0    12500\n",
      "1    12500\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Count the number of different labels in each datasets\n",
    "train_labels = pd.DataFrame(dataset[\"train\"]['label'], columns=[\"label\"])\n",
    "print(train_labels.groupby(\"label\")[\"label\"].count())\n",
    "\n",
    "test_labels = pd.DataFrame(dataset[\"test\"]['label'], columns=[\"label\"])\n",
    "print(test_labels.groupby(\"label\")[\"label\"].count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: How many splits does the dataset has?\n",
    "There are 3 splits: train, test and unsupervised\n",
    "\n",
    "### Question 2: How big are the splits ?\n",
    "train: 25000\n",
    "test: 25000\n",
    "unsupervised: 50000\n",
    "\n",
    "### Question 3: What is the proportion of each class on the supervised splits?\n",
    "train: 50% positive, 50% negative\n",
    "test: 50% positive, 50% negative"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2: Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "# Prepare the dataset\n",
    "\n",
    "def preprocess(dataset: pd.DataFrame) -> pd.DataFrame : \n",
    "    # First lower the case\n",
    "    dataset[\"document\"] = dataset[\"document\"].apply(lambda x: x.lower())\n",
    "    # Replace the punctuation with spaces. We keep the . ? ! - that may give revelant informations\n",
    "    # Replace HTML balise <br />\n",
    "    punctuation_to_remove = '|'.join(map(re.escape, sorted(list(filter(lambda p: p != '.' and p != '!' and p != '?' and p != '-', punctuation)), reverse=True)))\n",
    "    print(f\"Deleting all these punctuation: {punctuation_to_remove}\")\n",
    "    dataset[\"document\"] = dataset[\"document\"].apply(lambda x: re.sub(punctuation_to_remove, \" \", x.replace('<br />', \"\")))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all these punctuation: \\~|\\}|\\||\\{|`|_|\\^|\\]|\\\\|\\[|@|>|=|<|;|:|/|,|\\+|\\*|\\)|\\(|'|\\&|%|\\$|\\#|\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i rented i am curious-yellow from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i am curious  yellow  is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>if only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this film was probably inspired by godard s ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oh  brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>a hit at the time but now better categorised a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>i love this movie like no other. another time ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>this film and it s sequel barry mckenzie holds...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>the adventures of barry mckenzie  started lif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>the story centers around barry mckenzie who mu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document  class\n",
       "0      i rented i am curious-yellow from my video sto...      0\n",
       "1       i am curious  yellow  is a risible and preten...      0\n",
       "2      if only to avoid making this type of film in t...      0\n",
       "3      this film was probably inspired by godard s ma...      0\n",
       "4      oh  brother...after hearing about this ridicul...      0\n",
       "...                                                  ...    ...\n",
       "24995  a hit at the time but now better categorised a...      1\n",
       "24996  i love this movie like no other. another time ...      1\n",
       "24997  this film and it s sequel barry mckenzie holds...      1\n",
       "24998   the adventures of barry mckenzie  started lif...      1\n",
       "24999  the story centers around barry mckenzie who mu...      1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw = pd.DataFrame(dataset[\"train\"], columns=[\"text\", \"label\"]).rename(columns={\"text\": \"document\", \"label\": \"class\"})\n",
    "preprocessed_train = preprocess(train_raw)\n",
    "preprocessed_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all these punctuation: \\~|\\}|\\||\\{|`|_|\\^|\\]|\\\\|\\[|@|>|=|<|;|:|/|,|\\+|\\*|\\)|\\(|'|\\&|%|\\$|\\#|\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i love sci-fi and am willing to put up with a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worth the entertainment value of a rental  esp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>its a totally average film with a few semi-alr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>star rating        saturday night      friday ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>first off let me say  if you haven t enjoyed a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>just got around to seeing monster man yesterda...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>i got this as part of a competition prize. i w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>i got monster man in a box set of three films ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>five minutes in  i started to feel how naff th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>i caught this movie on the sci-fi channel rece...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document  class\n",
       "0      i love sci-fi and am willing to put up with a ...      0\n",
       "1      worth the entertainment value of a rental  esp...      0\n",
       "2      its a totally average film with a few semi-alr...      0\n",
       "3      star rating        saturday night      friday ...      0\n",
       "4      first off let me say  if you haven t enjoyed a...      0\n",
       "...                                                  ...    ...\n",
       "24995  just got around to seeing monster man yesterda...      1\n",
       "24996  i got this as part of a competition prize. i w...      1\n",
       "24997  i got monster man in a box set of three films ...      1\n",
       "24998  five minutes in  i started to feel how naff th...      1\n",
       "24999  i caught this movie on the sci-fi channel rece...      1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_raw = pd.DataFrame(dataset[\"test\"], columns=[\"text\", \"label\"]).rename(columns={\"text\": \"document\", \"label\": \"class\"})\n",
    "preprocessed_test = preprocess(test_raw)\n",
    "preprocessed_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Naive Bayes Classifier using pseudo-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_vocabulary(d: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Return the vocabulary of the dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d : pd.DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        The vocabulary\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(d[\"document\"])\n",
    "    return vectorizer.vocabulary_\n",
    "\n",
    "def train_naive_bayes(d: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Train a Naive Bayes classifier\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d : pd.DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    logprior : dict\n",
    "        The log prior of each class\n",
    "    loglikelihood : dict\n",
    "        The log likelihood of each word for each class\n",
    "    V : List[str]\n",
    "        The vocabulary\n",
    "    \"\"\"\n",
    "    classes = d[\"class\"].unique()\n",
    "    logprior = {}\n",
    "    bigdoc = {}\n",
    "    count = {}\n",
    "    loglikelihood = {}\n",
    "    V = get_vocabulary(d)\n",
    "    for c in classes:\n",
    "        n_doc = len(d)\n",
    "        n_c = len(d[d[\"class\"] == c])\n",
    "        logprior[c] = np.log(n_c / n_doc)\n",
    "        bigdoc[c] = \" \".join(d[d[\"class\"] == c][\"document\"])\n",
    "        for word in V:\n",
    "            count[(word, c)] = bigdoc[c].count(word)\n",
    "            loglikelihood[(word, c)] = np.log((count[(word, c)] + 1) / (sum(count.values()) + len(V)))\n",
    "    return logprior, loglikelihood, V\n",
    "\n",
    "def test_naive_bayes(testdoc, classes, logprior, loglikelihood, V) -> int:\n",
    "    \"\"\"\n",
    "    Test a Naive Bayes classifier\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    testdoc : str\n",
    "        The document to classify\n",
    "    classes : List[int]\n",
    "        The list of classes\n",
    "    logprior : dict\n",
    "        The log prior of each class\n",
    "    loglikelihood : dict\n",
    "        The log likelihood of each word for each class\n",
    "    V : List[str]\n",
    "        The vocabulary\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The predicted class\n",
    "    \"\"\"\n",
    "    sum_loglikelihood = {}\n",
    "    for c in classes:\n",
    "        sum_loglikelihood[c] = logprior[c]\n",
    "        for word in testdoc.split(\" \"):\n",
    "            if word in V:\n",
    "                sum_loglikelihood[c] += loglikelihood[(word, c)]\n",
    "    return max(sum_loglikelihood, key=sum_loglikelihood.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Naive Bayes Accuracy Score ->  50.0\n",
      "Manual Naive Bayes Precision Score ->  0.0\n",
      "Manual Naive Bayes Recall Score ->  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/nlp1-lab2/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "train_dataset_reduced = preprocessed_train.loc[::100, :]\n",
    "test_dataset_reduced = preprocessed_test.loc[::100, :]\n",
    "logprior_r, loglikelyhood_r, V_r = train_naive_bayes(train_dataset_reduced)\n",
    "\n",
    "all_res = []\n",
    "for row in test_dataset_reduced.iterrows():\n",
    "    test_doc = row[1][\"document\"]\n",
    "    res = test_naive_bayes(test_doc, preprocessed_test[\"class\"].unique(), logprior_r, loglikelyhood_r, V_r)\n",
    "    all_res.append(res)\n",
    "\n",
    "print(\"Manual Naive Bayes Accuracy Score -> \",accuracy_score(test_dataset_reduced[\"class\"], all_res)*100)\n",
    "print(\"Manual Naive Bayes Precision Score -> \",precision_score(test_dataset_reduced[\"class\"], all_res)*100)\n",
    "print(\"Manual Naive Bayes Recall Score -> \",recall_score(test_dataset_reduced[\"class\"], all_res)*100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Naive Bayes Classifier using sklearn (Pipeline with CountVectorizer and MultinomialNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_naive_bayes(d_train: pd.DataFrame, pipeline_params: dict = {}) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Train a Naive Bayes classifier using sklearn\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d_train : pd.DataFrame\n",
    "        The training dataset\n",
    "    pipeline_params : dict, optional\n",
    "        The parameters of the pipeline, by default {}\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pipeline\n",
    "        The trained pipeline\n",
    "    \"\"\"\n",
    "    # create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', CountVectorizer()),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "    pipeline.set_params(**pipeline_params)\n",
    "\n",
    "    # train the model\n",
    "    pipeline.fit(d_train[\"document\"], d_train[\"class\"])\n",
    "    return pipeline\n",
    "\n",
    "def test_sklearn_naive_bayes(pipeline: Pipeline, d_test: pd.DataFrame) -> List[int]:\n",
    "    \"\"\"\n",
    "    Test a Naive Bayes classifier using sklearn\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pipeline : Pipeline\n",
    "        The trained pipeline\n",
    "    d_test : pd.DataFrame\n",
    "        The test dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[int]\n",
    "        The predicted classes\n",
    "    \"\"\"\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = pipeline.predict(d_test[\"document\"])\n",
    "\n",
    "    print(\"Sklearn Naive Bayes Accuracy Score -> \",accuracy_score(d_test[\"class\"], predictions)*100)\n",
    "    print(\"Sklearn Naive Bayes Precision Score -> \",precision_score(d_test[\"class\"], predictions)*100)\n",
    "    print(\"Sklearn Naive Bayes Recall Score -> \",recall_score(d_test[\"class\"], predictions)*100)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Naive Bayes Accuracy Score ->  81.44\n",
      "Sklearn Naive Bayes Precision Score ->  86.05504587155963\n",
      "Sklearn Naive Bayes Recall Score ->  75.03999999999999\n"
     ]
    }
   ],
   "source": [
    "pipeline = sklearn_naive_bayes(preprocessed_train)\n",
    "predictions = test_sklearn_naive_bayes(pipeline, preprocessed_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Report the accuracy on the test set\n",
    "\n",
    "See prints above\n",
    "\n",
    "### Question 5: Most likely, the scikit-learn implementation will give better results. Looking at the documentation, explain why it could be the case.\n",
    "\n",
    "The scikit-learn implementation is better because it uses a MultinomialNB which is a more efficient way to compute the probabilities.\n",
    "\n",
    "### Question 6: Why is accuracy a sufficient measure of evaluation here?\n",
    "\n",
    "Because the dataset is balanced, we have the same number of positive and negative reviews. So the accuracy is a good measure of evaluation.\n",
    "\n",
    "### Question 7: Using one of the implementation, take at least 2 wrongly classified example from the test set and try explaining why the model failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blind date  columbia pictures  1934   was a decent film  but i have a few issues with this film. first of all  i don t fault the actors in this film at all  but more or less  i have a problem with the script. also  i understand that this film was made in the 1930 s and people were looking to escape reality  but the script made ann sothern s character look weak. she kept going back and forth between suitors and i felt as though she should have stayed with paul kelly s character in the end. he truly did care about her and her family and would have done anything for her and he did by giving her up in the end to fickle neil hamilton who in my opinion was only out for a good time. paul kelly s character  although a workaholic was a man of integrity and truly loved kitty  ann sothern  as opposed to neil hamilton  while he did like her a lot  i didn t see the depth of love that he had for her character. the production values were great  but the script could have used a little work.\n",
      "ben   rupert grint   is a deeply unhappy adolescent  the son of his unhappily married parents. his father   nicholas farrell   is a vicar and his mother   laura linney   is ... well  let s just say she s a somewhat hypocritical soldier in jesus  army. it s only when he takes a summer job as an assistant to a foul-mouthed  eccentric  once-famous and now-forgotten actress evie walton   julie walters   that he finally finds himself in true  harold and maude  fashion. of course  evie is deeply unhappy herself and it s only when these two sad sacks find each other that they can put their mutual misery aside and hit the road to happiness.of course it s corny and sentimental and very predictable but it has a hard side to it  too and walters  who could sleep-walk her way through this sort of thing if she wanted  is excellent. it s when she puts the craziness to one side and finds the pathos in the character   like hitting the bottle and throwing up in the sink   that she s at her best. the problem is she s the only interesting character in the film  and it s not because of the script which doesn t do anybody any favours . grint  on the other hand  isn t just unhappy  he s a bit of a bore as well while linney s starched bitch is completely one-dimensional.  still  she s got the english accent off pat . the best that can be said for it is that it s mildly enjoyable - with the emphasis on the mildly.\n",
      "\n",
      "[[4.22158007e-06 9.99995778e-01]]\n",
      "[[0.00150068 0.99849932]]\n"
     ]
    }
   ],
   "source": [
    "# We will take a look at the sklearn implementation\n",
    "# First we need to get the wrongly classified examples\n",
    "wrongly_classified = preprocessed_test[preprocessed_test[\"class\"] != predictions]\n",
    "\n",
    "# We will take the first 2 examples\n",
    "# We can see that the first example is a negative review but the model predicted it as a positive review\n",
    "# The second example is a positive review but the model predicted it as a negative review\n",
    "print(wrongly_classified.iloc[0][\"document\"])\n",
    "print(wrongly_classified.iloc[1][\"document\"])\n",
    "print()\n",
    "\n",
    "# Let's see the probability of each class for the first example\n",
    "print(pipeline.predict_proba([wrongly_classified.iloc[0][\"document\"]]))\n",
    "# Let's see the probability of each class for the second example\n",
    "print(pipeline.predict_proba([wrongly_classified.iloc[1][\"document\"]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model is very confident about its prediction for the two examples (0.99...) but it's wrong. These examples are very hard to classify because they are very close to the decision boundary and also mixing a movie description (which can have positive or negative connotations due to the life of the main character, etc) and a review. So the model is not able to classify them correctly because of the confusing bundary between description and facts and the opinion."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8: What are the top 10 most important words (features) for each class? (bonus points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the sklearn implementation to get the top 10 most important words for each class\n",
    "\n",
    "def get_top_10_words(pipeline: Pipeline) -> dict:\n",
    "    \"\"\"\n",
    "    Get the top 10 words for each class\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pipeline : Pipeline\n",
    "        The trained pipeline\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The top 10 words for each class\n",
    "    \"\"\"\n",
    "    top_10_words = {}\n",
    "    for c in preprocessed_test[\"class\"].unique():\n",
    "        loglikelihood = pipeline.named_steps[\"classifier\"].feature_log_prob_[c]\n",
    "        V = pipeline.named_steps[\"vectorizer\"].vocabulary_\n",
    "        top_10_words[c] = [list(V.keys())[list(V.values()).index(i)] for i in np.argsort(loglikelihood)[-10:]]\n",
    "    return top_10_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['was', 'that', 'this', 'in', 'it', 'is', 'to', 'of', 'and', 'the'],\n",
       " 1: ['as', 'this', 'that', 'it', 'in', 'is', 'to', 'of', 'and', 'the']}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_10_words(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Naive Bayes Accuracy Score ->  81.976\n",
      "Sklearn Naive Bayes Precision Score ->  86.22439731738264\n",
      "Sklearn Naive Bayes Recall Score ->  76.112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: ['story',\n",
       "  'don',\n",
       "  'time',\n",
       "  'really',\n",
       "  'bad',\n",
       "  'good',\n",
       "  'just',\n",
       "  'like',\n",
       "  'film',\n",
       "  'movie'],\n",
       " 1: ['people',\n",
       "  'really',\n",
       "  'great',\n",
       "  'time',\n",
       "  'story',\n",
       "  'just',\n",
       "  'good',\n",
       "  'like',\n",
       "  'movie',\n",
       "  'film']}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_stopwords = sklearn_naive_bayes(preprocessed_train, {\"vectorizer__stop_words\": \"english\"})\n",
    "predictions_stopwords = test_sklearn_naive_bayes(pipeline_stopwords, preprocessed_test)\n",
    "\n",
    "get_top_10_words(pipeline_stopwords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the top 10 words are more unique using stopwords\n",
    "\n",
    "### Question 9: Play with scikit-learn's version parameters. For example, see if you can consider unigram and bigram instead of only unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Naive Bayes Accuracy Score ->  84.244\n",
      "Sklearn Naive Bayes Precision Score ->  87.4857693318154\n",
      "Sklearn Naive Bayes Recall Score ->  79.92\n"
     ]
    }
   ],
   "source": [
    "# Unigram and bigram\n",
    "pipeline_bigram = sklearn_naive_bayes(preprocessed_train, {\"vectorizer__ngram_range\": (1, 2), \"vectorizer__stop_words\": \"english\"})\n",
    "predictions_bigram = test_sklearn_naive_bayes(pipeline_bigram, preprocessed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Naive Bayes Accuracy Score ->  85.672\n",
      "Sklearn Naive Bayes Precision Score ->  88.62612612612612\n",
      "Sklearn Naive Bayes Recall Score ->  81.848\n"
     ]
    }
   ],
   "source": [
    "# Unigram and bigram with stopwords\n",
    "pipeline_bigram_stopwords = sklearn_naive_bayes(preprocessed_train, {\"vectorizer__ngram_range\": (1, 2)})\n",
    "predictions_bigram_stopwords = test_sklearn_naive_bayes(pipeline_bigram_stopwords, preprocessed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Naive Bayes Accuracy Score ->  82.952\n",
      "Sklearn Naive Bayes Precision Score ->  87.63018454229857\n",
      "Sklearn Naive Bayes Recall Score ->  76.736\n"
     ]
    }
   ],
   "source": [
    "# Only bigram\n",
    "pipeline_only_bigram = sklearn_naive_bayes(preprocessed_train, {\"vectorizer__ngram_range\": (2, 2), \"vectorizer__stop_words\": \"english\"})\n",
    "predictions_only_bigram = test_sklearn_naive_bayes(pipeline_only_bigram, preprocessed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Naive Bayes Accuracy Score ->  86.952\n",
      "Sklearn Naive Bayes Precision Score ->  89.35753237900477\n",
      "Sklearn Naive Bayes Recall Score ->  83.896\n"
     ]
    }
   ],
   "source": [
    "# Only bigram with stopwords\n",
    "pipeline_only_bigram_stopwords = sklearn_naive_bayes(preprocessed_train, {\"vectorizer__ngram_range\": (2, 2)})\n",
    "predictions_only_bigram_stopwords = test_sklearn_naive_bayes(pipeline_only_bigram_stopwords, preprocessed_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is better with only unigrams and without removing stopwords."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import spacy\n",
    "from typing import List\n",
    "\n",
    "\n",
    "texts = [\n",
    "    \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n",
    "    \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n",
    "]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#for doc in nlp.pipe(texts, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
    "    # Do something with the doc here\n",
    " #    print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "\n",
    "pipeline = [\"lemmatizer\"]\n",
    "disable = ['tok2vec', 'tagger', 'parser', 'senter', 'attribute_ruler', 'ner']\n",
    "\n",
    "print(nlp.pipe_names)\n",
    "for doc in nlp.pipe(texts, disable=disable ,n_process=4):\n",
    "    print(doc.ents[\"parser\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}