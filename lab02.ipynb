{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["! pip install scikit-learn"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","corpus = [\n","    'This is the first document.',\n","    'This document is the second document.',\n","    'And this is the third one.',\n","    'Is this the first document?',\n","]\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(corpus)\n","X"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Part 3: Stemming\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install spacy"]},{"cell_type":"code","execution_count":133,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"]},{"name":"stderr","output_type":"stream","text":["/Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/nlp1-lab2/.venv/lib/python3.9/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n","  warnings.warn(Warnings.W108)\n"]},{"ename":"TypeError","evalue":"tuple indices must be integers or slices, not str","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[133], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(nlp\u001b[39m.\u001b[39mpipe_names)\n\u001b[1;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m nlp\u001b[39m.\u001b[39mpipe(texts, disable\u001b[39m=\u001b[39mdisable ,n_process\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m):\n\u001b[0;32m---> 22\u001b[0m     \u001b[39mprint\u001b[39m(doc\u001b[39m.\u001b[39;49ments[\u001b[39m\"\u001b[39;49m\u001b[39mparser\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n","\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"]}],"source":["#importing libraries\n","import spacy\n","from typing import List\n","\n","\n","texts = [\n","    \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n","    \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n","]\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","#for doc in nlp.pipe(texts, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n","    # Do something with the doc here\n"," #    print([(ent.text, ent.label_) for ent in doc.ents])\n","\n","\n","pipeline = [\"lemmatizer\"]\n","disable = ['tok2vec', 'tagger', 'parser', 'senter', 'attribute_ruler', 'ner']\n","\n","print(nlp.pipe_names)\n","for doc in nlp.pipe(texts, disable=disable ,n_process=4):\n","    print(doc.ents[\"parser\"])"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting en-core-web-sm==3.5.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in ./.venv/lib/python3.9/site-packages (from en-core-web-sm==3.5.0) (3.5.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in ./.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n","Requirement already satisfied: numpy>=1.15.0 in ./.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.2)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in ./.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n","Requirement already satisfied: pathy>=0.10.0 in ./.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in ./.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.6)\n","Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n","Requirement already satisfied: setuptools in ./.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (58.0.4)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n","Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n","Requirement already satisfied: typing-extensions>=4.2.0 in ./.venv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./.venv/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in ./.venv/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n","Installing collected packages: en-core-web-sm\n","Successfully installed en-core-web-sm-3.5.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}],"source":["! python -m spacy download en_core_web_sm"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["NLP LAB 02\n","Théo Ripoll - Quentin Fish - Nicolas"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Loading the dataset"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Part 1: The Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from datasets import load_dataset\n","from datasets import get_dataset_split_names\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset = load_dataset(\"imdb\")\n","dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_dataset_split_names(\"imdb\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Count the number of different labels in each datasets\n","train_labels = pd.DataFrame(dataset[\"train\"]['label'], columns=[\"label\"])\n","print(train_labels.groupby(\"label\")[\"label\"].count())\n","\n","test_labels = pd.DataFrame(dataset[\"test\"]['label'], columns=[\"label\"])\n","print(test_labels.groupby(\"label\")[\"label\"].count())"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Question 1: How many splits does the dataset has?\n","There are 3 splits: train, test and unsupervised\n","\n","### Question 2: How big are the splits ?\n","train: 25000\n","test: 25000\n","unsupervised: 50000\n","\n","### Question 3: What is the proportion of each class on the supervised splits?\n","train: 50% positive, 50% negative\n","test: 50% positive, 50% negative"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Partie 2: Naive Bayes classifier"]},{"cell_type":"code","execution_count":127,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\"|#|$|%|&|'|(|)|*|+|,|-|/|:|;|<|=|>|@|[|\\|]|^|_|`|{|||}|~\n"]},{"ename":"error","evalue":"nothing to repeat at position 16","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","Cell \u001b[0;32mIn[127], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     dataset[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: re\u001b[39m.\u001b[39msub(punctuation_to_remove, \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m, x))\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m dataset\n\u001b[0;32m---> 14\u001b[0m preprocessed \u001b[39m=\u001b[39m preprocess(pd\u001b[39m.\u001b[39;49mDataFrame(dataset[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m], columns\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39;49mloc[:\u001b[39m1\u001b[39;49m, :],)\n\u001b[1;32m     15\u001b[0m preprocessed\n","Cell \u001b[0;32mIn[127], line 11\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      9\u001b[0m punctuation_to_remove \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mlist\u001b[39m(\u001b[39mfilter\u001b[39m(\u001b[39mlambda\u001b[39;00m p: p \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m p \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m!\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m p \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m?\u001b[39m\u001b[39m'\u001b[39m,punctuation)))\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(punctuation_to_remove)\n\u001b[0;32m---> 11\u001b[0m dataset[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: re\u001b[39m.\u001b[39;49msub(punctuation_to_remove, \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m, x))\n\u001b[1;32m     12\u001b[0m \u001b[39mreturn\u001b[39;00m dataset\n","File \u001b[0;32m~/Documents/EPITA/ING2/SCIA/S8/NLP1/nlp1-lab2/.venv/lib/python3.9/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n","File \u001b[0;32m~/Documents/EPITA/ING2/SCIA/S8/NLP1/nlp1-lab2/.venv/lib/python3.9/site-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n","File \u001b[0;32m~/Documents/EPITA/ING2/SCIA/S8/NLP1/nlp1-lab2/.venv/lib/python3.9/site-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1175\u001b[0m             values,\n\u001b[1;32m   1176\u001b[0m             f,\n\u001b[1;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1178\u001b[0m         )\n\u001b[1;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n","File \u001b[0;32m~/Documents/EPITA/ING2/SCIA/S8/NLP1/nlp1-lab2/.venv/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","Cell \u001b[0;32mIn[127], line 11\u001b[0m, in \u001b[0;36mpreprocess.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      9\u001b[0m punctuation_to_remove \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mlist\u001b[39m(\u001b[39mfilter\u001b[39m(\u001b[39mlambda\u001b[39;00m p: p \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m p \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m!\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m p \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m?\u001b[39m\u001b[39m'\u001b[39m,punctuation)))\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(punctuation_to_remove)\n\u001b[0;32m---> 11\u001b[0m dataset[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: re\u001b[39m.\u001b[39;49msub(punctuation_to_remove, \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m, x))\n\u001b[1;32m     12\u001b[0m \u001b[39mreturn\u001b[39;00m dataset\n","File \u001b[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/re.py:210\u001b[0m, in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msub\u001b[39m(pattern, repl, string, count\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, flags\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m    204\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[39m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\u001b[39m.\u001b[39msub(repl, string, count)\n","File \u001b[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/re.py:304\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sre_compile\u001b[39m.\u001b[39misstring(pattern):\n\u001b[1;32m    303\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mfirst argument must be string or compiled pattern\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 304\u001b[0m p \u001b[39m=\u001b[39m sre_compile\u001b[39m.\u001b[39;49mcompile(pattern, flags)\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (flags \u001b[39m&\u001b[39m DEBUG):\n\u001b[1;32m    306\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(_cache) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m _MAXCACHE:\n\u001b[1;32m    307\u001b[0m         \u001b[39m# Drop the oldest item\u001b[39;00m\n","File \u001b[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/sre_compile.py:764\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m isstring(p):\n\u001b[1;32m    763\u001b[0m     pattern \u001b[39m=\u001b[39m p\n\u001b[0;32m--> 764\u001b[0m     p \u001b[39m=\u001b[39m sre_parse\u001b[39m.\u001b[39;49mparse(p, flags)\n\u001b[1;32m    765\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    766\u001b[0m     pattern \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/sre_parse.py:948\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(str, flags, state)\u001b[0m\n\u001b[1;32m    945\u001b[0m state\u001b[39m.\u001b[39mstr \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m\n\u001b[1;32m    947\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 948\u001b[0m     p \u001b[39m=\u001b[39m _parse_sub(source, state, flags \u001b[39m&\u001b[39;49m SRE_FLAG_VERBOSE, \u001b[39m0\u001b[39;49m)\n\u001b[1;32m    949\u001b[0m \u001b[39mexcept\u001b[39;00m Verbose:\n\u001b[1;32m    950\u001b[0m     \u001b[39m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[39;00m\n\u001b[1;32m    951\u001b[0m     \u001b[39m# on the safe side, we'll parse the whole thing again...\u001b[39;00m\n\u001b[1;32m    952\u001b[0m     state \u001b[39m=\u001b[39m State()\n","File \u001b[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/sre_parse.py:443\u001b[0m, in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    441\u001b[0m start \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39mtell()\n\u001b[1;32m    442\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     itemsappend(_parse(source, state, verbose, nested \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m    444\u001b[0m                        \u001b[39mnot\u001b[39;49;00m nested \u001b[39mand\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m items))\n\u001b[1;32m    445\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sourcematch(\u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    446\u001b[0m         \u001b[39mbreak\u001b[39;00m\n","File \u001b[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/sre_parse.py:668\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    666\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m item \u001b[39mor\u001b[39;00m item[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m AT:\n\u001b[0;32m--> 668\u001b[0m     \u001b[39mraise\u001b[39;00m source\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mnothing to repeat\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    669\u001b[0m                        source\u001b[39m.\u001b[39mtell() \u001b[39m-\u001b[39m here \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(this))\n\u001b[1;32m    670\u001b[0m \u001b[39mif\u001b[39;00m item[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39min\u001b[39;00m _REPEATCODES:\n\u001b[1;32m    671\u001b[0m     \u001b[39mraise\u001b[39;00m source\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mmultiple repeat\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    672\u001b[0m                        source\u001b[39m.\u001b[39mtell() \u001b[39m-\u001b[39m here \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(this))\n","\u001b[0;31merror\u001b[0m: nothing to repeat at position 16"]}],"source":["from string import punctuation\n","import re\n","# Prepare the dataset\n","\n","def preprocess(dataset: pd.DataFrame) -> pd.DataFrame : \n","    # First lower the case\n","    dataset[\"text\"] = dataset[\"text\"].apply(lambda x: x.lower())\n","    # Replace the punctuation with spaces. We keep the . ? ! that may give revelant informations\n","    punctuation_to_remove = \"|\".join(list(filter(lambda p: p != '.' and p != '!' and p != '?',punctuation)))\n","    print(punctuation_to_remove)\n","    dataset[\"text\"] = dataset[\"text\"].apply(lambda x: re.sub(punctuation_to_remove, \" \", x))\n","    return dataset\n","\n","preprocessed = preprocess(pd.DataFrame(dataset[\"train\"], columns=[\"text\", \"label\"]).loc[:1, :],)\n","preprocessed"]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[{"data":{"text/plain":["'\"|#|$|%|&|\\'|(|)|*|+|,|-|/|:|;|<|=|>|@|[|\\\\|]|^|_|`|{|||}|~'"]},"execution_count":98,"metadata":{},"output_type":"execute_result"}],"source":["from string import punctuation\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Classifier using pseudo-code"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>document</th>\n","      <th>class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>If only to avoid making this type of film in t...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>This film was probably inspired by Godard's Ma...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Oh, brother...after hearing about this ridicul...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>24995</th>\n","      <td>A hit at the time but now better categorised a...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24996</th>\n","      <td>I love this movie like no other. Another time ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24997</th>\n","      <td>This film and it's sequel Barry Mckenzie holds...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24998</th>\n","      <td>'The Adventures Of Barry McKenzie' started lif...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24999</th>\n","      <td>The story centers around Barry McKenzie who mu...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>25000 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                document  class\n","0      I rented I AM CURIOUS-YELLOW from my video sto...      0\n","1      \"I Am Curious: Yellow\" is a risible and preten...      0\n","2      If only to avoid making this type of film in t...      0\n","3      This film was probably inspired by Godard's Ma...      0\n","4      Oh, brother...after hearing about this ridicul...      0\n","...                                                  ...    ...\n","24995  A hit at the time but now better categorised a...      1\n","24996  I love this movie like no other. Another time ...      1\n","24997  This film and it's sequel Barry Mckenzie holds...      1\n","24998  'The Adventures Of Barry McKenzie' started lif...      1\n","24999  The story centers around Barry McKenzie who mu...      1\n","\n","[25000 rows x 2 columns]"]},"execution_count":84,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset_docs = pd.DataFrame(dataset[\"train\"][\"text\"], columns=[\"document\"])\n","train_dataset_classes = pd.DataFrame(dataset[\"train\"][\"label\"], columns=[\"class\"])\n","train_dataset = pd.concat([train_dataset_docs, train_dataset_classes], axis=1)\n","train_dataset"]},{"cell_type":"code","execution_count":130,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","def get_vocabulary(dataset: pd.DataFrame, min_freq: int = 0) -> List[str]:\n","    vectorizer = CountVectorizer(min_df=min_freq)\n","    vectorizer.fit(dataset[\"document\"])\n","    return vectorizer.vocabulary_\n","\n","def train_naive_bayes(dataset):\n","    classes = dataset[\"class\"].unique()\n","    logprior = {}\n","    bigdoc = {}\n","    count = {}\n","    loglikelihood = {}\n","    V = get_vocabulary(dataset)\n","    for c in classes:\n","        n_doc = len(dataset)\n","        n_c = len(dataset[dataset[\"class\"] == c])\n","        logprior[c] = np.log(n_c / n_doc)\n","        bigdoc[c] = \" \".join(dataset[dataset[\"class\"] == c][\"document\"])\n","        for word in V:\n","            count[(word, c)] = bigdoc[c].count(word)\n","            loglikelihood[(word, c)] = np.log((count[(word, c)] + 1) / (sum(count.values()) + len(V)))\n","    return logprior, loglikelihood, V\n","\n","def test_naive_bayes(dataset, logprior, loglikelihood, V):\n","    classes = dataset[\"class\"].unique()\n","    sum_loglikelihood = {}\n","    for c in classes:\n","        sum_loglikelihood[c] = logprior[c]\n","        for word in V:\n","            if word in dataset[\"document\"]:\n","                sum_loglikelihood[c] += loglikelihood[(word, c)]\n","    return max(sum_loglikelihood, key=sum_loglikelihood.get)\n"]},{"cell_type":"code","execution_count":131,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[131], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_naive_bayes(train_dataset)\n","Cell \u001b[0;32mIn[130], line 21\u001b[0m, in \u001b[0;36mtrain_naive_bayes\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     19\u001b[0m     bigdoc[c] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(dataset[dataset[\u001b[39m\"\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m c][\u001b[39m\"\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     20\u001b[0m     \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m V:\n\u001b[0;32m---> 21\u001b[0m         count[(word, c)] \u001b[39m=\u001b[39m bigdoc[c]\u001b[39m.\u001b[39;49mcount(word)\n\u001b[1;32m     22\u001b[0m         loglikelihood[(word, c)] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog((count[(word, c)] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m (\u001b[39msum\u001b[39m(count\u001b[39m.\u001b[39mvalues()) \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(V)))\n\u001b[1;32m     23\u001b[0m \u001b[39mreturn\u001b[39;00m logprior, loglikelihood, V\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["train_naive_bayes(train_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
