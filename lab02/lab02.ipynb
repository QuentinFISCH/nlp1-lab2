{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP LAB 02\n",
    "Théo Ripoll - Quentin Fish - Nicolas Fidel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import get_dataset_split_names\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/Users/quentinfisch/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "100%|██████████| 3/3 [00:00<00:00, 116.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'test', 'unsupervised']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dataset_split_names(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the number of labs in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    12500\n",
      "1    12500\n",
      "Name: label, dtype: int64\n",
      "label\n",
      "0    12500\n",
      "1    12500\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_labels = pd.DataFrame(dataset[\"train\"]['label'], columns=[\"label\"])\n",
    "print(train_labels.groupby(\"label\")[\"label\"].count())\n",
    "\n",
    "test_labels = pd.DataFrame(dataset[\"test\"]['label'], columns=[\"label\"])\n",
    "print(test_labels.groupby(\"label\")[\"label\"].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: How many splits does the dataset has?\n",
    "There are 3 splits: ```train```, ```test``` and ```unsupervised```\n",
    "\n",
    "### Question 2: How big are the splits ?\n",
    "train: 25000\n",
    "test: 25000\n",
    "unsupervised: 50000\n",
    "\n",
    "### Question 3: What is the proportion of each class on the supervised splits?\n",
    "train: 50% positive, 50% negative\n",
    "test: 50% positive, 50% negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2: Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "def preprocess(dataset: pd.DataFrame) -> pd.DataFrame :\n",
    "    \"\"\"\n",
    "    Preprocess the dataset by lowercasing the text and removing the punctuation manually\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : pd.DataFrame\n",
    "        The dataset to preprocess\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The preprocessed dataset\n",
    "    \"\"\"\n",
    "    # First lower the case\n",
    "    dataset[\"document\"] = dataset[\"document\"].apply(lambda x: x.lower())\n",
    "    # Replace the punctuation with spaces. We keep the ' - that may give revelant informations\n",
    "    # Replace HTML tag <br />\n",
    "    punctuation_to_remove = '|'.join(map(re.escape, sorted(list(filter(lambda p: p != \"'\" and p != '-', punctuation)), reverse=True)))\n",
    "    print(f\"Deleting all these punctuation: {punctuation_to_remove}\")\n",
    "    dataset[\"document\"] = dataset[\"document\"].apply(lambda x: re.sub(punctuation_to_remove, \" \", x.replace('<br />', \"\")))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the preprocessing steps to both the training and test sets. We choose to save them in a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all these punctuation: \\~|\\}|\\||\\{|`|_|\\^|\\]|\\\\|\\[|@|\\?|>|=|<|;|:|/|\\.|,|\\+|\\*|\\)|\\(|\\&|%|\\$|\\#|\"|!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i rented i am curious-yellow from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i am curious  yellow  is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>if only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this film was probably inspired by godard's ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oh  brother   after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>a hit at the time but now better categorised a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>i love this movie like no other  another time ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>this film and it's sequel barry mckenzie holds...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>'the adventures of barry mckenzie' started lif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>the story centers around barry mckenzie who mu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document  class\n",
       "0      i rented i am curious-yellow from my video sto...      0\n",
       "1       i am curious  yellow  is a risible and preten...      0\n",
       "2      if only to avoid making this type of film in t...      0\n",
       "3      this film was probably inspired by godard's ma...      0\n",
       "4      oh  brother   after hearing about this ridicul...      0\n",
       "...                                                  ...    ...\n",
       "24995  a hit at the time but now better categorised a...      1\n",
       "24996  i love this movie like no other  another time ...      1\n",
       "24997  this film and it's sequel barry mckenzie holds...      1\n",
       "24998  'the adventures of barry mckenzie' started lif...      1\n",
       "24999  the story centers around barry mckenzie who mu...      1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw = pd.DataFrame(dataset[\"train\"], columns=[\"text\", \"label\"]).rename(columns={\"text\": \"document\", \"label\": \"class\"})\n",
    "preprocessed_train = preprocess(train_raw)\n",
    "preprocessed_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all these punctuation: \\~|\\}|\\||\\{|`|_|\\^|\\]|\\\\|\\[|@|\\?|>|=|<|;|:|/|\\.|,|\\+|\\*|\\)|\\(|\\&|%|\\$|\\#|\"|!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i love sci-fi and am willing to put up with a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worth the entertainment value of a rental  esp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>its a totally average film with a few semi-alr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>star rating        saturday night      friday ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>first off let me say  if you haven't enjoyed a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>just got around to seeing monster man yesterda...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>i got this as part of a competition prize  i w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>i got monster man in a box set of three films ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>five minutes in  i started to feel how naff th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>i caught this movie on the sci-fi channel rece...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document  class\n",
       "0      i love sci-fi and am willing to put up with a ...      0\n",
       "1      worth the entertainment value of a rental  esp...      0\n",
       "2      its a totally average film with a few semi-alr...      0\n",
       "3      star rating        saturday night      friday ...      0\n",
       "4      first off let me say  if you haven't enjoyed a...      0\n",
       "...                                                  ...    ...\n",
       "24995  just got around to seeing monster man yesterda...      1\n",
       "24996  i got this as part of a competition prize  i w...      1\n",
       "24997  i got monster man in a box set of three films ...      1\n",
       "24998  five minutes in  i started to feel how naff th...      1\n",
       "24999  i caught this movie on the sci-fi channel rece...      1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_raw = pd.DataFrame(dataset[\"test\"], columns=[\"text\", \"label\"]).rename(columns={\"text\": \"document\", \"label\": \"class\"})\n",
    "preprocessed_test = preprocess(test_raw)\n",
    "preprocessed_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Naive Bayes Classifier using pseudo-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_vocabulary(d: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Return the vocabulary of the dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d : pd.DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        The vocabulary\n",
    "    \"\"\"\n",
    "    res = list(set(\" \".join(d[\"document\"]).split(\" \")))\n",
    "    # Remove empty string and words without any letter\n",
    "    res = list(filter(lambda x: x != \"\" and re.search(\"[a-zA-Z]\", x), res))\n",
    "    return res\n",
    "\n",
    "def train_naive_bayes(d: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Train a Naive Bayes classifier\n",
    "    Apply pseudo code from lecture 2\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d : pd.DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    logprior : dict\n",
    "        The log prior of each class\n",
    "    loglikelihood : dict\n",
    "        The log likelihood of each word for each class\n",
    "    V : List[str]\n",
    "        The vocabulary\n",
    "    \"\"\"\n",
    "    classes = d[\"class\"].unique()\n",
    "    logprior = {}\n",
    "    bigdoc = {}\n",
    "    loglikelihood = {}\n",
    "    V = get_vocabulary(d)\n",
    "    for c in classes:\n",
    "        count = {}\n",
    "        n_doc = len(d)\n",
    "        n_c = len(d[d[\"class\"] == c])\n",
    "        logprior[c] = np.log(n_c / n_doc)\n",
    "        bigdoc[c] = list(\" \".join(d[d[\"class\"] == c][\"document\"]).split(\" \"))\n",
    "        for word in V:\n",
    "            count[(word, c)] = bigdoc[c].count(word)\n",
    "        for word in V:\n",
    "            loglikelihood[(word, c)] = np.log((count[(word, c)] + 1) / (sum(count.values()) + len(V)))\n",
    "    return logprior, loglikelihood, V\n",
    "\n",
    "def test_naive_bayes(testdoc, classes, logprior, loglikelihood, V) -> int:\n",
    "    \"\"\"\n",
    "    Test a Naive Bayes classifier\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    testdoc : str\n",
    "        The document to classify\n",
    "    classes : List[int]\n",
    "        The list of classes\n",
    "    logprior : dict\n",
    "        The log prior of each class\n",
    "    loglikelihood : dict\n",
    "        The log likelihood of each word for each class\n",
    "    V : List[str]\n",
    "        The vocabulary\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The predicted class\n",
    "    \"\"\"\n",
    "    sum_loglikelihood = {}\n",
    "    for c in classes:\n",
    "        sum_loglikelihood[c] = logprior[c]\n",
    "        for word in testdoc.split(\" \"):\n",
    "            if word in V:\n",
    "                sum_loglikelihood[c] += loglikelihood[(word, c)]\n",
    "    return max(sum_loglikelihood, key=sum_loglikelihood.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Naive Bayes Accuracy Score ->  81.364\n",
      "Manual Naive Bayes Precision Score ->  85.78077941042255\n",
      "Manual Naive Bayes Recall Score ->  75.19200000000001\n"
     ]
    }
   ],
   "source": [
    "logprior_r, loglikelyhood_r, V_r = train_naive_bayes(preprocessed_train)\n",
    "\n",
    "all_res = []\n",
    "for row in preprocessed_test.iterrows():\n",
    "    test_doc = row[1][\"document\"]\n",
    "    res = test_naive_bayes(test_doc, preprocessed_test[\"class\"].unique(), logprior_r, loglikelyhood_r, V_r)\n",
    "    all_res.append(res)\n",
    "\n",
    "print(\"Manual Naive Bayes Accuracy Score -> \",accuracy_score(preprocessed_test[\"class\"], all_res)*100)\n",
    "print(\"Manual Naive Bayes Precision Score -> \",precision_score(preprocessed_test[\"class\"], all_res)*100)\n",
    "print(\"Manual Naive Bayes Recall Score -> \",recall_score(preprocessed_test[\"class\"], all_res)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Naive Bayes Classifier using sklearn (Pipeline with CountVectorizer and MultinomialNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a pipeline with a CountVectorizer and a MultinomialNB. We will use the default parameters for both of them as a first try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_naive_bayes(d_train: pd.DataFrame, pipeline_params: dict = {}) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Train a Naive Bayes classifier using sklearn\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d_train : pd.DataFrame\n",
    "        The training dataset\n",
    "    pipeline_params : dict, optional\n",
    "        The parameters of the pipeline, by default {}\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pipeline\n",
    "        The trained pipeline\n",
    "    \"\"\"\n",
    "    # create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', CountVectorizer()),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "    pipeline.set_params(**pipeline_params)\n",
    "\n",
    "    # train the model\n",
    "    pipeline.fit(d_train[\"document\"], d_train[\"class\"])\n",
    "    return pipeline\n",
    "\n",
    "def test_sklearn_naive_bayes(pipeline: Pipeline, d_test: pd.DataFrame) -> List[int]:\n",
    "    \"\"\"\n",
    "    Test a Naive Bayes classifier using sklearn\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pipeline : Pipeline\n",
    "        The trained pipeline\n",
    "    d_test : pd.DataFrame\n",
    "        The test dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[int]\n",
    "        The predicted classes\n",
    "    \"\"\"\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = pipeline.predict(d_test[\"document\"])\n",
    "\n",
    "    print(\"Sklearn Naive Bayes Accuracy Score -> \",accuracy_score(d_test[\"class\"], predictions)*100)\n",
    "    print(\"Sklearn Naive Bayes Precision Score -> \",precision_score(d_test[\"class\"], predictions)*100)\n",
    "    print(\"Sklearn Naive Bayes Recall Score -> \",recall_score(d_test[\"class\"], predictions)*100)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Naive Bayes Accuracy Score ->  81.44\n",
      "Sklearn Naive Bayes Precision Score ->  86.05504587155963\n",
      "Sklearn Naive Bayes Recall Score ->  75.03999999999999\n"
     ]
    }
   ],
   "source": [
    "pipeline = sklearn_naive_bayes(preprocessed_train)\n",
    "predictions = test_sklearn_naive_bayes(pipeline, preprocessed_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Report the accuracy on the test set\n",
    "\n",
    "See prints above\n",
    "\n",
    "### Question 5: Most likely, the scikit-learn implementation will give better results. Looking at the documentation, explain why it could be the case.\n",
    "\n",
    "The scikit-learn implementation is better because it uses a MultinomialNB which is a more efficient way to compute the probabilities. It also uses a CountVectorizer which is a more efficient way to count the words in the dataset.\n",
    "\n",
    "### Question 6: Why is accuracy a sufficient measure of evaluation here?\n",
    "\n",
    "Because the dataset is balanced, we have the same number of positive and negative reviews. So the accuracy is a good measure of evaluation.\n",
    "\n",
    "### Question 7: Using one of the implementation, take at least 2 wrongly classified example from the test set and try explaining why the model failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blind date  columbia pictures  1934   was a decent film  but i have a few issues with this film  first of all  i don't fault the actors in this film at all  but more or less  i have a problem with the script  also  i understand that this film was made in the 1930's and people were looking to escape reality  but the script made ann sothern's character look weak  she kept going back and forth between suitors and i felt as though she should have stayed with paul kelly's character in the end  he truly did care about her and her family and would have done anything for her and he did by giving her up in the end to fickle neil hamilton who in my opinion was only out for a good time  paul kelly's character  although a workaholic was a man of integrity and truly loved kitty  ann sothern  as opposed to neil hamilton  while he did like her a lot  i didn't see the depth of love that he had for her character  the production values were great  but the script could have used a little work \n",
      "ben   rupert grint   is a deeply unhappy adolescent  the son of his unhappily married parents  his father   nicholas farrell   is a vicar and his mother   laura linney   is     well  let's just say she's a somewhat hypocritical soldier in jesus' army  it's only when he takes a summer job as an assistant to a foul-mouthed  eccentric  once-famous and now-forgotten actress evie walton   julie walters   that he finally finds himself in true 'harold and maude' fashion  of course  evie is deeply unhappy herself and it's only when these two sad sacks find each other that they can put their mutual misery aside and hit the road to happiness of course it's corny and sentimental and very predictable but it has a hard side to it  too and walters  who could sleep-walk her way through this sort of thing if she wanted  is excellent  it's when she puts the craziness to one side and finds the pathos in the character   like hitting the bottle and throwing up in the sink   that she's at her best  the problem is she's the only interesting character in the film  and it's not because of the script which doesn't do anybody any favours   grint  on the other hand  isn't just unhappy  he's a bit of a bore as well while linney's starched bitch is completely one-dimensional   still  she's got the english accent off pat   the best that can be said for it is that it's mildly enjoyable - with the emphasis on the mildly \n",
      "\n",
      "[[4.22158007e-06 9.99995778e-01]]\n",
      "[[0.00150068 0.99849932]]\n"
     ]
    }
   ],
   "source": [
    "# We will take a look at the sklearn implementation\n",
    "# First we need to get the wrongly classified examples\n",
    "wrongly_classified = preprocessed_test[preprocessed_test[\"class\"] != predictions]\n",
    "\n",
    "# We will take the first 2 examples\n",
    "# We can see that the first example is a negative review but the model predicted it as a positive review\n",
    "# The second example is a positive review but the model predicted it as a negative review\n",
    "print(wrongly_classified.iloc[0][\"document\"])\n",
    "print(wrongly_classified.iloc[1][\"document\"])\n",
    "print()\n",
    "\n",
    "# Let's see the probability of each class for the first example\n",
    "print(pipeline.predict_proba([wrongly_classified.iloc[0][\"document\"]]))\n",
    "# Let's see the probability of each class for the second example\n",
    "print(pipeline.predict_proba([wrongly_classified.iloc[1][\"document\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model is very confident about its prediction for the two examples (0.99...) but it's wrong. These examples are very hard to classify because they are very close to the decision boundary and also mixing a movie description (which can have positive or negative connotations due to the life of the main character, etc) and a review. So the model is not able to classify them correctly because of the confusing bundary between description and facts and the opinion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8: What are the top 10 most important words (features) for each class? (bonus points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the sklearn implementation to get the top 10 most important words for each class using the log likelihood as a metric\n",
    "\n",
    "def get_top_10_words(pipeline: Pipeline) -> dict:\n",
    "    \"\"\"\n",
    "    Get the top 10 words for each class\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pipeline : Pipeline\n",
    "        The trained pipeline\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The top 10 words for each class\n",
    "    \"\"\"\n",
    "    top_10_words = {}\n",
    "    for c in preprocessed_test[\"class\"].unique():\n",
    "        loglikelihood = pipeline.named_steps[\"classifier\"].feature_log_prob_[c]\n",
    "        V = pipeline.named_steps[\"vectorizer\"].vocabulary_\n",
    "        top_10_words[c] = [list(V.keys())[list(V.values()).index(i)] for i in np.argsort(loglikelihood)[-10:]]\n",
    "    return top_10_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['was', 'that', 'this', 'in', 'it', 'is', 'to', 'of', 'and', 'the'],\n",
       " 1: ['as', 'this', 'that', 'it', 'in', 'is', 'to', 'of', 'and', 'the']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_10_words(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words we retreive are stop words, so they are not very meaningful. Let's try to remove them and see if we get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Naive Bayes Accuracy Score ->  81.976\n",
      "Sklearn Naive Bayes Precision Score ->  86.22439731738264\n",
      "Sklearn Naive Bayes Recall Score ->  76.112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: ['story',\n",
       "  'don',\n",
       "  'time',\n",
       "  'really',\n",
       "  'bad',\n",
       "  'good',\n",
       "  'just',\n",
       "  'like',\n",
       "  'film',\n",
       "  'movie'],\n",
       " 1: ['people',\n",
       "  'really',\n",
       "  'great',\n",
       "  'time',\n",
       "  'story',\n",
       "  'just',\n",
       "  'good',\n",
       "  'like',\n",
       "  'movie',\n",
       "  'film']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_without_stopwords = sklearn_naive_bayes(preprocessed_train, {\"vectorizer__stop_words\": \"english\"})\n",
    "predictions_without_stopwords = test_sklearn_naive_bayes(pipeline_without_stopwords, preprocessed_test)\n",
    "\n",
    "get_top_10_words(pipeline_without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the top 10 words are more unique using stopwords, but the results are pretty equivalent with or without stopwords.\n",
    "\n",
    "### Question 9: Play with scikit-learn's version parameters. For example, see if you can consider unigram and bigram instead of only unigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare previous results using sklearn with the results using unigram and bigram, and with/without removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Naive Bayes Accuracy Score ->  84.244\n",
      "Sklearn Naive Bayes Precision Score ->  87.4857693318154\n",
      "Sklearn Naive Bayes Recall Score ->  79.92\n"
     ]
    }
   ],
   "source": [
    "# Unigram and bigram\n",
    "pipeline_bigram = sklearn_naive_bayes(preprocessed_train, {\"vectorizer__ngram_range\": (1, 2), \"vectorizer__stop_words\": \"english\"})\n",
    "predictions_bigram = test_sklearn_naive_bayes(pipeline_bigram, preprocessed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Naive Bayes Accuracy Score ->  85.672\n",
      "Sklearn Naive Bayes Precision Score ->  88.62612612612612\n",
      "Sklearn Naive Bayes Recall Score ->  81.848\n"
     ]
    }
   ],
   "source": [
    "# Unigram and bigram with stopwords\n",
    "pipeline_bigram_stopwords = sklearn_naive_bayes(preprocessed_train, {\"vectorizer__ngram_range\": (1, 2)})\n",
    "predictions_bigram_stopwords = test_sklearn_naive_bayes(pipeline_bigram_stopwords, preprocessed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Naive Bayes Accuracy Score ->  82.952\n",
      "Sklearn Naive Bayes Precision Score ->  87.63018454229857\n",
      "Sklearn Naive Bayes Recall Score ->  76.736\n"
     ]
    }
   ],
   "source": [
    "# Only bigram\n",
    "pipeline_only_bigram = sklearn_naive_bayes(preprocessed_train, {\"vectorizer__ngram_range\": (2, 2), \"vectorizer__stop_words\": \"english\"})\n",
    "predictions_only_bigram = test_sklearn_naive_bayes(pipeline_only_bigram, preprocessed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Naive Bayes Accuracy Score ->  86.952\n",
      "Sklearn Naive Bayes Precision Score ->  89.35753237900477\n",
      "Sklearn Naive Bayes Recall Score ->  83.896\n"
     ]
    }
   ],
   "source": [
    "# Only bigram with stopwords\n",
    "pipeline_only_bigram_stopwords = sklearn_naive_bayes(preprocessed_train, {\"vectorizer__ngram_range\": (2, 2)})\n",
    "predictions_only_bigram_stopwords = test_sklearn_naive_bayes(pipeline_only_bigram_stopwords, preprocessed_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is better with only bigrams and without removing stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Stemming & Lemmatization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will add preprocessing, including stemming and leammatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add an extra module for spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization preprocessing\n",
    "\n",
    "Let's start with a small example to understand how to recover a lem.\n",
    "\n",
    "In this case we will use Spacy, especially its pipeline features to do preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was\n",
      "Original : rented, New: rent\n",
      "Original : AM, New: be\n",
      "Original : CURIOUS, New: curious\n",
      "Original : surrounded, New: surround\n",
      "Original : was, New: be\n"
     ]
    }
   ],
   "source": [
    "# Take a 20 characters sentence example from the test dataset\n",
    "test_list = dataset['train']['text'][0].split()[:20]\n",
    "test_sentence = ' '.join(test_list)\n",
    "\n",
    "# Lemmatize the sentence\n",
    "doc = nlp(test_sentence)\n",
    "\n",
    "# Get all token\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "print(f'Original Sentence: {test_sentence}')\n",
    "for token in doc:\n",
    "    if token.text != token.lemma_:\n",
    "        print(f'Original : {token.text}, New: {token.lemma_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results look good, words are reduced to their root form.\n",
    "\n",
    "Let's define a preprocessing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_preprocessor(x_list: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Preprocessing function to lowercase and remove punctuation\n",
    "    of a list of string and lemmatize each string.\n",
    "    \n",
    "    Args:\n",
    "        x_list: List of strings\n",
    "    \n",
    "    Returns:\n",
    "        List of preprocessed strings.\n",
    "    \"\"\"\n",
    "    no_punc_lower = [x.lower().translate(str.maketrans(\"\", \"\", punctuation)) for x in x_list]\n",
    "    spacy_nlp = spacy.load('en_core_web_sm')\n",
    "    res = []\n",
    "    for sentence in no_punc_lower:\n",
    "        doc = spacy_nlp(sentence)\n",
    "        s = []\n",
    "        for word in doc:\n",
    "            s.append(word.lemma_)\n",
    "        s = ' '.join(s)\n",
    "        res.append(s)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a example of the result :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I rent I be curiousyellow from my video store because of all the controversy that surround it when it be first release in 1967 I also hear that at first it be seize by us customs if it ever try to enter this country therefore be a fan of film consider controversial I really have to see this for myselfbr br the plot be center around a young swedish drama student name lena who want to learn everything she can about life in particular she want to focus her attention to make some sort of documentary on what the average swede think about certain political issue such as the vietnam war and race issue in the united states in between ask politician and ordinary denizen of stockholm about their opinion on politic she have sex with her drama teacher classmate and married menbr br what kill I about I be curiousyellow be that 40 year ago this be consider pornographic really the sex and nudity scene be few and far between even then its not shoot like some cheaply make porno while my countryman mind find it shock in reality sex and nudity be a major staple in swedish cinema even ingmar bergman arguably their answer to good old boy john ford have sex scene in his filmsbr br I do commend the filmmaker for the fact that any sex show in the film be show for artistic purpose rather than just to shock people and make money to be show in pornographic theater in america I be curiousyellow be a good film for anyone want to study the meat and potatoe no pun intend of swedish cinema but really this film do not have much of a plot']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset['train']['text'][:1])\n",
    "lemma_preprocessor(dataset['train']['text'][:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the preprocessing is working well: words are reduced to their lemma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a small example to understand how to recover a lem.\n",
    "\n",
    "In this case we will use NLTK, another library than Spacy, but it offers stemming unlike Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/quentinfisch/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was\n",
      "--Word--            --Stem--            \n",
      "I                   i                   \n",
      "rented              rent                \n",
      "I                   i                   \n",
      "AM                  am                  \n",
      "CURIOUS-YELLOW      curious-yellow      \n",
      "from                from                \n",
      "my                  my                  \n",
      "video               video               \n",
      "store               store               \n",
      "because             becaus              \n",
      "of                  of                  \n",
      "all                 all                 \n",
      "the                 the                 \n",
      "controversy         controversi         \n",
      "that                that                \n",
      "surrounded          surround            \n",
      "it                  it                  \n",
      "when                when                \n",
      "it                  it                  \n",
      "was                 wa                  \n"
     ]
    }
   ],
   "source": [
    "# Initialize Python porter stemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "test_list = dataset['train']['text'][0].split()[:20]\n",
    "test_sentence = ' '.join(test_list)\n",
    "\n",
    "# Example inflections to reduce\n",
    "example_words = [\"program\",\"programming\",\"programer\",\"programs\",\"programmed\"]\n",
    "\n",
    "print(f'Original Sentence: {test_sentence}')\n",
    "# Perform stemming\n",
    "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Stem--\"))\n",
    "for word in test_list:\n",
    "    print (\"{0:20}{1:20}\".format(word, ps.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, results are stasisfyng. However, we observe some errors, such as \"becaus\" instead of \"because\", or \"wa\" instead of \"was\".\n",
    "\n",
    "Let's define a preprocessing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_preprocessor(x_list: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Preprocessing function to stem each string.\n",
    "    \n",
    "    Args:\n",
    "        x_list: List of strings\n",
    "    \n",
    "    Returns:\n",
    "        List of preprocessed strings.\n",
    "    \"\"\"\n",
    "    spacy_nlp = spacy.load('en_core_web_sm')\n",
    "    res = []\n",
    "    ps = PorterStemmer()\n",
    "    for sentence in x_list:\n",
    "        doc = spacy_nlp(sentence)\n",
    "        s = []\n",
    "        for word in doc:\n",
    "            s.append(ps.stem(str(word)))\n",
    "        s = ' '.join(s)\n",
    "        res.append(s)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['program', 'program', 'program', 'program', 'program']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_words = [\"program\",\"programming\",\"programer\",\"programs\",\"programmed\"]\n",
    "stem_preprocessor(example_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Stem and Lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemma training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both are working well. Now let's try to use lemmatization in our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I rent I be curiousyellow from my video store because of all the controversy that surround it when it be first release in 1967   I also hear that at first it be seize by u s   custom if it ever try to enter this country   therefore be a fan of film consider   controversial   I really have to see this for myself the plot be center around a young swedish drama student name lena who want to learn everything she can about life   in particular she want to focus her attention to make some sort of documentary on what the average swede think about certain political issue such as the vietnam war and race issue in the united states   in between ask politician and ordinary denizen of stockholm about their opinion on politic   she have sex with her drama teacher   classmate   and married man what kill I about I be curiousyellow be that 40 year ago   this be consider pornographic   really   the sex and nudity scene be few and far between   even then its not shoot like some cheaply make porno   while my countryman mind find it shocking   in reality sex and nudity be a major staple in swedish cinema   even ingmar bergman   arguably their answer to good old boy john ford   have sex scene in his film I do commend the filmmaker for the fact that any sex show in the film be show for artistic purpose rather than just to shock people and make money to be show in pornographic theater in america   I be curiousyellow be a good film for anyone want to study the meat and potato   no pun intend   of swedish cinema   but really   this film do not have much of a plot',\n",
       " '  I be curious   yellow   be a risible and pretentious steaming pile   it do not matter what one political view be because this film can hardly be take seriously on any level   as for the claim that frontal male nudity be an automatic nc17   that be not true   I ve see rrate film with male nudity   grant   they only offer some fleeting view   but where be the rrate film with gape vulvas and flap labia   nowhere   because they do not exist   the same go for those crappy cable show   schlong swinge in the breeze but not a clitoris in sight   and those pretentious indie movie like the brown bunny   in which be treat to the site of vincent gallos throb johnson   but not a trace of pink visible on chloe sevigny   before cry   or imply    doublestandard   in matter of nudity   the mentally obtuse should take into account one unavoidably obvious anatomical difference between man and woman   there be no genital on display when actress appear nude   and the same can not be say for a man   in fact   you generally will not see female genital in an american film in anything short of porn or explicit erotica   this allege doublestandard be less a double standard than an admittedly depressing ability to come to term culturally with the inside of women body']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use stem_preprocessor to preprocess the training and test data\n",
    "preprocessed_train_lemma = lemma_preprocessor(train_raw[\"document\"][:2])\n",
    "preprocessed_train_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a function that will drive the model by adding the preprocessor lemma to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def sklearn_naive_bayes_lemma(d_train: pd.DataFrame, pipeline_params: dict = {}) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Train a Naive Bayes classifier using sklearn with lemmatization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d_train : pd.DataFrame\n",
    "        The training dataset\n",
    "    pipeline_params : dict, optional\n",
    "        The parameters of the pipeline, by default {}\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pipeline\n",
    "        The trained pipeline\n",
    "    \"\"\"\n",
    "    # create pipeline with lemmatization, vectorizer and classifier\n",
    "    pipeline = Pipeline([\n",
    "        ('lemmatizer', FunctionTransformer(lemma_preprocessor)),\n",
    "        ('vectorizer', CountVectorizer()),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "    pipeline.set_params(**pipeline_params)\n",
    "\n",
    "    # train the model\n",
    "    pipeline.fit(d_train[\"document\"], d_train[\"class\"])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and evaluation of the model again with these pretreatment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Naive Bayes Accuracy Score ->  80.976\n",
      "Sklearn Naive Bayes Precision Score ->  85.6078719882288\n",
      "Sklearn Naive Bayes Recall Score ->  74.47200000000001\n"
     ]
    }
   ],
   "source": [
    "pipeline_lemma = sklearn_naive_bayes_lemma(train_raw)\n",
    "predictions_lemma = test_sklearn_naive_bayes(pipeline_lemma, test_raw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are not better than before (with default settings): 80.97% vs 81.44% accuracy.\n",
    "This is probably due to the fact that the lemmatization is not very efficient in this case. This can be caused by the fact the language is English, and the lemmatization is not very efficient for this language because of it's low morphology, removing information that could be useful for the classifier.\n",
    "\n",
    "Let's try with stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Stem training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a function that will drive the model by adding the preprocessor stem to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def sklearn_naive_bayes_stem(d_train: pd.DataFrame, pipeline_params: dict = {}) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Train a Naive Bayes classifier using sklearn with lemmatization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d_train : pd.DataFrame\n",
    "        The training dataset\n",
    "    pipeline_params : dict, optional\n",
    "        The parameters of the pipeline, by default {}\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pipeline\n",
    "        The trained pipeline\n",
    "    \"\"\"\n",
    "    # create pipeline with lemmatization, vectorizer and classifier\n",
    "    pipeline = Pipeline([\n",
    "        ('lemmatizer', FunctionTransformer(stem_preprocessor)),\n",
    "        ('vectorizer', CountVectorizer()),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "    pipeline.set_params(**pipeline_params)\n",
    "\n",
    "    # train the model\n",
    "    pipeline.fit(d_train[\"document\"], d_train[\"class\"])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Naive Bayes Accuracy Score ->  80.696\n",
      "Sklearn Naive Bayes Precision Score ->  85.29898804047839\n",
      "Sklearn Naive Bayes Recall Score ->  74.176\n"
     ]
    }
   ],
   "source": [
    "pipeline_stem = sklearn_naive_bayes_stem(train_raw)\n",
    "predictions_stem = test_sklearn_naive_bayes(pipeline_stem, test_raw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the results are even worse than before (with default settings): 80.69% vs 81.44% accuracy. Again, we surely have the same problem as before, the stemming is not very efficient in this case.\n",
    "Lemmatization is better than stemming in this case, because it's more aggressive on words changed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
