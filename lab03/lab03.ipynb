{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/Users/quentinfisch/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "100%|██████████| 3/3 [00:00<00:00, 121.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "def preprocess(dataset: pd.DataFrame) -> pd.DataFrame :\n",
    "    \"\"\"\n",
    "    Preprocess the dataset by lowercasing the text and removing the punctuation manually\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : pd.DataFrame\n",
    "        The dataset to preprocess\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The preprocessed dataset\n",
    "    \"\"\"\n",
    "    # First lower the case\n",
    "    dataset[\"document\"] = dataset[\"document\"].apply(lambda x: x.lower())\n",
    "    # Replace the punctuation with spaces. We keep the ' - that may give revelant informations\n",
    "    # Replace HTML tag <br />\n",
    "    punctuation_to_remove = '|'.join(map(re.escape, sorted(list(filter(lambda p: p != \"'\" and p != '-' and p != \"!\", punctuation)), reverse=True)))\n",
    "    print(f\"Deleting all these punctuation: {punctuation_to_remove}\")\n",
    "    dataset[\"document\"] = dataset[\"document\"].apply(lambda x: re.sub(punctuation_to_remove, \" \", x.replace('<br />', \"\")))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all these punctuation: \\~|\\}|\\||\\{|`|_|\\^|\\]|\\\\|\\[|@|\\?|>|=|<|;|:|/|\\.|,|\\+|\\*|\\)|\\(|\\&|%|\\$|\\#|\"\n",
      "Deleting all these punctuation: \\~|\\}|\\||\\{|`|_|\\^|\\]|\\\\|\\[|@|\\?|>|=|<|;|:|/|\\.|,|\\+|\\*|\\)|\\(|\\&|%|\\$|\\#|\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                                document  class\n",
       " 0      i rented i am curious-yellow from my video sto...      0\n",
       " 1       i am curious  yellow  is a risible and preten...      0\n",
       " 2      if only to avoid making this type of film in t...      0\n",
       " 3      this film was probably inspired by godard's ma...      0\n",
       " 4      oh  brother   after hearing about this ridicul...      0\n",
       " ...                                                  ...    ...\n",
       " 24995  a hit at the time but now better categorised a...      1\n",
       " 24996  i love this movie like no other  another time ...      1\n",
       " 24997  this film and it's sequel barry mckenzie holds...      1\n",
       " 24998  'the adventures of barry mckenzie' started lif...      1\n",
       " 24999  the story centers around barry mckenzie who mu...      1\n",
       " \n",
       " [25000 rows x 2 columns],\n",
       "                                                 document  class\n",
       " 0      i love sci-fi and am willing to put up with a ...      0\n",
       " 1      worth the entertainment value of a rental  esp...      0\n",
       " 2      its a totally average film with a few semi-alr...      0\n",
       " 3      star rating        saturday night      friday ...      0\n",
       " 4      first off let me say  if you haven't enjoyed a...      0\n",
       " ...                                                  ...    ...\n",
       " 24995  just got around to seeing monster man yesterda...      1\n",
       " 24996  i got this as part of a competition prize  i w...      1\n",
       " 24997  i got monster man in a box set of three films ...      1\n",
       " 24998  five minutes in  i started to feel how naff th...      1\n",
       " 24999  i caught this movie on the sci-fi channel rece...      1\n",
       " \n",
       " [25000 rows x 2 columns])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw = pd.DataFrame(dataset[\"train\"], columns=[\"text\", \"label\"]).rename(columns={\"text\": \"document\", \"label\": \"class\"})\n",
    "preprocessed_train = preprocess(train_raw)\n",
    "\n",
    "test_raw = pd.DataFrame(dataset[\"test\"], columns=[\"text\", \"label\"]).rename(columns={\"text\": \"document\", \"label\": \"class\"})\n",
    "preprocessed_test = preprocess(test_raw)\n",
    "preprocessed_train, preprocessed_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load lexicon and keep only interesting tokens (one above the treshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>$:</th>\n",
       "      <td>-1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>%-)</th>\n",
       "      <td>-1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>( '}{' )</th>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('-:</th>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(':</th>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>|^:</th>\n",
       "      <td>-1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>||-:</th>\n",
       "      <td>-2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>}:</th>\n",
       "      <td>-2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>}:(</th>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>}:-(</th>\n",
       "      <td>-2.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5830 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Score\n",
       "Token          \n",
       "$:         -1.5\n",
       "%-)        -1.5\n",
       "( '}{' )    1.6\n",
       "('-:        2.2\n",
       "(':         2.3\n",
       "...         ...\n",
       "|^:        -1.1\n",
       "||-:       -2.3\n",
       "}:         -2.1\n",
       "}:(        -2.0\n",
       "}:-(       -2.1\n",
       "\n",
       "[5830 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 1\n",
    "lexicon = pd.read_csv(\"vader_lexicon.txt\", sep=\"\\t\", names=['Token', \"Score\", \"Std\", \"Vector\"]).drop(columns=[\"Std\", \"Vector\"]).set_index(\"Token\")\n",
    "lexicon = lexicon[(lexicon[\"Score\"] <= -threshold) | (lexicon[\"Score\"] >= threshold)]\n",
    "lexicon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the following features:\n",
    "- 1 if \"no\" appears in the document, 0 otherwise.\n",
    "- The count of first and second pronouns in the document.\n",
    "- 1 if \"!\" is in the document, 0 otherwise.\n",
    "- Log(word count in the document).\n",
    "- Number of words in the document which are in the positive lexicon.\n",
    "- Number of words in the document which are in the negative lexicon.\n",
    "- [Bonus] Add another feature of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the following features:\n",
    "# - 1 if \"no\" appears in the document, 0 otherwise.\n",
    "# - The count of first and second pronouns in the document.\n",
    "# - 1 if \"!\" is in the document, 0 otherwise.\n",
    "# - Log(word count in the document).\n",
    "# - Number of words in the document which are in the positive lexicon.\n",
    "# - Number of words in the document which are in the negative lexicon.\n",
    "# - [Bonus] Add another feature of your choice.\n",
    "\n",
    "def is_in_lexicon(word: str, positive: bool):\n",
    "    try:\n",
    "        score = lexicon.at[word, \"Score\"].item()\n",
    "        return score >= threshold if positive else score <= -threshold\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def generate_features(dataset: pd.DataFrame) -> pd.DataFrame :\n",
    "    \"\"\"\n",
    "    Generate the features for the dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : pd.DataFrame\n",
    "        The dataset to generate the features for\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The dataset with the features\n",
    "    \"\"\"\n",
    "    dataset[\"no\"] = dataset[\"document\"].apply(lambda x: 1 if \"no\" in x.split(\" \") else 0)\n",
    "    dataset[\"pronouns\"] = dataset[\"document\"].apply(lambda x: x.split(\" \")).apply(lambda x: x.count(\"i\") + x.count(\"we\") + x.count(\"you\"))\n",
    "    dataset[\"exclamation\"] = dataset[\"document\"].apply(lambda x: 1 if \"!\" in x else 0)\n",
    "    dataset[\"log_word_count\"] = dataset[\"document\"].apply(lambda x: np.log(len(x.split(\" \"))))\n",
    "    dataset[\"positive_lexicon\"] = dataset[\"document\"].apply(lambda x: len(list(filter(lambda w: is_in_lexicon(w, True), x.split(\" \")))))\n",
    "    dataset[\"negative_lexicon\"] = dataset[\"document\"].apply(lambda x: len(list(filter(lambda w: is_in_lexicon(w, False), x.split(\" \")))))\n",
    "\n",
    "    # add feature vector column\n",
    "    dataset[\"feature_vector\"] = dataset.apply(lambda x: [x[\"no\"], x[\"pronouns\"], x[\"exclamation\"], x[\"log_word_count\"], x[\"positive_lexicon\"], x[\"negative_lexicon\"]], axis=1)\n",
    "    # drop the other columns\n",
    "    dataset = dataset.drop(columns=[\"no\", \"pronouns\", \"exclamation\", \"log_word_count\", \"positive_lexicon\", \"negative_lexicon\"])\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>class</th>\n",
       "      <th>feature_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i rented i am curious-yellow from my video sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 11, 0, 5.739792912179234, 7, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>it was great to see some of my favorite stars ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 9, 0, 5.680172609017068, 14, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>if the crew behind  zombie chronicles  ever re...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 9, 0, 5.327876168789581, 9, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>i have not seen many low budget films i must a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 7, 0, 5.209486152841421, 8, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>i have read all of the love come softly books ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 5, 0, 5.0106352940962555, 5, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24950</th>\n",
       "      <td>i definitely recommend reading the book prior ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 6, 1, 5.703782474656201, 14, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24960</th>\n",
       "      <td>i used to watch this show when i was a little ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 14, 0, 5.123963979403259, 9, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24970</th>\n",
       "      <td>i've seen this movie and i must say i'm very i...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 13, 0, 5.0689042022202315, 13, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24980</th>\n",
       "      <td>i was pleased to see that she had black hair! ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 8, 1, 5.049856007249537, 9, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24990</th>\n",
       "      <td>like i said its a hidden surprise  it well wri...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 4, 0, 4.127134385045092, 9, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document  class  \\\n",
       "0      i rented i am curious-yellow from my video sto...      0   \n",
       "10     it was great to see some of my favorite stars ...      0   \n",
       "20     if the crew behind  zombie chronicles  ever re...      0   \n",
       "30     i have not seen many low budget films i must a...      0   \n",
       "40     i have read all of the love come softly books ...      0   \n",
       "...                                                  ...    ...   \n",
       "24950  i definitely recommend reading the book prior ...      1   \n",
       "24960  i used to watch this show when i was a little ...      1   \n",
       "24970  i've seen this movie and i must say i'm very i...      1   \n",
       "24980  i was pleased to see that she had black hair! ...      1   \n",
       "24990  like i said its a hidden surprise  it well wri...      1   \n",
       "\n",
       "                              feature_vector  \n",
       "0        [1, 11, 0, 5.739792912179234, 7, 6]  \n",
       "10       [1, 9, 0, 5.680172609017068, 14, 4]  \n",
       "20        [1, 9, 0, 5.327876168789581, 9, 2]  \n",
       "30        [0, 7, 0, 5.209486152841421, 8, 7]  \n",
       "40       [0, 5, 0, 5.0106352940962555, 5, 3]  \n",
       "...                                      ...  \n",
       "24950    [0, 6, 1, 5.703782474656201, 14, 3]  \n",
       "24960    [0, 14, 0, 5.123963979403259, 9, 0]  \n",
       "24970  [0, 13, 0, 5.0689042022202315, 13, 2]  \n",
       "24980     [0, 8, 1, 5.049856007249537, 9, 3]  \n",
       "24990     [0, 4, 0, 4.127134385045092, 9, 0]  \n",
       "\n",
       "[2500 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_train = preprocessed_train.iloc[::10].copy()\n",
    "reduced_train = generate_features(reduced_train)\n",
    "reduced_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>class</th>\n",
       "      <th>feature_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i love sci-fi and am willing to put up with a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 5, 1, 5.62040086571715, 9, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>this flick is a waste of time i expect from an...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 4.948759890378168, 3, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>low budget horror movie  if you don't raise yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 6, 1, 5.225746673713202, 10, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>lowe returns to the nest after  yet another  f...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 4, 1, 5.459585514144159, 8, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>i can't believe the high marks people have giv...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 6, 1, 5.6240175061873385, 8, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24950</th>\n",
       "      <td>especially by lambert  this is the essential...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 8, 0, 5.10594547390058, 8, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24960</th>\n",
       "      <td>watch it with an open mind  it is very differe...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 0, 4.02535169073515, 4, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24970</th>\n",
       "      <td>slipknot is a heavy metal band from the great ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 8, 0, 5.117993812416755, 6, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24980</th>\n",
       "      <td>this film is just plain lovely  it's funny as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 3, 1, 4.912654885736052, 5, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24990</th>\n",
       "      <td>i first saw this on demand  or on tv  i'm not ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 9, 1, 4.969813299576001, 10, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document  class  \\\n",
       "0      i love sci-fi and am willing to put up with a ...      0   \n",
       "10     this flick is a waste of time i expect from an...      0   \n",
       "20     low budget horror movie  if you don't raise yo...      0   \n",
       "30     lowe returns to the nest after  yet another  f...      0   \n",
       "40     i can't believe the high marks people have giv...      0   \n",
       "...                                                  ...    ...   \n",
       "24950    especially by lambert  this is the essential...      1   \n",
       "24960  watch it with an open mind  it is very differe...      1   \n",
       "24970  slipknot is a heavy metal band from the great ...      1   \n",
       "24980  this film is just plain lovely  it's funny as ...      1   \n",
       "24990  i first saw this on demand  or on tv  i'm not ...      1   \n",
       "\n",
       "                            feature_vector  \n",
       "0        [0, 5, 1, 5.62040086571715, 9, 8]  \n",
       "10     [1, 1, 0, 4.948759890378168, 3, 12]  \n",
       "20     [0, 6, 1, 5.225746673713202, 10, 7]  \n",
       "30      [0, 4, 1, 5.459585514144159, 8, 9]  \n",
       "40     [0, 6, 1, 5.6240175061873385, 8, 9]  \n",
       "...                                    ...  \n",
       "24950    [0, 8, 0, 5.10594547390058, 8, 3]  \n",
       "24960    [0, 1, 0, 4.02535169073515, 4, 0]  \n",
       "24970   [0, 8, 0, 5.117993812416755, 6, 1]  \n",
       "24980   [0, 3, 1, 4.912654885736052, 5, 2]  \n",
       "24990  [0, 9, 1, 4.969813299576001, 10, 0]  \n",
       "\n",
       "[2500 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_test = preprocessed_test.iloc[::10].copy()\n",
    "\n",
    "reduced_test = generate_features(reduced_test)\n",
    "reduced_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set generated\n",
      "Test set generated\n"
     ]
    }
   ],
   "source": [
    "preprocessed_train = generate_features(preprocessed_train)\n",
    "print(\"Train set generated\")\n",
    "preprocessed_test = generate_features(preprocessed_test)\n",
    "print(\"Test set generated\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    \"\"\"A linear regression implementation\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, nb_classes: int) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: the dimension of the input features.\n",
    "            nb_classes: the number of classes to predict.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, nb_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: the input tensor.\n",
    "        Returns:\n",
    "            The output of the linear layer.\n",
    "        \"\"\"\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(6, 1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([21250, 6]),\n",
       " torch.Size([3750, 6]),\n",
       " torch.Size([21250, 1]),\n",
       " torch.Size([3750, 1]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train = preprocessed_train[\"feature_vector\"]\n",
    "# convert to numpy array 2d\n",
    "features_train = np.array(features_train.to_list())\n",
    "labels_train = preprocessed_train[\"class\"].to_numpy()\n",
    "\n",
    "features_train = torch.tensor(features_train, dtype=torch.float32)\n",
    "labels_train = torch.tensor(labels_train, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    features_train,\n",
    "    labels_train,\n",
    "    test_size=0.15,\n",
    "    stratify=labels_train,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([25000, 6]), torch.Size([25000, 1]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's do the same feature engineering for the test set\n",
    "features_test = preprocessed_test[\"feature_vector\"]\n",
    "features_test = np.array(features_test.to_list())\n",
    "X_test = torch.tensor(features_test, dtype=torch.float32)\n",
    "\n",
    "labels_test = preprocessed_test[\"class\"].to_numpy()\n",
    "y_test = torch.tensor(labels_test, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8912, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5868, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5859, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5853, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5850, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5849, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5848, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5848, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5848, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5848, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "CPU times: user 680 ms, sys: 176 ms, total: 856 ms\n",
      "Wall time: 597 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "# Keeping an eye on the losses\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # Setting all gradients to zero.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Sending the whole training set through the model.\n",
    "    predictions = model(X_train)\n",
    "    # Computing the loss.\n",
    "    loss = criterion(predictions, y_train)\n",
    "    train_losses.append(loss.item())\n",
    "    if epoch % 100 == 0:\n",
    "        print(loss)\n",
    "    # Computing the gradients and gradient descent.\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # When computing the validation loss, we do not want to update the weights.\n",
    "    # torch.no_grad tells PyTorch to not save the necessary data used for\n",
    "    # gradient descent.\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_valid)\n",
    "        loss = criterion(predictions, y_valid)\n",
    "        test_losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x28828af10>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(train_losses)), train_losses, label=\"Training loss\")\n",
    "plt.plot(np.arange(len(test_losses)), test_losses, label=\"Test loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7152 0.7 0.71408\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    p_train = torch.sigmoid(model(X_train))\n",
    "    p_train = np.round(p_train.numpy())\n",
    "    training_accuracy = np.mean(p_train == y_train.numpy())\n",
    "    p_valid = torch.sigmoid(model(X_valid))\n",
    "    p_valid = np.round(p_valid.numpy())\n",
    "    valid_accuracy = np.mean(p_valid == y_valid.numpy())\n",
    "    p_test = torch.sigmoid(model(X_test))\n",
    "    p_test = np.round(p_test.numpy())\n",
    "    test_accuracy = np.mean(p_test == y_test.numpy())\n",
    "print(training_accuracy, valid_accuracy, test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02932489 -0.02839465 -0.00931369 -0.02108636  0.12831183 -0.15112449]]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Look at the weights of your classifier. Which features seems to play most for both classes ?\n",
    "\n",
    "# Let's look at the weights of the linear layer\n",
    "weights = model.linear.weight.detach().numpy()\n",
    "print(weights)\n",
    "print(np.argmax(weights))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature playing the most important role in the classification is the number of words in the document which are in the positive lexicon. This is not surprising as the positive lexicon contains words such as \"good\", \"great\", \"excellent\", etc. which are very indicative of a positive review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    4     6    11 ... 24991 24998 24999]\n",
      "first off let me say  if you haven't enjoyed a van damme movie since bloodsport  you probably will not like this movie  most of these movies may not have the best plots or best actors but i enjoy these kinds of movies for what they are  this movie is much better than any of the movies the other action guys  segal and dolph  have thought about putting out the past few years  van damme is good in the movie  the movie is only worth watching to van damme fans  it is not as good as wake of death  which i highly recommend to anyone of likes van damme  or in hell but  in my opinion it's worth watching  it has the same type of feel to it as nowhere to run  good fun stuff!\n",
      "document          first off let me say  if you haven't enjoyed a...\n",
      "class                                                             0\n",
      "feature_vector                  [0, 6, 1, 4.997212273764115, 12, 2]\n",
      "Name: 4, dtype: object\n",
      "isaac florentine has made some of the best western martial arts action movies ever produced  in particular us seals 2  cold harvest  special forces and undisputed 2 are all action classics  you can tell isaac has a real passion for the genre and his films are always eventful  creative and sharp affairs  with some of the best fight sequences an action fan could hope for  in particular he has found a muse with scott adkins  as talented an actor and action performer as you could hope for  this is borne out with special forces and undisputed 2  but unfortunately the shepherd just doesn't live up to their abilities there is no doubt that jcvd looks better here fight-wise than he has done in years  especially in the fight he has  for pretty much no reason  in a prison cell  and in the final showdown with scott  but look in his eyes  jcvd seems to be dead inside  there's nothing in his eyes at all  it's like he just doesn't care about anything throughout the whole film  and this is the leading man there are other dodgy aspects to the film  script-wise and visually  but the main problem is that you are utterly unable to empathise with the hero of the film  a genuine shame as i know we all wanted this film to be as special as it genuinely could have been  there are some good bits  mostly the action scenes themselves  this film had a terrific director and action choreographer  and an awesome opponent for jcvd to face down  this could have been the one to bring the veteran action star back up to scratch in the balls-out action movie stakes sincerely a shame that this didn't happen \n",
      "document          isaac florentine has made some of the best wes...\n",
      "class                                                             0\n",
      "feature_vector                  [1, 6, 0, 5.75890177387728, 21, 11]\n",
      "Name: 6, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Take two wrongly classified samples in the test set and try explaining why the model was wrong.\n",
    "\n",
    "# Let's look at the wrongly classified samples\n",
    "wrongly_classified = np.where(p_test != y_test.numpy())[0]\n",
    "print(wrongly_classified)\n",
    "\n",
    "# Let's look at the first wrongly classified sample\n",
    "print(preprocessed_test.iloc[wrongly_classified[0]][\"document\"], preprocessed_test.iloc[wrongly_classified[0]], sep=\"\\n\")\n",
    "\n",
    "# Let's look at the second wrongly classified sample\n",
    "print(preprocessed_test.iloc[wrongly_classified[1]][\"document\"], preprocessed_test.iloc[wrongly_classified[1]], sep=\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These wrondly classified examples are mostly positive reviews which are classified as negative. This is surely because they are really hard to classify, mixing positive and negative parts. In the first one: 'if you haven't enjoyed a van damme movie since bloodsport you probably will not like this movie' is negative when read on its own, but positive when read in the context of the whole review. The second one also contains negative or mixed parts inside the whole review, which is very confusing for the classifier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/29 21:51:08 WARN Utils: Your hostname, MacBook-Pro-de-FISCH.local resolves to a loopback address: 127.0.0.1; using 192.168.1.96 instead (on interface en0)\n",
      "23/03/29 21:51:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/29 21:51:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/03/29 21:51:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# let's use pyspark to preprocess the data\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- class: long (nullable = true)\n",
      " |-- document: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/29 21:51:17 WARN TaskSetManager: Stage 0 contains a task of very large size (3893 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/03/29 21:51:22 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 0 (TID 0): Attempting to kill Python Worker\n",
      "+-----+--------------------+\n",
      "|class|            document|\n",
      "+-----+--------------------+\n",
      "|    0|I rented I AM CUR...|\n",
      "|    0|\"I Am Curious: Ye...|\n",
      "|    0|If only to avoid ...|\n",
      "|    0|This film was pro...|\n",
      "|    0|Oh, brother...aft...|\n",
      "|    0|I would put this ...|\n",
      "|    0|Whoever wrote the...|\n",
      "|    0|When I first saw ...|\n",
      "|    0|Who are these \"Th...|\n",
      "|    0|This is said to b...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# use 'dataset' variable and convert it to a spark dataframe\n",
    "spark_df = spark.createDataFrame(dataset[\"train\"], schema=[\"class\", \"document\"])\n",
    "\n",
    "# Let's look at the schema of the dataframe\n",
    "spark_df.printSchema()\n",
    "\n",
    "# Let's look at the first 10 rows of the dataframe\n",
    "spark_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_spark(dataset: pd.DataFrame) -> pd.DataFrame :\n",
    "    \"\"\"\n",
    "    Preprocess the dataset by lowercasing the text and removing the punctuation manually using spark\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : pd.DataFrame\n",
    "        The dataset to preprocess\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The preprocessed dataset\n",
    "    \"\"\"\n",
    "    # First lower the case\n",
    "    dataset = dataset.withColumn(\"document\", F.lower(F.col(\"document\")))\n",
    "    # Replace the punctuation with spaces. We keep the ' - that may give revelant informations\n",
    "    # Replace HTML tag <br />\n",
    "    dataset = dataset.withColumn(\"document\", F.regexp_replace(F.col(\"document\"), r\"[^a-zA-Z0-9'-]\", \" \"))\n",
    "    dataset = dataset.withColumn(\"document\", F.regexp_replace(F.col(\"document\"), r\"<br />\", \" \"))\n",
    "    # Remove the extra spaces\n",
    "    dataset = dataset.withColumn(\"document\", F.regexp_replace(F.col(\"document\"), r\"\\s+\", \" \"))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/29 21:57:27 WARN TaskSetManager: Stage 14 contains a task of very large size (3893 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/29 21:57:31 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 14 (TID 20): Attempting to kill Python Worker\n",
      "+-----+--------------------+\n",
      "|class|            document|\n",
      "+-----+--------------------+\n",
      "|    0|i rented i am cur...|\n",
      "|    0| i am curious yel...|\n",
      "|    0|if only to avoid ...|\n",
      "|    0|this film was pro...|\n",
      "|    0|oh brother after ...|\n",
      "|    0|i would put this ...|\n",
      "|    0|whoever wrote the...|\n",
      "|    0|when i first saw ...|\n",
      "|    0|who are these the...|\n",
      "|    0|this is said to b...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/29 21:57:32 WARN TaskSetManager: Stage 15 contains a task of very large size (3861 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/29 21:57:36 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 15 (TID 21): Attempting to kill Python Worker\n",
      "+-----+--------------------+\n",
      "|class|            document|\n",
      "+-----+--------------------+\n",
      "|    0|i love sci-fi and...|\n",
      "|    0|worth the enterta...|\n",
      "|    0|its a totally ave...|\n",
      "|    0|star rating satur...|\n",
      "|    0|first off let me ...|\n",
      "|    0|i had high hopes ...|\n",
      "|    0|isaac florentine ...|\n",
      "|    0|it actually pains...|\n",
      "|    0|technically i'am ...|\n",
      "|    0|honestly awful fi...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "preprocessed_train_spark = preprocess_spark(spark_df)\n",
    "preprocessed_train_spark.show(10)\n",
    "\n",
    "preprocessed_test_spark = preprocess_spark(spark.createDataFrame(dataset[\"test\"], schema=[\"class\", \"document\"]))\n",
    "preprocessed_test_spark.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|   Token|Score|\n",
      "+--------+-----+\n",
      "|      $:| -1.5|\n",
      "|     %-)| -1.5|\n",
      "|( '}{' )|  1.6|\n",
      "|    ('-:|  2.2|\n",
      "|     (':|  2.3|\n",
      "|    ((-:|  2.1|\n",
      "|      (*|  1.1|\n",
      "|     (-*|  1.3|\n",
      "|     (-:|  1.6|\n",
      "|    (-:0|  2.8|\n",
      "+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"( '}{' )\",\n",
       " \"('-:\",\n",
       " \"(':\",\n",
       " '((-:',\n",
       " '(*',\n",
       " '(-*',\n",
       " '(-:',\n",
       " '(-:0',\n",
       " '(-:o',\n",
       " '(-:O',\n",
       " '(-:|>*',\n",
       " '(-;',\n",
       " '(-;|',\n",
       " '(8',\n",
       " '(:',\n",
       " '(:0',\n",
       " '(:o',\n",
       " '(:O',\n",
       " '(;',\n",
       " '(=',\n",
       " '(?:',\n",
       " '(^:',\n",
       " '(^;',\n",
       " '(^;0',\n",
       " '(^;o',\n",
       " '(o:',\n",
       " '*-:',\n",
       " '*-;',\n",
       " '*:',\n",
       " '*<|:-)',\n",
       " '*\\\\0/*',\n",
       " '*^:',\n",
       " ',-:',\n",
       " \"---'-;-{@\",\n",
       " '--<--<@',\n",
       " '0:)',\n",
       " '0:-)',\n",
       " '0:-3',\n",
       " '0:03',\n",
       " '0;^)',\n",
       " '10q',\n",
       " '1337',\n",
       " '143',\n",
       " '1432',\n",
       " '14aa41',\n",
       " '2g2b4g',\n",
       " '2qt',\n",
       " '5fs',\n",
       " '8)',\n",
       " '8-d',\n",
       " '8d',\n",
       " \":')\",\n",
       " \":'-)\",\n",
       " ':)',\n",
       " ':*',\n",
       " ':-)',\n",
       " ':-))',\n",
       " ':-*',\n",
       " ':-,',\n",
       " ':-d',\n",
       " ':-D',\n",
       " ':-p',\n",
       " ':-p',\n",
       " ':3',\n",
       " ':>',\n",
       " ':?)',\n",
       " ':d',\n",
       " ':D',\n",
       " ':p',\n",
       " ':]',\n",
       " ':^)',\n",
       " ':^*',\n",
       " ':c)',\n",
       " ':o)',\n",
       " ':P',\n",
       " ':}',\n",
       " ':Þ',\n",
       " ';-)',\n",
       " ';-*',\n",
       " ';^)',\n",
       " '<3',\n",
       " '<:',\n",
       " '=)',\n",
       " '=-3',\n",
       " '=-d',\n",
       " '=-D',\n",
       " '=3',\n",
       " '=d',\n",
       " '=D',\n",
       " '=]',\n",
       " '=p',\n",
       " '>:p',\n",
       " '>_>^',\n",
       " '@>-->--',\n",
       " \"@}-;-'---\",\n",
       " 'aas',\n",
       " 'aayf',\n",
       " 'alol',\n",
       " 'ambw',\n",
       " 'aml',\n",
       " 'bff',\n",
       " 'bffn',\n",
       " 'bl',\n",
       " 'b^d',\n",
       " 'ez',\n",
       " 'fav',\n",
       " 'ff',\n",
       " 'foaf',\n",
       " 'ftw',\n",
       " 'fwb',\n",
       " 'g1',\n",
       " 'gg',\n",
       " 'gga',\n",
       " 'gj',\n",
       " 'gl',\n",
       " 'gla',\n",
       " 'gn',\n",
       " 'gr8',\n",
       " 'gt',\n",
       " 'h&k',\n",
       " 'hagd',\n",
       " 'hagn',\n",
       " 'hago',\n",
       " 'hak',\n",
       " 'hand',\n",
       " 'heart',\n",
       " 'hearts',\n",
       " 'hho1/2k',\n",
       " 'hhoj',\n",
       " 'hugz',\n",
       " 'hi5',\n",
       " 'ilu',\n",
       " 'iluaaf',\n",
       " 'ily',\n",
       " 'ily2',\n",
       " 'iyq',\n",
       " 'j/j',\n",
       " 'j/k',\n",
       " 'j/p',\n",
       " 'j/w',\n",
       " 'j4f',\n",
       " 'j4g',\n",
       " 'jhomf',\n",
       " 'jj',\n",
       " 'jw',\n",
       " 'k4y',\n",
       " 'kfy',\n",
       " 'kk',\n",
       " 'kmuf',\n",
       " 'l',\n",
       " 'l&r',\n",
       " 'laoj',\n",
       " 'lmao',\n",
       " 'lmbao',\n",
       " 'lmfao',\n",
       " 'lmso',\n",
       " 'lol',\n",
       " 'lolz',\n",
       " 'lts',\n",
       " 'ly',\n",
       " 'ly4e',\n",
       " 'lya',\n",
       " 'lyb',\n",
       " 'lyl',\n",
       " 'lylab',\n",
       " 'lylas',\n",
       " 'lylb',\n",
       " 'm8',\n",
       " 'mml',\n",
       " 'muah',\n",
       " 'mwah',\n",
       " 'n1',\n",
       " 'nbd',\n",
       " 'nh',\n",
       " 'np',\n",
       " 'ntmu',\n",
       " 'o:)',\n",
       " 'o:-)',\n",
       " 'o:-3',\n",
       " 'o:3',\n",
       " 'o;^)',\n",
       " 'ok',\n",
       " 'ptl',\n",
       " 'qt',\n",
       " 'r&r',\n",
       " 'rofl',\n",
       " 'roflmao',\n",
       " 'rotfl',\n",
       " 'rotflmao',\n",
       " 'rotflmfao',\n",
       " 'rotflol',\n",
       " 'rotgl',\n",
       " 'rotglmao',\n",
       " 'sete',\n",
       " 'sfete',\n",
       " 'sgtm',\n",
       " 'slaw',\n",
       " 'swak',\n",
       " 'tgif',\n",
       " 'thks',\n",
       " 'thx',\n",
       " 'tia',\n",
       " 'tnx',\n",
       " 'true',\n",
       " 'tx',\n",
       " 'txs',\n",
       " 'ty',\n",
       " 'tyvm',\n",
       " 'urw',\n",
       " 'vbg',\n",
       " 'vbs',\n",
       " 'vip',\n",
       " 'vwd',\n",
       " 'vwp',\n",
       " 'wd',\n",
       " 'wp',\n",
       " 'wtg',\n",
       " 'x-d',\n",
       " 'x-p',\n",
       " 'xd',\n",
       " 'xlnt',\n",
       " 'xoxo',\n",
       " 'xoxozzz',\n",
       " 'xp',\n",
       " 'xqzt',\n",
       " 'yolo',\n",
       " 'yvw',\n",
       " 'yw',\n",
       " 'ywia',\n",
       " '[:',\n",
       " '[;',\n",
       " '[=',\n",
       " '\\\\o/',\n",
       " '^<_<',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'absolve',\n",
       " 'absolved',\n",
       " 'absolves',\n",
       " 'absolving',\n",
       " 'accept',\n",
       " 'acceptabilities',\n",
       " 'acceptability',\n",
       " 'acceptable',\n",
       " 'acceptableness',\n",
       " 'acceptably',\n",
       " 'acceptance',\n",
       " 'acceptances',\n",
       " 'acceptant',\n",
       " 'acceptation',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'accepts',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishes',\n",
       " 'achievable',\n",
       " 'acquitted',\n",
       " 'acquitting',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'actives',\n",
       " 'admirability',\n",
       " 'admirable',\n",
       " 'admirableness',\n",
       " 'admirably',\n",
       " 'admiral',\n",
       " 'admirals',\n",
       " 'admiralties',\n",
       " 'admiralty',\n",
       " 'admiration',\n",
       " 'admirations',\n",
       " 'admire',\n",
       " 'admired',\n",
       " 'admirer',\n",
       " 'admirers',\n",
       " 'admires',\n",
       " 'admiring',\n",
       " 'admiringly',\n",
       " 'admits',\n",
       " 'adorability',\n",
       " 'adorable',\n",
       " 'adorableness',\n",
       " 'adorably',\n",
       " 'adoration',\n",
       " 'adorations',\n",
       " 'adore',\n",
       " 'adored',\n",
       " 'adorer',\n",
       " 'adorers',\n",
       " 'adores',\n",
       " 'adoring',\n",
       " 'adoringly',\n",
       " 'adorner',\n",
       " 'adorning',\n",
       " 'adornment',\n",
       " 'advanced',\n",
       " 'advantage',\n",
       " 'advantaged',\n",
       " 'advantageous',\n",
       " 'advantageously',\n",
       " 'advantageousness',\n",
       " 'advantages',\n",
       " 'advantaging',\n",
       " 'adventure',\n",
       " 'adventured',\n",
       " 'adventurer',\n",
       " 'adventures',\n",
       " 'adventuresome',\n",
       " 'adventuresomeness',\n",
       " 'adventuresses',\n",
       " 'adventuring',\n",
       " 'adventurism',\n",
       " 'adventurist',\n",
       " 'adventuristic',\n",
       " 'adventurists',\n",
       " 'adventurous',\n",
       " 'adventurously',\n",
       " 'adventurousness',\n",
       " 'affection',\n",
       " 'affectional',\n",
       " 'affectionally',\n",
       " 'affectionate',\n",
       " 'affectionately',\n",
       " 'affectioned',\n",
       " 'affections',\n",
       " 'agog',\n",
       " 'agree',\n",
       " 'agreeability',\n",
       " 'agreeable',\n",
       " 'agreeableness',\n",
       " 'agreeablenesses',\n",
       " 'agreeably',\n",
       " 'agreed',\n",
       " 'agreeing',\n",
       " 'agreement',\n",
       " 'agreements',\n",
       " 'alert',\n",
       " 'alive',\n",
       " 'alright',\n",
       " 'amaze',\n",
       " 'amazed',\n",
       " 'amazedly',\n",
       " 'amazement',\n",
       " 'amazements',\n",
       " 'amazes',\n",
       " 'amazing',\n",
       " 'amazonstone',\n",
       " 'ambitious',\n",
       " 'amor',\n",
       " 'amorino',\n",
       " 'amorist',\n",
       " 'amoristic',\n",
       " 'amoroso',\n",
       " 'amorous',\n",
       " 'amorously',\n",
       " 'amorousness',\n",
       " 'amuse',\n",
       " 'amused',\n",
       " 'amusedly',\n",
       " 'amusement',\n",
       " 'amusements',\n",
       " 'amuser',\n",
       " 'amusers',\n",
       " 'amuses',\n",
       " 'amusing',\n",
       " 'amusingness',\n",
       " 'amusive',\n",
       " 'aok',\n",
       " 'apologise',\n",
       " 'apologized',\n",
       " 'apologizes',\n",
       " 'appease',\n",
       " 'appeasing',\n",
       " 'applaud',\n",
       " 'applauded',\n",
       " 'applauding',\n",
       " 'applauds',\n",
       " 'applause',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'appreciates',\n",
       " 'appreciating',\n",
       " 'appreciation',\n",
       " 'appreciations',\n",
       " 'appreciative',\n",
       " 'appreciatively',\n",
       " 'appreciativeness',\n",
       " 'appreciator',\n",
       " 'appreciators',\n",
       " 'appreciatory',\n",
       " 'apprehensible',\n",
       " 'approval',\n",
       " 'approved',\n",
       " 'approves',\n",
       " 'ardent',\n",
       " 'asset',\n",
       " 'assurance',\n",
       " 'assurances',\n",
       " 'assure',\n",
       " 'assured',\n",
       " 'assuredly',\n",
       " 'assuredness',\n",
       " 'assurers',\n",
       " 'assures',\n",
       " 'assurgent',\n",
       " 'assuring',\n",
       " 'astonished',\n",
       " 'astound',\n",
       " 'astounded',\n",
       " 'astounding',\n",
       " 'astoundingly',\n",
       " 'astounds',\n",
       " 'attachment',\n",
       " 'attachments',\n",
       " 'attract',\n",
       " 'attractant',\n",
       " 'attractants',\n",
       " 'attracted',\n",
       " 'attracting',\n",
       " 'attraction',\n",
       " 'attractions',\n",
       " 'attractive',\n",
       " 'attractively',\n",
       " 'attractiveness',\n",
       " 'attractivenesses',\n",
       " 'attractor',\n",
       " 'attractors',\n",
       " 'attracts',\n",
       " 'avid',\n",
       " 'award',\n",
       " 'awardable',\n",
       " 'awarded',\n",
       " 'awardee',\n",
       " 'awardees',\n",
       " 'awarders',\n",
       " 'awarding',\n",
       " 'awards',\n",
       " 'awesome',\n",
       " 'badass',\n",
       " 'beatific',\n",
       " 'beaut',\n",
       " 'beauteous',\n",
       " 'beauteously',\n",
       " 'beauteousness',\n",
       " 'beautician',\n",
       " 'beauties',\n",
       " 'beautification',\n",
       " 'beautifications',\n",
       " 'beautified',\n",
       " 'beautifier',\n",
       " 'beautifiers',\n",
       " 'beautifies',\n",
       " 'beautiful',\n",
       " 'beautifuler',\n",
       " 'beautifulest',\n",
       " 'beautifully',\n",
       " 'beautifulness',\n",
       " 'beautify',\n",
       " 'beautifying',\n",
       " 'beauts',\n",
       " 'beauty',\n",
       " 'beloved',\n",
       " 'benefic',\n",
       " 'beneficed',\n",
       " 'beneficence',\n",
       " 'beneficences',\n",
       " 'beneficent',\n",
       " 'beneficently',\n",
       " 'benefices',\n",
       " 'beneficial',\n",
       " 'beneficially',\n",
       " 'beneficialness',\n",
       " 'beneficiaries',\n",
       " 'beneficiary',\n",
       " 'beneficiate',\n",
       " 'benefit',\n",
       " 'benefits',\n",
       " 'benefitted',\n",
       " 'benefitting',\n",
       " 'benevolence',\n",
       " 'benevolences',\n",
       " 'benevolent',\n",
       " 'benevolently',\n",
       " 'benevolentness',\n",
       " 'benign',\n",
       " 'benignant',\n",
       " 'benignantly',\n",
       " 'benignity',\n",
       " 'best',\n",
       " 'better',\n",
       " 'bless',\n",
       " 'blessed',\n",
       " 'blesseder',\n",
       " 'blessedest',\n",
       " 'blessedly',\n",
       " 'blessedness',\n",
       " 'blesser',\n",
       " 'blessers',\n",
       " 'blesses',\n",
       " 'blessing',\n",
       " 'blessings',\n",
       " 'bliss',\n",
       " 'blissful',\n",
       " 'blithe',\n",
       " 'blockbuster',\n",
       " 'bold',\n",
       " 'bolder',\n",
       " 'boldest',\n",
       " 'boldly',\n",
       " 'boldness',\n",
       " 'bolds',\n",
       " 'bonus',\n",
       " 'bonuses',\n",
       " 'boost',\n",
       " 'boosted',\n",
       " 'boosting',\n",
       " 'boosts',\n",
       " 'brave',\n",
       " 'braved',\n",
       " 'bravely',\n",
       " 'braver',\n",
       " 'braveries',\n",
       " 'bravery',\n",
       " 'braves',\n",
       " 'bravest',\n",
       " 'breathtaking',\n",
       " 'bright',\n",
       " 'brighten',\n",
       " 'brightened',\n",
       " 'brightener',\n",
       " 'brighteners',\n",
       " 'brightening',\n",
       " 'brightens',\n",
       " 'brighter',\n",
       " 'brightest',\n",
       " 'brightly',\n",
       " 'brightness',\n",
       " 'brightnesses',\n",
       " 'brightwork',\n",
       " 'brilliance',\n",
       " 'brilliances',\n",
       " 'brilliancies',\n",
       " 'brilliancy',\n",
       " 'brilliant',\n",
       " 'brilliantines',\n",
       " 'brilliantly',\n",
       " 'brilliants',\n",
       " 'bwahahah',\n",
       " 'calm',\n",
       " 'calmative',\n",
       " 'calmed',\n",
       " 'calmer',\n",
       " 'calmest',\n",
       " 'calming',\n",
       " 'calmly',\n",
       " 'calmness',\n",
       " 'calmnesses',\n",
       " 'calms',\n",
       " 'capable',\n",
       " 'captivated',\n",
       " 'care',\n",
       " 'cared',\n",
       " 'carefree',\n",
       " 'carefulness',\n",
       " 'cares',\n",
       " 'caring',\n",
       " 'celebrate',\n",
       " 'celebrated',\n",
       " 'celebrates',\n",
       " 'celebrating',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'certainty',\n",
       " 'champ',\n",
       " 'champagne',\n",
       " 'champed',\n",
       " 'champion',\n",
       " 'championed',\n",
       " 'championing',\n",
       " 'champions',\n",
       " 'championship',\n",
       " 'championships',\n",
       " 'champs',\n",
       " 'champy',\n",
       " 'chance',\n",
       " 'charitable',\n",
       " 'charitableness',\n",
       " 'charitablenesses',\n",
       " 'charitably',\n",
       " 'charities',\n",
       " 'charity',\n",
       " 'charm',\n",
       " 'charmed',\n",
       " 'charmer',\n",
       " 'charmers',\n",
       " 'charming',\n",
       " 'charminger',\n",
       " 'charmingest',\n",
       " 'charmingly',\n",
       " 'charms',\n",
       " 'cheer',\n",
       " 'cheered',\n",
       " 'cheerer',\n",
       " 'cheerers',\n",
       " 'cheerful',\n",
       " 'cheerfuller',\n",
       " 'cheerfullest',\n",
       " 'cheerfully',\n",
       " 'cheerfulness',\n",
       " 'cheerier',\n",
       " 'cheeriest',\n",
       " 'cheerily',\n",
       " 'cheeriness',\n",
       " 'cheering',\n",
       " 'cheerio',\n",
       " 'cheerlead',\n",
       " 'cheerleaders',\n",
       " 'cheerleading',\n",
       " 'cheerleads',\n",
       " 'cheerled',\n",
       " 'cheerly',\n",
       " 'cheers',\n",
       " 'cheery',\n",
       " 'cherish',\n",
       " 'cherishable',\n",
       " 'cherished',\n",
       " 'cherisher',\n",
       " 'cherishers',\n",
       " 'cherishes',\n",
       " 'cherishing',\n",
       " 'chic',\n",
       " 'chuckle',\n",
       " 'chuckled',\n",
       " 'chucklers',\n",
       " 'chuckles',\n",
       " 'chucklesome',\n",
       " 'chuckling',\n",
       " 'chucklingly',\n",
       " 'clarity',\n",
       " 'classy',\n",
       " 'clean',\n",
       " 'clear',\n",
       " 'clearly',\n",
       " 'clever',\n",
       " 'cleverer',\n",
       " 'cleverest',\n",
       " 'cleverish',\n",
       " 'cleverly',\n",
       " 'cleverness',\n",
       " 'clevernesses',\n",
       " 'comedian',\n",
       " 'comedians',\n",
       " 'comedic',\n",
       " 'comedically',\n",
       " 'comediennes',\n",
       " 'comedies',\n",
       " 'comedy',\n",
       " 'comfort',\n",
       " 'comfortable',\n",
       " 'comfortableness',\n",
       " 'comfortably',\n",
       " 'comforted',\n",
       " 'comforter',\n",
       " 'comforters',\n",
       " 'comforting',\n",
       " 'comfortingly',\n",
       " 'comforts',\n",
       " 'commend',\n",
       " 'commended',\n",
       " 'commit',\n",
       " 'commitment',\n",
       " 'committed',\n",
       " 'compassion',\n",
       " 'compassionate',\n",
       " 'compassionated',\n",
       " 'compassionately',\n",
       " 'compassionates',\n",
       " 'compassionating',\n",
       " 'competent',\n",
       " 'compliment',\n",
       " 'complimentarily',\n",
       " 'complimentary',\n",
       " 'complimented',\n",
       " 'complimenting',\n",
       " 'compliments',\n",
       " 'comprehensive',\n",
       " 'conciliate',\n",
       " 'conciliated',\n",
       " 'conciliates',\n",
       " 'conciliating',\n",
       " 'confidence',\n",
       " 'confident',\n",
       " 'confidently',\n",
       " 'congrats',\n",
       " 'congratulate',\n",
       " 'congratulation',\n",
       " 'congratulations',\n",
       " 'consents',\n",
       " 'considerate',\n",
       " 'consolable',\n",
       " 'contented',\n",
       " 'contentedly',\n",
       " 'contentedness',\n",
       " 'contentment',\n",
       " 'convince',\n",
       " 'convinced',\n",
       " 'convincing',\n",
       " 'convincingly',\n",
       " 'convivial',\n",
       " 'cool',\n",
       " 'courage',\n",
       " 'courageous',\n",
       " 'courageously',\n",
       " 'courageousness',\n",
       " 'courteous',\n",
       " 'courtesy',\n",
       " 'coziness',\n",
       " 'create',\n",
       " 'created',\n",
       " 'creates',\n",
       " 'creating',\n",
       " 'creation',\n",
       " 'creationisms',\n",
       " 'creations',\n",
       " 'creative',\n",
       " 'creatively',\n",
       " 'creativeness',\n",
       " 'creativities',\n",
       " 'creativity',\n",
       " 'credit',\n",
       " 'creditabilities',\n",
       " 'creditability',\n",
       " 'creditable',\n",
       " 'creditableness',\n",
       " 'creditably',\n",
       " 'credited',\n",
       " 'credits',\n",
       " 'creditworthiness',\n",
       " 'creditworthy',\n",
       " 'curious',\n",
       " 'cute',\n",
       " 'cutely',\n",
       " 'cuteness',\n",
       " 'cutenesses',\n",
       " 'cuter',\n",
       " 'cutes',\n",
       " 'cutesie',\n",
       " 'cutesier',\n",
       " 'cutesiest',\n",
       " 'cutest',\n",
       " 'cutesy',\n",
       " 'cutey',\n",
       " 'cuteys',\n",
       " 'cutie',\n",
       " 'cutiepie',\n",
       " 'cuties',\n",
       " 'd-:',\n",
       " 'd:',\n",
       " 'd=',\n",
       " 'daring',\n",
       " 'daringly',\n",
       " 'daringness',\n",
       " 'darling',\n",
       " 'darlingly',\n",
       " 'darlingness',\n",
       " 'darlings',\n",
       " 'dauntless',\n",
       " 'dear',\n",
       " 'dearer',\n",
       " 'dearest',\n",
       " 'dearie',\n",
       " 'dearies',\n",
       " 'dearly',\n",
       " 'dearness',\n",
       " 'dears',\n",
       " 'deary',\n",
       " 'dedicated',\n",
       " 'definite',\n",
       " 'definitely',\n",
       " 'delectable',\n",
       " 'delectables',\n",
       " 'delectably',\n",
       " 'delicately',\n",
       " 'delicious',\n",
       " 'deliciously',\n",
       " 'deliciousness',\n",
       " 'delight',\n",
       " 'delighted',\n",
       " 'delightedly',\n",
       " 'delightedness',\n",
       " 'delighter',\n",
       " 'delighters',\n",
       " 'delightful',\n",
       " 'delightfully',\n",
       " 'delightfulness',\n",
       " 'delighting',\n",
       " 'delights',\n",
       " 'delightsome',\n",
       " 'desirable',\n",
       " 'desire',\n",
       " 'desired',\n",
       " 'desirous',\n",
       " 'determinacy',\n",
       " 'determinately',\n",
       " 'determinateness',\n",
       " 'determination',\n",
       " 'determinative',\n",
       " 'determinator',\n",
       " 'determined',\n",
       " 'devote',\n",
       " 'devoted',\n",
       " 'devotedly',\n",
       " 'devotedness',\n",
       " 'devotee',\n",
       " 'devotement',\n",
       " 'devotements',\n",
       " 'devotes',\n",
       " 'devoting',\n",
       " 'devotion',\n",
       " 'devotional',\n",
       " 'devotionally',\n",
       " 'devotionals',\n",
       " 'devotions',\n",
       " 'diamond',\n",
       " 'dignified',\n",
       " 'dignifies',\n",
       " 'dignify',\n",
       " 'dignifying',\n",
       " 'dignitary',\n",
       " 'dignities',\n",
       " 'dignity',\n",
       " 'divination',\n",
       " 'divinations',\n",
       " 'divinatory',\n",
       " 'divine',\n",
       " 'divinely',\n",
       " 'diviners',\n",
       " 'divinest',\n",
       " 'divinities',\n",
       " 'divinity',\n",
       " 'divinize',\n",
       " 'doubtlessly',\n",
       " 'dream',\n",
       " 'dreams',\n",
       " 'dynamic',\n",
       " 'dynamical',\n",
       " 'dynamically',\n",
       " 'dynamics',\n",
       " 'dynamism',\n",
       " 'dynamisms',\n",
       " 'dynamist',\n",
       " 'dynamistic',\n",
       " 'eager',\n",
       " 'eagerly',\n",
       " 'eagerness',\n",
       " 'eagers',\n",
       " 'earnest',\n",
       " 'ease',\n",
       " 'eased',\n",
       " 'easeful',\n",
       " 'easefully',\n",
       " 'easement',\n",
       " 'eases',\n",
       " 'easier',\n",
       " 'easiest',\n",
       " 'easily',\n",
       " 'easiness',\n",
       " 'easing',\n",
       " 'easy',\n",
       " 'easygoing',\n",
       " 'easygoingness',\n",
       " 'ecstacy',\n",
       " 'ecstasies',\n",
       " 'ecstasy',\n",
       " 'ecstatic',\n",
       " 'ecstatically',\n",
       " 'ecstatics',\n",
       " 'effective',\n",
       " 'effectively',\n",
       " 'efficiencies',\n",
       " 'efficiency',\n",
       " 'efficient',\n",
       " 'efficiently',\n",
       " 'elated',\n",
       " 'elation',\n",
       " 'elegance',\n",
       " 'elegances',\n",
       " 'elegancies',\n",
       " 'elegancy',\n",
       " 'elegant',\n",
       " 'elegantly',\n",
       " 'embrace',\n",
       " 'empathetic',\n",
       " 'enchanted',\n",
       " 'encourage',\n",
       " 'encouraged',\n",
       " 'encouragement',\n",
       " 'encouragements',\n",
       " 'encourager',\n",
       " 'encouragers',\n",
       " 'encourages',\n",
       " 'encouraging',\n",
       " 'encouragingly',\n",
       " 'endorse',\n",
       " 'endorsed',\n",
       " 'endorsement',\n",
       " 'endorses',\n",
       " 'energetic',\n",
       " 'energetically',\n",
       " 'energise',\n",
       " 'energised',\n",
       " 'energises',\n",
       " 'energising',\n",
       " 'energization',\n",
       " 'energizations',\n",
       " 'energize',\n",
       " 'energized',\n",
       " 'energizer',\n",
       " 'energizers',\n",
       " 'energizes',\n",
       " 'energizing',\n",
       " 'energy',\n",
       " 'engage',\n",
       " 'engaged',\n",
       " 'engagement',\n",
       " 'engager',\n",
       " 'engagers',\n",
       " 'engages',\n",
       " 'engaging',\n",
       " 'engagingly',\n",
       " 'enjoy',\n",
       " 'enjoyable',\n",
       " 'enjoyableness',\n",
       " 'enjoyably',\n",
       " 'enjoyed',\n",
       " 'enjoyer',\n",
       " 'enjoyers',\n",
       " 'enjoying',\n",
       " 'enjoyment',\n",
       " 'enjoyments',\n",
       " 'enjoys',\n",
       " 'enlighten',\n",
       " 'enlightened',\n",
       " 'enlightening',\n",
       " 'enlightens',\n",
       " 'enrapture',\n",
       " 'ensure',\n",
       " 'ensuring',\n",
       " 'enterprising',\n",
       " 'entertain',\n",
       " 'entertained',\n",
       " 'entertainer',\n",
       " 'entertainers',\n",
       " 'entertaining',\n",
       " 'entertainingly',\n",
       " 'entertainment',\n",
       " 'entertainments',\n",
       " 'entertains',\n",
       " 'enthuse',\n",
       " 'enthused',\n",
       " 'enthuses',\n",
       " 'enthusiasm',\n",
       " 'enthusiasms',\n",
       " 'enthusiast',\n",
       " 'enthusiastic',\n",
       " 'enthusiastically',\n",
       " 'enthusiasts',\n",
       " 'enthusing',\n",
       " 'entitled',\n",
       " 'esteemed',\n",
       " 'ethical',\n",
       " 'euphoria',\n",
       " 'euphoric',\n",
       " 'excel',\n",
       " 'excelled',\n",
       " 'excellence',\n",
       " 'excellences',\n",
       " 'excellencies',\n",
       " 'excellency',\n",
       " 'excellent',\n",
       " 'excellently',\n",
       " 'excelling',\n",
       " 'excels',\n",
       " 'excitabilities',\n",
       " 'excitability',\n",
       " 'excitable',\n",
       " 'excitableness',\n",
       " 'excitant',\n",
       " 'excitants',\n",
       " 'excitation',\n",
       " 'excitations',\n",
       " 'excitatory',\n",
       " 'excite',\n",
       " 'excited',\n",
       " 'excitedly',\n",
       " 'excitement',\n",
       " 'excitements',\n",
       " 'exciter',\n",
       " 'exciters',\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lexicon = pd.read_csv(\"vader_lexicon.txt\", sep=\"\\t\", names=['Token', \"Score\", \"Std\", \"Vector\"]).drop(columns=[\"Std\", \"Vector\"]).set_index(\"Token\")\n",
    "# lexicon = lexicon[(lexicon[\"Score\"] <= -threshold) | (lexicon[\"Score\"] >= threshold)]\n",
    "# convert this code to spark\n",
    "\n",
    "lexicon_spark = spark.read.csv(\"vader_lexicon.txt\", sep=\"\\t\", header=False, inferSchema=True)\n",
    "lexicon_spark = lexicon_spark.withColumnRenamed(\"_c0\", \"Token\").withColumnRenamed(\"_c1\", \"Score\")\n",
    "lexicon_spark = lexicon_spark.drop(\"_c2\", \"_c3\")\n",
    "lexicon_spark = lexicon_spark.filter((F.col(\"Score\") <= -threshold) | (F.col(\"Score\") >= threshold))\n",
    "lexicon_spark.show(10)\n",
    "\n",
    "positive_lexicon_spark = lexicon_spark.filter(F.col(\"Score\") >= threshold)\n",
    "# convert it to a list\n",
    "positive_lexicon_list = positive_lexicon_spark.select(\"Token\").rdd.flatMap(lambda x: x).collect()\n",
    "negative_lexicon_spark = lexicon_spark.filter(F.col(\"Score\") <= -threshold)\n",
    "# convert it to a list\n",
    "negative_lexicon_list = negative_lexicon_spark.select(\"Token\").rdd.flatMap(lambda x: x).collect()\n",
    "positive_lexicon_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the following features using spark:\n",
    "# - 1 if \"no\" appears in the document, 0 otherwise.\n",
    "# - The count of first and second pronouns in the document.\n",
    "# - 1 if \"!\" is in the document, 0 otherwise.\n",
    "# - Log(word count in the document).\n",
    "# - Number of words in the document which are in the positive lexicon.\n",
    "# - Number of words in the document which are in the negative lexicon.\n",
    "# - [Bonus] Add another feature of your choice.\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf(returnType='int')\n",
    "def cnt_no(s):\n",
    "    return s.count(\"no\")\n",
    "\n",
    "@udf(returnType='int')\n",
    "def cnt_pronouns(s):\n",
    "    return s.count('i') + s.count('you') + s.count('we')\n",
    "\n",
    "# @udf(returnType='int')\n",
    "def cnt_lex(tokens):\n",
    "    return udf(lambda s: cate(s, tokens))\n",
    "\n",
    "def cate(s, tokens):\n",
    "    return sum([1 for w in s if w in tokens])\n",
    "\n",
    "def generate_features_spark(df):\n",
    "    \"\"\"\n",
    "    Generate the features for the dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The dataset to generate the features for\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The dataset with the features\n",
    "    \"\"\"\n",
    "    # 1 if \"no\" appears in the document (splited by words), 0 otherwise\n",
    "    # check if \"no\" is in the document\n",
    "    df = df.withColumn(\"no\", cnt_no(F.split(F.col(\"document\"), \" \")))\n",
    "    # The count of first and second pronouns in the document.\n",
    "    df = df.withColumn(\"pronouns\", cnt_pronouns(F.split(F.col(\"document\"), \" \")))\n",
    "    # 1 if \"!\" is in the document, 0 otherwise.\n",
    "    df = df.withColumn(\"exclamation\", F.when(F.col(\"document\").contains(\"!\"), 1).otherwise(0))\n",
    "    # Log(word count in the document).\n",
    "    df = df.withColumn(\"log_word_count\", F.log(F.size(F.array_distinct(F.split(F.col(\"document\"), \" \")))))\n",
    "    # Number of words in the document which are in the positive lexicon (score >= 1 in the lexicon_spark dataframe)\n",
    "    # add the positive lexicon as a column to the dataframe\n",
    "    df = df.withColumn(\"positive_lexicon\", cnt_lex(positive_lexicon_list)(F.split(F.col(\"document\"), \" \")))\n",
    "    # Number of words in the document which are in the negative lexicon.\n",
    "    df = df.withColumn(\"negative_lexicon\", cnt_lex(negative_lexicon_list)(F.split(F.col(\"document\"), \" \")))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/29 21:57:44 WARN TaskSetManager: Stage 16 contains a task of very large size (3893 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---+--------+-----------+------------------+----------------+----------------+\n",
      "|class|            document| no|pronouns|exclamation|    log_word_count|positive_lexicon|negative_lexicon|\n",
      "+-----+--------------------+---+--------+-----------+------------------+----------------+----------------+\n",
      "|    0|i rented i am cur...|  1|       7|          0| 5.187385805840755|               7|               6|\n",
      "|    0| i am curious yel...|  1|       2|          0|5.0369526024136295|               5|               4|\n",
      "|    0|if only to avoid ...|  2|       0|          0|4.2626798770413155|               3|               3|\n",
      "|    0|this film was pro...|  0|       3|          0| 4.477336814478207|               5|               5|\n",
      "|    0|oh brother after ...|  0|      10|          0| 5.267858159063328|               4|              11|\n",
      "|    0|i would put this ...|  0|       3|          0| 4.406719247264253|               3|               7|\n",
      "|    0|whoever wrote the...|  0|       4|          0| 4.465908118654584|               0|               4|\n",
      "|    0|when i first saw ...|  0|       7|          0| 5.056245805348308|              17|               6|\n",
      "|    0|who are these the...|  3|       2|          0| 5.655991810819852|              33|              15|\n",
      "|    0|this is said to b...|  1|       2|          0| 4.948759890378168|              10|               9|\n",
      "+-----+--------------------+---+--------+-----------+------------------+----------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "23/03/29 21:58:27 WARN TaskSetManager: Stage 17 contains a task of very large size (3861 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---+--------+-----------+------------------+----------------+----------------+\n",
      "|class|            document| no|pronouns|exclamation|    log_word_count|positive_lexicon|negative_lexicon|\n",
      "+-----+--------------------+---+--------+-----------+------------------+----------------+----------------+\n",
      "|    0|i love sci-fi and...|  0|       4|          0| 5.062595033026967|               9|               8|\n",
      "|    0|worth the enterta...|  0|       6|          0|5.0106352940962555|              16|              14|\n",
      "|    0|its a totally ave...|  0|       0|          0| 4.406719247264253|               2|               3|\n",
      "|    0|star rating satur...|  1|       1|          0| 5.442417710521793|               9|              18|\n",
      "|    0|first off let me ...|  0|       4|          0| 4.454347296253507|              12|               2|\n",
      "|    0|i had high hopes ...|  2|       1|          0| 4.836281906951478|               8|              11|\n",
      "|    0|isaac florentine ...|  2|       5|          0| 5.081404364984463|              21|              11|\n",
      "|    0|it actually pains...|  0|       4|          0|  4.77912349311153|               6|              15|\n",
      "|    0|technically i'am ...|  0|      10|          0| 4.543294782270004|               5|               6|\n",
      "|    0|honestly awful fi...|  0|       6|          0|4.6913478822291435|               7|              14|\n",
      "+-----+--------------------+---+--------+-----------+------------------+----------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "features_train_spark = generate_features_spark(preprocessed_train_spark)\n",
    "features_train_spark.show(10)\n",
    "\n",
    "features_test_spark = generate_features_spark(preprocessed_test_spark)\n",
    "features_test_spark.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/29 22:02:44 WARN TaskSetManager: Stage 18 contains a task of very large size (3893 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/29 22:04:11 WARN TaskSetManager: Stage 19 contains a task of very large size (3861 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>document</th>\n",
       "      <th>no</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>exclamation</th>\n",
       "      <th>log_word_count</th>\n",
       "      <th>positive_lexicon</th>\n",
       "      <th>negative_lexicon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>i rented i am curious-yellow from my video sto...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5.187386</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>i am curious yellow is a risible and pretenti...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5.036953</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>if only to avoid making this type of film in t...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.262680</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>this film was probably inspired by godard's ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4.477337</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>oh brother after hearing about this ridiculous...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5.267858</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>1</td>\n",
       "      <td>a hit at the time but now better categorised a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.382027</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>1</td>\n",
       "      <td>i love this movie like no other another time i...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4.875197</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>1</td>\n",
       "      <td>this film and it's sequel barry mckenzie holds...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4.574711</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>1</td>\n",
       "      <td>'the adventures of barry mckenzie' started lif...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5.988961</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>1</td>\n",
       "      <td>the story centers around barry mckenzie who mu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.850148</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class                                           document  no  pronouns  \\\n",
       "0          0  i rented i am curious-yellow from my video sto...   1         7   \n",
       "1          0   i am curious yellow is a risible and pretenti...   1         2   \n",
       "2          0  if only to avoid making this type of film in t...   2         0   \n",
       "3          0  this film was probably inspired by godard's ma...   0         3   \n",
       "4          0  oh brother after hearing about this ridiculous...   0        10   \n",
       "...      ...                                                ...  ..       ...   \n",
       "24995      1  a hit at the time but now better categorised a...   0         0   \n",
       "24996      1  i love this movie like no other another time i...   1         6   \n",
       "24997      1  this film and it's sequel barry mckenzie holds...   0         2   \n",
       "24998      1  'the adventures of barry mckenzie' started lif...   0         6   \n",
       "24999      1  the story centers around barry mckenzie who mu...   0         0   \n",
       "\n",
       "       exclamation  log_word_count positive_lexicon negative_lexicon  \n",
       "0                0        5.187386                7                6  \n",
       "1                0        5.036953                5                4  \n",
       "2                0        4.262680                3                3  \n",
       "3                0        4.477337                5                5  \n",
       "4                0        5.267858                4               11  \n",
       "...            ...             ...              ...              ...  \n",
       "24995            0        4.382027                7                3  \n",
       "24996            0        4.875197                8                6  \n",
       "24997            0        4.574711               10                3  \n",
       "24998            0        5.988961               27               12  \n",
       "24999            0        3.850148                2                1  \n",
       "\n",
       "[25000 rows x 8 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the spark dataframe to a pandas dataframe\n",
    "df_pandas_train = features_train_spark.toPandas()\n",
    "df_pandas_test = features_test_spark.toPandas()\n",
    "df_pandas_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas_train[\"feature_vector\"] = df_pandas_train.apply(lambda x: [x[\"no\"], x[\"pronouns\"], x[\"exclamation\"], x[\"log_word_count\"], x[\"positive_lexicon\"], x[\"negative_lexicon\"]], axis=1)\n",
    "df_pandas_train = df_pandas_train.drop(columns=[\"no\", \"pronouns\", \"exclamation\", \"log_word_count\", \"positive_lexicon\", \"negative_lexicon\"])\n",
    "\n",
    "df_pandas_test[\"feature_vector\"] = df_pandas_test.apply(lambda x: [x[\"no\"], x[\"pronouns\"], x[\"exclamation\"], x[\"log_word_count\"], x[\"positive_lexicon\"], x[\"negative_lexicon\"]], axis=1)\n",
    "df_pandas_test = df_pandas_test.drop(columns=[\"no\", \"pronouns\", \"exclamation\", \"log_word_count\", \"positive_lexicon\", \"negative_lexicon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>document</th>\n",
       "      <th>feature_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>i love sci-fi and am willing to put up with a ...</td>\n",
       "      <td>[0, 4, 0, 5.062595033026967, 9, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>worth the entertainment value of a rental espe...</td>\n",
       "      <td>[0, 6, 0, 5.0106352940962555, 16, 14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>its a totally average film with a few semi-alr...</td>\n",
       "      <td>[0, 0, 0, 4.406719247264253, 2, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>star rating saturday night friday night friday...</td>\n",
       "      <td>[1, 1, 0, 5.442417710521793, 9, 18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>first off let me say if you haven't enjoyed a ...</td>\n",
       "      <td>[0, 4, 0, 4.454347296253507, 12, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>1</td>\n",
       "      <td>just got around to seeing monster man yesterda...</td>\n",
       "      <td>[2, 23, 0, 5.87493073085203, 47, 18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>1</td>\n",
       "      <td>i got this as part of a competition prize i wa...</td>\n",
       "      <td>[0, 6, 0, 4.574710978503383, 9, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>1</td>\n",
       "      <td>i got monster man in a box set of three films ...</td>\n",
       "      <td>[0, 2, 0, 4.948759890378168, 13, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>1</td>\n",
       "      <td>five minutes in i started to feel how naff thi...</td>\n",
       "      <td>[1, 9, 0, 5.327876168789581, 20, 14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>1</td>\n",
       "      <td>i caught this movie on the sci-fi channel rece...</td>\n",
       "      <td>[0, 1, 0, 4.624972813284271, 7, 9]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class                                           document  \\\n",
       "0          0  i love sci-fi and am willing to put up with a ...   \n",
       "1          0  worth the entertainment value of a rental espe...   \n",
       "2          0  its a totally average film with a few semi-alr...   \n",
       "3          0  star rating saturday night friday night friday...   \n",
       "4          0  first off let me say if you haven't enjoyed a ...   \n",
       "...      ...                                                ...   \n",
       "24995      1  just got around to seeing monster man yesterda...   \n",
       "24996      1  i got this as part of a competition prize i wa...   \n",
       "24997      1  i got monster man in a box set of three films ...   \n",
       "24998      1  five minutes in i started to feel how naff thi...   \n",
       "24999      1  i caught this movie on the sci-fi channel rece...   \n",
       "\n",
       "                              feature_vector  \n",
       "0         [0, 4, 0, 5.062595033026967, 9, 8]  \n",
       "1      [0, 6, 0, 5.0106352940962555, 16, 14]  \n",
       "2         [0, 0, 0, 4.406719247264253, 2, 3]  \n",
       "3        [1, 1, 0, 5.442417710521793, 9, 18]  \n",
       "4        [0, 4, 0, 4.454347296253507, 12, 2]  \n",
       "...                                      ...  \n",
       "24995   [2, 23, 0, 5.87493073085203, 47, 18]  \n",
       "24996     [0, 6, 0, 4.574710978503383, 9, 5]  \n",
       "24997    [0, 2, 0, 4.948759890378168, 13, 5]  \n",
       "24998   [1, 9, 0, 5.327876168789581, 20, 14]  \n",
       "24999     [0, 1, 0, 4.624972813284271, 7, 9]  \n",
       "\n",
       "[25000 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the rest of the computation as in the previous part\n",
    "# TODO: refactor to have functions instead of repeating the code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
