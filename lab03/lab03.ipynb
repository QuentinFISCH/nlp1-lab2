{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import get_dataset_split_names\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/Users/quentinfisch/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "100%|██████████| 3/3 [00:00<00:00, 92.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "def preprocess(dataset: pd.DataFrame) -> pd.DataFrame :\n",
    "    \"\"\"\n",
    "    Preprocess the dataset by lowercasing the text and removing the punctuation manually\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : pd.DataFrame\n",
    "        The dataset to preprocess\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The preprocessed dataset\n",
    "    \"\"\"\n",
    "    # First lower the case\n",
    "    dataset[\"document\"] = dataset[\"document\"].apply(lambda x: x.lower())\n",
    "    # Replace the punctuation with spaces. We keep the ' - that may give revelant informations\n",
    "    # Replace HTML tag <br />\n",
    "    punctuation_to_remove = '|'.join(map(re.escape, sorted(list(filter(lambda p: p != \"'\" and p != '-' and p != \"!\", punctuation)), reverse=True)))\n",
    "    print(f\"Deleting all these punctuation: {punctuation_to_remove}\")\n",
    "    dataset[\"document\"] = dataset[\"document\"].apply(lambda x: re.sub(punctuation_to_remove, \" \", x.replace('<br />', \"\")))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.DataFrame(dataset[\"train\"], columns=[\"text\", \"label\"]).rename(columns={\"text\": \"document\", \"label\": \"class\"})\n",
    "preprocessed_train = preprocess(train_raw)\n",
    "preprocessed_train\n",
    "\n",
    "test_raw = pd.DataFrame(dataset[\"test\"], columns=[\"text\", \"label\"]).rename(columns={\"text\": \"document\", \"label\": \"class\"})\n",
    "preprocessed_test = preprocess(test_raw)\n",
    "preprocessed_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load lexicon and keep only interesting tokens (one above the treshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>$:</th>\n",
       "      <td>-1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>%-)</th>\n",
       "      <td>-1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>( '}{' )</th>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('-:</th>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(':</th>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>|^:</th>\n",
       "      <td>-1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>||-:</th>\n",
       "      <td>-2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>}:</th>\n",
       "      <td>-2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>}:(</th>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>}:-(</th>\n",
       "      <td>-2.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5830 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Score\n",
       "Token          \n",
       "$:         -1.5\n",
       "%-)        -1.5\n",
       "( '}{' )    1.6\n",
       "('-:        2.2\n",
       "(':         2.3\n",
       "...         ...\n",
       "|^:        -1.1\n",
       "||-:       -2.3\n",
       "}:         -2.1\n",
       "}:(        -2.0\n",
       "}:-(       -2.1\n",
       "\n",
       "[5830 rows x 1 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treshhold = 1\n",
    "lexicon = pd.read_csv(\"vader_lexicon.txt\", sep=\"\\t\", names=['Token', \"Score\", \"Std\", \"Vector\"]).drop(columns=[\"Std\", \"Vector\"]).set_index(\"Token\")\n",
    "lexicon = lexicon[(lexicon[\"Score\"] <= -treshhold) | (lexicon[\"Score\"] >= treshhold)]\n",
    "lexicon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the following features:\n",
    "- 1 if \"no\" appears in the document, 0 otherwise.\n",
    "- The count of first and second pronouns in the document.\n",
    "- 1 if \"!\" is in the document, 0 otherwise.\n",
    "- Log(word count in the document).\n",
    "- Number of words in the document which are in the positive lexicon.\n",
    "- Number of words in the document which are in the negative lexicon.\n",
    "- [Bonus] Add another feature of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the following features:\n",
    "# - 1 if \"no\" appears in the document, 0 otherwise.\n",
    "# - The count of first and second pronouns in the document.\n",
    "# - 1 if \"!\" is in the document, 0 otherwise.\n",
    "# - Log(word count in the document).\n",
    "# - Number of words in the document which are in the positive lexicon.\n",
    "# - Number of words in the document which are in the negative lexicon.\n",
    "# - [Bonus] Add another feature of your choice.\n",
    "\n",
    "def generate_features(dataset: pd.DataFrame) -> pd.DataFrame :\n",
    "    \"\"\"\n",
    "    Generate the features for the dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : pd.DataFrame\n",
    "        The dataset to generate the features for\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The dataset with the features\n",
    "    \"\"\"\n",
    "    # First lower the case\n",
    "    dataset[\"no\"] = dataset[\"document\"].apply(lambda x: 1 if \"no\" in x.split(\" \") else 0)\n",
    "    dataset[\"pronouns\"] = dataset[\"document\"].apply(lambda x: x.count(\"i\") + x.count(\"me\") + x.count(\"my\") + x.count(\"mine\") + x.count(\"we\") + x.count(\"us\") + x.count(\"our\") + x.count(\"ours\") + x.count(\"you\") + x.count(\"your\") + x.count(\"yours\") + x.count(\"u\") + x.count(\"ur\") + x.count(\"urs\"))\n",
    "    dataset[\"exclamation\"] = dataset[\"document\"].apply(lambda x: 1 if \"!\" in x else 0)\n",
    "    dataset[\"log_word_count\"] = dataset[\"document\"].apply(lambda x: np.log(len(x.split(\" \"))))\n",
    "    dataset[\"positive_lexicon\"] = dataset[\"document\"].apply(lambda x: len(list(filter(lambda w: w in lexicon.index and lexicon.at[w, \"Score\"] >= 1, x.split(\" \")))))\n",
    "    dataset[\"negative_lexicon\"] = dataset[\"document\"].apply(lambda x: len(list(filter(lambda w: w in lexicon.index and lexicon.at[w, \"Score\"] <= -1, x.split(\" \")))))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m features_train \u001b[39m=\u001b[39m generate_features(preprocessed_train)\n",
      "Cell \u001b[0;32mIn[42], line 29\u001b[0m, in \u001b[0;36mgenerate_features\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     27\u001b[0m dataset[\u001b[39m\"\u001b[39m\u001b[39mexclamation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m!\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m x \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m)\n\u001b[1;32m     28\u001b[0m dataset[\u001b[39m\"\u001b[39m\u001b[39mlog_word_count\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: np\u001b[39m.\u001b[39mlog(\u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m))))\n\u001b[0;32m---> 29\u001b[0m dataset[\u001b[39m\"\u001b[39m\u001b[39mpositive_lexicon\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39;49m\u001b[39mdocument\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: \u001b[39mlen\u001b[39;49m(\u001b[39mlist\u001b[39;49m(\u001b[39mfilter\u001b[39;49m(\u001b[39mlambda\u001b[39;49;00m w: w \u001b[39min\u001b[39;49;00m lexicon\u001b[39m.\u001b[39;49mindex \u001b[39mand\u001b[39;49;00m lexicon\u001b[39m.\u001b[39;49mat[w, \u001b[39m\"\u001b[39;49m\u001b[39mScore\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m>\u001b[39;49m\u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, x\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m)))))\n\u001b[1;32m     30\u001b[0m dataset[\u001b[39m\"\u001b[39m\u001b[39mnegative_lexicon\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(\u001b[39mfilter\u001b[39m(\u001b[39mlambda\u001b[39;00m w: w \u001b[39min\u001b[39;00m lexicon\u001b[39m.\u001b[39mindex \u001b[39mand\u001b[39;00m lexicon\u001b[39m.\u001b[39mat[w, \u001b[39m\"\u001b[39m\u001b[39mScore\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, x\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)))))\n\u001b[1;32m     31\u001b[0m \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1175\u001b[0m             values,\n\u001b[1;32m   1176\u001b[0m             f,\n\u001b[1;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1178\u001b[0m         )\n\u001b[1;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[42], line 29\u001b[0m, in \u001b[0;36mgenerate_features.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     27\u001b[0m dataset[\u001b[39m\"\u001b[39m\u001b[39mexclamation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m!\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m x \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m)\n\u001b[1;32m     28\u001b[0m dataset[\u001b[39m\"\u001b[39m\u001b[39mlog_word_count\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: np\u001b[39m.\u001b[39mlog(\u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m))))\n\u001b[0;32m---> 29\u001b[0m dataset[\u001b[39m\"\u001b[39m\u001b[39mpositive_lexicon\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39;49m(\u001b[39mfilter\u001b[39;49m(\u001b[39mlambda\u001b[39;49;00m w: w \u001b[39min\u001b[39;49;00m lexicon\u001b[39m.\u001b[39;49mindex \u001b[39mand\u001b[39;49;00m lexicon\u001b[39m.\u001b[39;49mat[w, \u001b[39m\"\u001b[39;49m\u001b[39mScore\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m>\u001b[39;49m\u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, x\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m)))))\n\u001b[1;32m     30\u001b[0m dataset[\u001b[39m\"\u001b[39m\u001b[39mnegative_lexicon\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(\u001b[39mfilter\u001b[39m(\u001b[39mlambda\u001b[39;00m w: w \u001b[39min\u001b[39;00m lexicon\u001b[39m.\u001b[39mindex \u001b[39mand\u001b[39;00m lexicon\u001b[39m.\u001b[39mat[w, \u001b[39m\"\u001b[39m\u001b[39mScore\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, x\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)))))\n\u001b[1;32m     31\u001b[0m \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages/pandas/core/generic.py:1527\u001b[0m, in \u001b[0;36mNDFrame.__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[1;32m   1526\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__nonzero__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NoReturn:\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1528\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe truth value of a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m is ambiguous. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1529\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUse a.empty, a.bool(), a.item(), a.any() or a.all().\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1530\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "features_train = generate_features(preprocessed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = \"no\"\n",
    "w in lexicon.index and lexicon.loc[w, \"Score\"] >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = preprocessed_train.iloc[0][\"document\"]\n",
    "len(list(filter(lambda w: w in lexicon.index and lexicon.loc[w, \"Score\"] >= 1, sentence.split(\" \"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
