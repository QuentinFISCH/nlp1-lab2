{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## PyTorch tutorial copy paste"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","from torchtext.datasets import multi30k, Multi30k\n","from typing import Iterable, List\n","\n","\n","# We need to modify the URLs for the dataset since the links to the original dataset are broken\n","# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n","multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n","multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n","\n","SRC_LANGUAGE = 'de'\n","TGT_LANGUAGE = 'en'\n","\n","# Place-holders\n","token_transform = {}\n","vocab_transform = {}"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: spacy in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (3.5.3)\n","Requirement already satisfied: sacrebleu in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (2.3.1)\n","Requirement already satisfied: torchdata in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (0.6.1)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy) (1.1.1)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy) (1.0.9)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy) (0.7.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy) (2.0.8)\n","Requirement already satisfied: jinja2 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy) (3.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy) (2.4.6)\n","Requirement already satisfied: pathy>=0.10.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy) (0.10.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy) (2.28.2)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy) (1.0.4)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy) (1.10.6)\n","Requirement already satisfied: packaging>=20.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy) (23.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy) (2.0.7)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy) (3.3.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy) (4.65.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy) (6.3.0)\n","Requirement already satisfied: setuptools in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy) (67.6.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy) (3.0.12)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy) (8.1.9)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy) (3.0.8)\n","Requirement already satisfied: numpy>=1.15.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy) (1.24.2)\n","Requirement already satisfied: regex in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from sacrebleu) (2022.10.31)\n","Requirement already satisfied: tabulate>=0.8.9 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from sacrebleu) (0.9.0)\n","Requirement already satisfied: portalocker in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from sacrebleu) (2.7.0)\n","Requirement already satisfied: colorama in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from sacrebleu) (0.4.6)\n","Requirement already satisfied: lxml in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from sacrebleu) (4.9.2)\n","Requirement already satisfied: urllib3>=1.25 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from torchdata) (1.26.15)\n","Requirement already satisfied: torch==2.0.1 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from torchdata) (2.0.1)\n","Requirement already satisfied: typing-extensions in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from torch==2.0.1->torchdata) (4.5.0)\n","Requirement already satisfied: networkx in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from torch==2.0.1->torchdata) (3.0)\n","Requirement already satisfied: sympy in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from torch==2.0.1->torchdata) (1.11.1)\n","Requirement already satisfied: filelock in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from torch==2.0.1->torchdata) (3.10.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from jinja2->spacy) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from sympy->torch==2.0.1->torchdata) (1.3.0)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","Note: you may need to restart the kernel to use updated packages.\n","Collecting en-core-web-sm==3.5.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from en-core-web-sm==3.5.0) (3.5.3)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.2)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n","Requirement already satisfied: setuptools in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.6.0)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n","Requirement already satisfied: pathy>=0.10.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n","Requirement already satisfied: packaging>=20.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n","Requirement already satisfied: jinja2 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n","Requirement already satisfied: numpy>=1.15.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.2)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.6)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","Collecting de-core-news-sm==3.5.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.5.0/de_core_news_sm-3.5.0-py3-none-any.whl (14.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from de-core-news-sm==3.5.0) (3.5.3)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.8)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.1.1)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (6.3.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.3.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.7)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.9)\n","Requirement already satisfied: numpy>=1.15.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.24.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.28.2)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.65.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.4)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.7.0)\n","Requirement already satisfied: pathy>=0.10.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.10.1)\n","Requirement already satisfied: packaging>=20.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (23.0)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.1.9)\n","Requirement already satisfied: jinja2 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.1.2)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.8)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.4.6)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.10.6)\n","Requirement already satisfied: setuptools in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (67.6.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.1.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2022.12.7)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.0.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.7.9)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.1.2)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('de_core_news_sm')\n"]}],"source":["%pip install spacy sacrebleu torchdata -U\n","!python -m spacy download en_core_web_sm\n","!python -m spacy download de_core_news_sm"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n","token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n","\n","\n","# helper function to yield list of tokens\n","def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n","    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n","\n","    for data_sample in data_iter:\n","        yield token_transform[language](data_sample[language_index[language]])\n","\n","# Define special symbols and indices\n","UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n","# Make sure the tokens are in order of their indices to properly insert them in vocab\n","special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n","\n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","    # Training data Iterator\n","    train_iter = list(Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE)))\n","    # Create torchtext's Vocab object\n","    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n","                                                    min_freq=1,\n","                                                    specials=special_symbols,\n","                                                    special_first=True)\n","\n","# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n","# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","  vocab_transform[ln].set_default_index(UNK_IDX)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["from torch import Tensor\n","import torch\n","import torch.nn as nn\n","from torch.nn import Transformer\n","import math\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n","class PositionalEncoding(nn.Module):\n","    def __init__(self,\n","                 emb_size: int,\n","                 dropout: float,\n","                 maxlen: int = 5000):\n","        super(PositionalEncoding, self).__init__()\n","        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n","        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n","        pos_embedding = torch.zeros((maxlen, emb_size))\n","        pos_embedding[:, 0::2] = torch.sin(pos * den)\n","        pos_embedding[:, 1::2] = torch.cos(pos * den)\n","        pos_embedding = pos_embedding.unsqueeze(-2)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer('pos_embedding', pos_embedding)\n","\n","    def forward(self, token_embedding: Tensor):\n","        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n","\n","# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n","class TokenEmbedding(nn.Module):\n","    def __init__(self, vocab_size: int, emb_size):\n","        super(TokenEmbedding, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, emb_size)\n","        self.emb_size = emb_size\n","\n","    def forward(self, tokens: Tensor):\n","        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n","\n","# Seq2Seq Network\n","class Seq2SeqTransformer(nn.Module):\n","    def __init__(self,\n","                 num_encoder_layers: int,\n","                 num_decoder_layers: int,\n","                 emb_size: int,\n","                 nhead: int,\n","                 src_vocab_size: int,\n","                 tgt_vocab_size: int,\n","                 dim_feedforward: int = 512,\n","                 dropout: float = 0.1):\n","        super(Seq2SeqTransformer, self).__init__()\n","        self.transformer = Transformer(d_model=emb_size,\n","                                       nhead=nhead,\n","                                       num_encoder_layers=num_encoder_layers,\n","                                       num_decoder_layers=num_decoder_layers,\n","                                       dim_feedforward=dim_feedforward,\n","                                       dropout=dropout)\n","        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n","        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n","        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n","        self.positional_encoding = PositionalEncoding(\n","            emb_size, dropout=dropout)\n","\n","    def forward(self,\n","                src: Tensor,\n","                trg: Tensor,\n","                src_mask: Tensor,\n","                tgt_mask: Tensor,\n","                src_padding_mask: Tensor,\n","                tgt_padding_mask: Tensor,\n","                memory_key_padding_mask: Tensor):\n","        src_emb = self.positional_encoding(self.src_tok_emb(src))\n","        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n","        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n","                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n","        return self.generator(outs)\n","\n","    def encode(self, src: Tensor, src_mask: Tensor):\n","        return self.transformer.encoder(self.positional_encoding(\n","                            self.src_tok_emb(src)), src_mask)\n","\n","    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n","        return self.transformer.decoder(self.positional_encoding(\n","                          self.tgt_tok_emb(tgt)), memory,\n","                          tgt_mask)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["def generate_square_subsequent_mask(sz):\n","    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n","    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","    return mask\n","\n","\n","def create_mask(src, tgt):\n","    src_seq_len = src.shape[0]\n","    tgt_seq_len = tgt.shape[0]\n","\n","    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n","    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n","\n","    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n","    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n","    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["torch.manual_seed(0)\n","\n","SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n","TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n","EMB_SIZE = 512\n","NHEAD = 8\n","FFN_HID_DIM = 512\n","BATCH_SIZE = 128\n","NUM_ENCODER_LAYERS = 3\n","NUM_DECODER_LAYERS = 3\n","\n","transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n","                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n","\n","for p in transformer.parameters():\n","    if p.dim() > 1:\n","        nn.init.xavier_uniform_(p)\n","\n","transformer = transformer.to(DEVICE)\n","\n","loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n","\n","optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["from torch.nn.utils.rnn import pad_sequence\n","\n","# helper function to club together sequential operations\n","def sequential_transforms(*transforms):\n","    def func(txt_input):\n","        for transform in transforms:\n","            txt_input = transform(txt_input)\n","        return txt_input\n","    return func\n","\n","# function to add BOS/EOS and create tensor for input sequence indices\n","def tensor_transform(token_ids: List[int]):\n","    return torch.cat((torch.tensor([BOS_IDX]),\n","                      torch.tensor(token_ids),\n","                      torch.tensor([EOS_IDX])))\n","\n","# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n","text_transform = {}\n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n","                                               vocab_transform[ln], #Numericalization\n","                                               tensor_transform) # Add BOS/EOS and create tensor\n","\n","\n","# function to collate data samples into batch tensors\n","def collate_fn(batch):\n","    src_batch, tgt_batch = [], []\n","    for src_sample, tgt_sample in batch:\n","        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n","        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n","\n","    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n","    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n","    return src_batch, tgt_batch"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","def train_epoch(model, optimizer):\n","    model.train()\n","    losses = 0\n","    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n","    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n","\n","    for src, tgt in train_dataloader:\n","        src = src.to(DEVICE)\n","        tgt = tgt.to(DEVICE)\n","\n","        tgt_input = tgt[:-1, :]\n","\n","        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n","\n","        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n","\n","        optimizer.zero_grad()\n","\n","        tgt_out = tgt[1:, :]\n","        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","        loss.backward()\n","\n","        optimizer.step()\n","        losses += loss.item()\n","\n","    return losses / len(list(train_dataloader))\n","\n","\n","def evaluate(model):\n","    model.eval()\n","    losses = 0\n","\n","    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n","    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n","\n","    for src, tgt in val_dataloader:\n","        src = src.to(DEVICE)\n","        tgt = tgt.to(DEVICE)\n","\n","        tgt_input = tgt[:-1, :]\n","\n","        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n","\n","        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n","\n","        tgt_out = tgt[1:, :]\n","        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","        losses += loss.item()\n","\n","    return losses / len(list(val_dataloader))"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","/Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n","  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1, Train loss: 5.342, Val loss: 4.107, Epoch time = 343.495s\n","Epoch: 2, Train loss: 3.760, Val loss: 3.308, Epoch time = 319.152s\n","Epoch: 3, Train loss: 3.156, Val loss: 2.893, Epoch time = 323.218s\n","Epoch: 4, Train loss: 2.765, Val loss: 2.631, Epoch time = 328.611s\n","Epoch: 5, Train loss: 2.478, Val loss: 2.436, Epoch time = 329.120s\n","Epoch: 6, Train loss: 2.249, Val loss: 2.300, Epoch time = 338.191s\n","Epoch: 7, Train loss: 2.057, Val loss: 2.189, Epoch time = 339.081s\n","Epoch: 8, Train loss: 1.893, Val loss: 2.123, Epoch time = 335.729s\n","Epoch: 9, Train loss: 1.754, Val loss: 2.052, Epoch time = 333.173s\n","Epoch: 10, Train loss: 1.628, Val loss: 2.005, Epoch time = 336.888s\n","Epoch: 11, Train loss: 1.519, Val loss: 1.975, Epoch time = 330.446s\n","Epoch: 12, Train loss: 1.417, Val loss: 1.956, Epoch time = 329.546s\n","Epoch: 13, Train loss: 1.331, Val loss: 1.965, Epoch time = 328.958s\n","Epoch: 14, Train loss: 1.249, Val loss: 1.960, Epoch time = 321.621s\n","Epoch: 15, Train loss: 1.171, Val loss: 1.915, Epoch time = 320.837s\n","Epoch: 16, Train loss: 1.100, Val loss: 1.916, Epoch time = 309.242s\n","Epoch: 17, Train loss: 1.036, Val loss: 1.913, Epoch time = 311.059s\n","Epoch: 18, Train loss: 0.976, Val loss: 1.922, Epoch time = 316.606s\n"]}],"source":["from timeit import default_timer as timer\n","NUM_EPOCHS = 18\n","\n","for epoch in range(1, NUM_EPOCHS+1):\n","    start_time = timer()\n","    train_loss = train_epoch(transformer, optimizer)\n","    end_time = timer()\n","    val_loss = evaluate(transformer)\n","    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n","\n","# function to generate output sequence using greedy algorithm\n","def greedy_decode(model, src, src_mask, max_len, start_symbol):\n","    src = src.to(DEVICE)\n","    src_mask = src_mask.to(DEVICE)\n","\n","    memory = model.encode(src, src_mask)\n","    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n","    for i in range(max_len-1):\n","        memory = memory.to(DEVICE)\n","        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n","                    .type(torch.bool)).to(DEVICE)\n","        out = model.decode(ys, memory, tgt_mask)\n","        out = out.transpose(0, 1)\n","        prob = model.generator(out[:, -1])\n","        _, next_word = torch.max(prob, dim=1)\n","        next_word = next_word.item()\n","\n","        ys = torch.cat([ys,\n","                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n","        if next_word == EOS_IDX:\n","            break\n","    return ys\n","\n","\n","# actual function to translate input sentence into target language\n","def translate(model: torch.nn.Module, src_sentence: str):\n","    model.eval()\n","    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n","    num_tokens = src.shape[0]\n","    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n","    tgt_tokens = greedy_decode(\n","        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n","    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" A group of people stand in front of an igloo . \n"]}],"source":["print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Theoritical questions answer\n","\n","* In the positional encoding, why are we using a combination of sinus and cosinus?\n","* In the Seq2SeqTransformer class,\n","    * What is the parameter nhead for?\n","    * What is the point of the generator?\n","* Describe the goal of the create_mask function. Why does it handle differently the source and target masks?"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### In the positional encoding, why are we using a combination of sinus and cosinus?\n","\n","The positional encoding is a way to encode the position of the words in the sentence. We use sinus and cosinus to encode the position of the words in the sentence. These functions are bounded so the modification of the embedding only a little to not change the nature of it. We combine a sinus and a cosinus because they have different periods, which allows us to have a different encoding for each position, thus avoiding the problem that we could have two words with the same position because of the periodicity of the sinus and cosinus.\n","\n","#### In the Seq2SeqTransformer class, What is the parameter nhead for?\n","\n","The parameter nhead is the number of heads in the multihead attention. It is the number of parallel attention layers. It is used to increase the representational power of the model.\n","\n","#### In the Seq2SeqTransformer class, What is the point of the generator?\n","\n","The generator is a linear layer that takes the output of the decoder and returns the logits. It maps the output of the decoder to the vocabulary.\n","\n","#### Describe the goal of the create_mask function. Why does it handle differently the source and target masks?\n","\n","The goal of the create_mask function is to create a mask for the source and the target. The source mask is used to mask the padding tokens. The target mask is used to mask the padding tokens and the future tokens."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Decoding functions"]},{"cell_type":"code","execution_count":159,"metadata":{},"outputs":[],"source":["def top_k_sampling_with_temperature(model: torch.nn.Module,\n","                                    src: torch.Tensor,\n","                                    src_mask: torch.Tensor,\n","                                    max_len: int,\n","                                    start_symbol: int,\n","                                    k: int,\n","                                    temperature: float) -> torch.Tensor:\n","    \"\"\"\n","    Top K sampling algorithm with temperature\n","\n","    -------\n","    Args:\n","    model: torch.nn.Module\n","        Transformer model\n","    src: torch.Tensor\n","        Source tensor\n","    src_mask: torch.Tensor\n","        Source mask tensor\n","    max_len: int\n","        Maximum length of the output sequence\n","    start_symbol: int\n","        Start symbol\n","    k: int\n","        Top K\n","    temperature: float\n","        Temperature\n","\n","    -------\n","    Returns:\n","    ys: torch.Tensor\n","        Output tensor\n","    \"\"\"\n","    src = src.to(DEVICE)\n","    src_mask = src_mask.to(DEVICE)\n","\n","    memory = model.encode(src, src_mask)\n","    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n","    for i in range(max_len-1):\n","        memory = memory.to(DEVICE)\n","        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n","                    .type(torch.bool)).to(DEVICE)\n","        out = model.decode(ys, memory, tgt_mask)\n","        out = out.transpose(0, 1)\n","        prob = model.generator(out[:, -1])\n","        prob = torch.div(prob, temperature)\n","        prob = torch.softmax(prob, dim=1)\n","        top_k_prob, top_k_idx = torch.topk(prob, k=k, dim=1)\n","        next_word = torch.multinomial(top_k_prob, num_samples=1)\n","        next_word = top_k_idx[torch.arange(top_k_idx.size(0)), next_word.squeeze(1)]\n","        next_word = next_word.item()\n","\n","        ys = torch.cat([ys,\n","                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n","        if next_word == EOS_IDX:\n","            break\n","    return ys\n","\n","def translate_top_k(model: torch.nn.Module, src_sentence: str, k: int, temperature: float) -> str:\n","    \"\"\"\n","    Translate source sentence using top k sampling algorithm with temperature\n","\n","    -------\n","    Args:\n","    model: torch.nn.Module\n","        Transformer model\n","    src_sentence: str\n","        Source sentence\n","    k: int\n","        Top K\n","    temperature: float\n","        Temperature\n","\n","    -------\n","    Returns:\n","    str\n","        The translated sentence\n","    \"\"\"\n","    model.eval()\n","    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n","    num_tokens = src.shape[0]\n","    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n","    tgt_tokens = top_k_sampling_with_temperature(\n","        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, k=k, temperature=temperature).flatten()\n","    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"]},{"cell_type":"code","execution_count":176,"metadata":{},"outputs":[],"source":["def top_p_sampling_with_temperature(model: torch.nn.Module,\n","                                    src: torch.Tensor,\n","                                    src_mask: torch.Tensor,\n","                                    max_len: int,\n","                                    start_symbol: int,\n","                                    p: int,\n","                                    temperature: float) -> torch.Tensor:\n","    \"\"\"\n","    Top P sampling algorithm with temperature\n","\n","    -------\n","    Args:\n","    model: torch.nn.Module\n","        Transformer model\n","    src: torch.Tensor\n","        Source tensor\n","    src_mask: torch.Tensor\n","        Source mask tensor\n","    max_len: int\n","        Maximum length of the output sequence\n","    start_symbol: int\n","        Start symbol\n","    p: int\n","        Top P\n","    temperature: float\n","        Temperature\n","\n","    -------\n","    Returns:\n","    ys: torch.Tensor\n","        Output tensor\n","    \"\"\"\n","    src = src.to(DEVICE)\n","    src_mask = src_mask.to(DEVICE)\n","\n","    memory = model.encode(src, src_mask)\n","    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n","    for i in range(max_len-1):\n","        memory = memory.to(DEVICE)\n","        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n","                    .type(torch.bool)).to(DEVICE)\n","        out = model.decode(ys, memory, tgt_mask)\n","        out = out.transpose(0, 1)\n","        prob = model.generator(out[:, -1])\n","        prob = torch.div(prob, temperature)\n","        prob = torch.softmax(prob, dim=1)\n","        sorted_prob, sorted_idx = torch.sort(prob, descending=True, dim=1)\n","        cumulative_prob = torch.cumsum(sorted_prob, dim=1)\n","        sorted_idx_to_remove = cumulative_prob > p\n","        sorted_idx_to_remove[:, 1:] = sorted_idx_to_remove[:, :-1].clone()\n","        sorted_idx_to_remove[:, 0] = 0\n","        sorted_idx_to_remove = sorted_idx_to_remove.type(torch.bool)\n","        sorted_idx[sorted_idx_to_remove] = -1\n","        sorted_idx = sorted_idx[sorted_idx != -1]\n","        sorted_idx = sorted_idx.type(torch.float)\n","        next_word = torch.multinomial(sorted_idx, num_samples=1)\n","        next_word = sorted_idx[next_word]\n","        next_word = next_word.item()\n","\n","        ys = torch.cat([ys,\n","                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n","        if next_word == EOS_IDX:\n","            break\n","    return ys\n","\n","def translate_top_p(model: torch.nn.Module, src_sentence: str, p: int, temperature: float) -> str:\n","    \"\"\"\n","    Translate source sentence using top p sampling algorithm with temperature\n","\n","    -------\n","    Args:\n","    model: torch.nn.Module\n","        Transformer model\n","    src_sentence: str\n","        Source sentence\n","    p: int\n","        Top P\n","    temperature: float\n","        Temperature\n","\n","    -------\n","    Returns:\n","    str\n","        The translated sentence\n","    \"\"\"\n","    model.eval()\n","    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n","    num_tokens = src.shape[0]\n","    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n","    tgt_tokens = top_p_sampling_with_temperature(\n","        model, src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, p=p, temperature=temperature).flatten()\n","    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"]},{"cell_type":"code","execution_count":161,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Greedy translate:\n"," A group of people stand in front of an igloo . \n","\n","Top K sampling with temperature (k=5, temperature 1.0, 0.5 and 0.1):\n"," A group of people stand in front of an igloo \n"," A group of people stand in front of an igloo . \n"," A group of people stand in front of an igloo . \n","\n","Top P sampling with temperature (p=0.9, temperature 1.0, 0.5 and 0.1):\n"," A group of people standing in front an pledge Artists . \n"," A group of people stand in front of an overpass . \n"," A group of people stand in front of an igloo . \n","\n","Top K sampling with temperature (k=10, temperature 1.0, 0.5 and 0.1):\n"," A group of people stand in front of an igloo . \n"," A group of people stand in front of an igloo . \n"," A group of people stand in front of an igloo . \n","\n","Top P sampling with temperature (p=0.5, temperature 1.0, 0.5 and 0.1):\n"," A group of people stand in front of an Ohio calligraphy . \n"," A group of people stand in front of an igloo . \n"," A group of people stand in front of an igloo . \n","\n","Top K sampling with temperature (k=50, temperature 1.0, 0.5 and 0.1):\n"," A group of people stand in front of an auditorium . \n"," A group of people stand in front of an olive . \n"," A group of people standing in front of an igloo . \n","\n","Top K sampling with temperature (k=100, temperature 1.0, 0.5 and 0.1):\n"," A group of people stand in front of an unidentified room . \n"," A group of people stand in front of an igloo . \n"," A group of people stand in front of an igloo . \n"]}],"source":["print(\"Greedy translate:\")\n","print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))\n","\n","print(\"\\nTop K sampling with temperature (k=5, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=5, temperature=1.0))\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=5, temperature=0.5))\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=5, temperature=0.1))\n","\n","print(\"\\nTop P sampling with temperature (p=0.9, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p=0.9, temperature=1.0))\n","print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p=0.9, temperature=0.5))\n","print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p=0.9, temperature=0.1))\n","\n","print(\"\\nTop K sampling with temperature (k=10, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=10, temperature=1.0))\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=10, temperature=0.5))\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=10, temperature=0.1))\n","\n","print(\"\\nTop P sampling with temperature (p=0.5, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p=0.5, temperature=1.0))\n","print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p=0.5, temperature=0.5))\n","print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p=0.5, temperature=0.1))\n","\n","print(\"\\nTop K sampling with temperature (k=50, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=50, temperature=1.0))\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=50, temperature=0.5))\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=50, temperature=0.1))\n","\n","print(\"\\nTop K sampling with temperature (k=100, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=100, temperature=1.0))\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=100, temperature=0.5))\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=100, temperature=0.1))"]},{"cell_type":"code","execution_count":162,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Greedy translate:\n"," While the man in a kitchen is sitting in the living room . \n","\n","Top K sampling with temperature (k=5, temperature 1.0, 0.5 and 0.1):\n"," While sitting in the kitchen in a kitchen , the woman in a living room . \n"," The man in the kitchen is sitting in the living room . \n"," While sitting in the kitchen in a kitchen with the woman in her living room . \n","\n","Top P sampling with temperature (p=0.9, temperature 1.0, 0.5 and 0.1):\n"," Skiers in the kitchen in the kitchen sitting in a warehouse . \n"," While seated in the kitchen , the man sits in a living room . \n"," While sitting in the kitchen in a living room . \n","\n","Top K sampling with temperature (k=10, temperature 1.0, 0.5 and 0.1):\n"," The man in the kitchen is sitting in a room . \n"," A man in a kitchen is sitting in the living room . \n"," The man in the kitchen is sitting in the living room . \n","\n","Top P sampling with temperature (p=0.5, temperature 1.0, 0.5 and 0.1):\n"," While sitting in the kitchen in a living room . \n"," While sitting in the kitchen in a living room . \n"," While the man in a kitchen is sitting in the living room . \n","\n","Top K sampling with temperature (k=50, temperature 1.0, 0.5 and 0.1):\n"," The Man in the kitchen is sitting in a living room . \n"," While sitting in the kitchen in a kitchen with the woman in his living room . \n"," While the man in a kitchen is sitting in the living room . \n","\n","Top K sampling with temperature (k=100, temperature 1.0, 0.5 and 0.1):\n"," On the black kitchen , the man in a kitchen in the living room . \n"," While seated in the kitchen in a kitchen with the woman in his living room . \n"," The man in the kitchen is sitting in the living room . \n"]}],"source":["print(\"Greedy translate:\")\n","print(translate(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\"))\n","\n","print(\"\\nTop K sampling with temperature (k=5, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_k(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", k=5, temperature=1.0))\n","print(translate_top_k(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", k=5, temperature=0.5))\n","print(translate_top_k(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", k=5, temperature=0.1))\n","\n","print(\"\\nTop P sampling with temperature (p=0.9, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_p(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", p=0.9, temperature=1.0))\n","print(translate_top_p(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", p=0.9, temperature=0.5))\n","print(translate_top_p(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", p=0.9, temperature=0.1))\n","\n","print(\"\\nTop K sampling with temperature (k=10, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_k(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", k=10, temperature=1.0))\n","print(translate_top_k(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", k=10, temperature=0.5))\n","print(translate_top_k(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", k=10, temperature=0.1))\n","\n","print(\"\\nTop P sampling with temperature (p=0.5, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_p(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", p=0.5, temperature=1.0))\n","print(translate_top_p(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", p=0.5, temperature=0.5))\n","print(translate_top_p(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", p=0.5, temperature=0.1))\n","\n","print(\"\\nTop K sampling with temperature (k=50, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_k(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", k=50, temperature=1.0))\n","print(translate_top_k(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", k=50, temperature=0.5))\n","print(translate_top_k(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", k=50, temperature=0.1))\n","\n","print(\"\\nTop K sampling with temperature (k=100, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_k(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", k=100, temperature=1))\n","print(translate_top_k(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", k=10, temperature=0.5))\n","print(translate_top_k(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", k=100, temperature=0.1))"]},{"cell_type":"code","execution_count":163,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Greedy translate:\n"," The woman walks in the woods and picking up books . \n","\n","Top K sampling with temperature (k=5, temperature 1.0, 0.5 and 0.1):\n"," The woman is walking in the forest and gathering . \n"," The lady walks in the forest and picking up lunch . \n"," The woman walks in the woods and picking up books . \n","\n","Top P sampling with temperature (p=0.9, temperature 1.0, 0.5 and 0.1):\n"," The lady walks in the woods strums everywhere . \n"," The woman walks in the forest examining properly time . \n"," The woman walks in the forest and picking up books . \n","\n","Top K sampling with temperature (k=10, temperature 1.0, 0.5 and 0.1):\n"," The woman walks in the woods while picking up him . \n"," The woman is walking in the woods while picking time . \n"," The woman walks in the woods and picking up sale . \n","\n","Top P sampling with temperature (p=0.5, temperature 1.0, 0.5 and 0.1):\n"," The woman walks in the woods gestures and wonderful . \n"," The woman walks in the woods and picking up customers . \n"," The woman walks in the woods and picking up books . \n","\n","Top K sampling with temperature (k=50, temperature 1.0, 0.5 and 0.1):\n"," The woman walks in the forest as he picks books . \n"," The woman walks in the woods as if picking up . \n"," The woman walks in the woods and picking up books . \n","\n","Top K sampling with temperature (k=100, temperature 1.0, 0.5 and 0.1):\n"," The lady walks in the woods with steam and figures . \n"," The woman is walking in the forest and picking up books . \n"," The woman walks in the woods and picking up books . \n"]}],"source":["print(\"Greedy translate:\")\n","print(translate(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\"))\n","\n","print(\"\\nTop K sampling with temperature (k=5, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_k(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", k=5, temperature=1.0))\n","print(translate_top_k(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", k=5, temperature=0.5))\n","print(translate_top_k(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", k=5, temperature=0.1))\n","\n","print(\"\\nTop P sampling with temperature (p=0.9, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_p(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", p=0.9, temperature=1.0))\n","print(translate_top_p(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", p=0.9, temperature=0.5))\n","print(translate_top_p(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", p=0.9, temperature=0.1))\n","\n","print(\"\\nTop K sampling with temperature (k=10, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_k(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", k=10, temperature=1.0))\n","print(translate_top_k(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", k=10, temperature=0.5))\n","print(translate_top_k(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", k=10, temperature=0.1))\n","\n","print(\"\\nTop P sampling with temperature (p=0.5, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_p(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", p=0.5, temperature=1.0))\n","print(translate_top_p(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", p=0.5, temperature=0.5))\n","print(translate_top_p(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", p=0.5, temperature=0.1))\n","\n","print(\"\\nTop K sampling with temperature (k=50, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_k(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", k=50, temperature=1.0))\n","print(translate_top_k(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", k=50, temperature=0.5))\n","print(translate_top_k(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", k=50, temperature=0.1))\n","\n","print(\"\\nTop K sampling with temperature (k=100, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_k(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", k=100, temperature=1))\n","print(translate_top_k(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", k=10, temperature=0.5))\n","print(translate_top_k(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", k=100, temperature=0.1))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Compute the BLEU score of the model"]},{"cell_type":"code","execution_count":171,"metadata":{},"outputs":[],"source":["from sacrebleu import corpus_bleu, BLEU\n","\n","def compute_bleu(model: torch.nn.Module,\n","                 data_loader: torch.utils.data.DataLoader,\n","                 translate_method=translate,\n","                 params = {}) -> BLEU:\n","    \"\"\"\n","    Compute the sentence-level BLEU score\n","\n","    -------\n","    Args:\n","    model: torch.nn.Module\n","        Transformer model\n","    data_loader: torch.utils.data.DataLoader\n","        DataLoader for the dataset\n","    translate_method: Callable\n","        Translation function\n","    params: dict\n","        Parameters for the translation function\n","\n","    -------\n","    Returns:\n","    BLEUScore\n","        Sentence-level BLEU score\n","    \"\"\"\n","    model.eval()\n","    refs = []\n","    preds = []\n","    with torch.no_grad():\n","        for src, tgt in data_loader:\n","            translated_sentence = translate_method(model, src, **params)\n","            refs.append(tgt)\n","            preds.append(translated_sentence)\n","    return corpus_bleu(preds, refs)"]},{"cell_type":"code","execution_count":174,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["BLEU = 0.32 9.3/0.2/0.1/0.1 (BP = 1.000 ratio = 13.286 hyp_len = 279 ref_len = 21)\n","BLEU = 0.32 8.8/0.2/0.1/0.1 (BP = 1.000 ratio = 13.476 hyp_len = 283 ref_len = 21)\n","BLEU = 0.33 10.1/0.2/0.1/0.1 (BP = 1.000 ratio = 13.143 hyp_len = 276 ref_len = 21)\n","BLEU = 0.32 9.2/0.2/0.1/0.1 (BP = 1.000 ratio = 13.476 hyp_len = 283 ref_len = 21)\n","BLEU = 0.33 7.3/0.2/0.1/0.1 (BP = 1.000 ratio = 12.333 hyp_len = 259 ref_len = 21)\n","BLEU = 0.31 8.7/0.2/0.1/0.1 (BP = 1.000 ratio = 13.667 hyp_len = 287 ref_len = 21)\n","BLEU = 0.33 9.4/0.2/0.1/0.1 (BP = 1.000 ratio = 13.190 hyp_len = 277 ref_len = 21)\n","BLEU = 0.32 9.8/0.2/0.1/0.1 (BP = 1.000 ratio = 13.667 hyp_len = 287 ref_len = 21)\n","BLEU = 0.32 9.1/0.2/0.1/0.1 (BP = 1.000 ratio = 13.571 hyp_len = 285 ref_len = 21)\n","BLEU = 0.32 9.2/0.2/0.1/0.1 (BP = 1.000 ratio = 13.429 hyp_len = 282 ref_len = 21)\n","BLEU = 0.31 8.8/0.2/0.1/0.1 (BP = 1.000 ratio = 13.571 hyp_len = 285 ref_len = 21)\n","BLEU = 0.32 9.2/0.2/0.1/0.1 (BP = 1.000 ratio = 13.429 hyp_len = 282 ref_len = 21)\n","BLEU = 0.32 9.3/0.2/0.1/0.1 (BP = 1.000 ratio = 13.333 hyp_len = 280 ref_len = 21)\n","BLEU = 0.31 8.8/0.2/0.1/0.1 (BP = 1.000 ratio = 13.524 hyp_len = 284 ref_len = 21)\n","BLEU = 0.31 9.1/0.2/0.1/0.1 (BP = 1.000 ratio = 13.667 hyp_len = 287 ref_len = 21)\n","BLEU = 0.33 9.4/0.2/0.1/0.1 (BP = 1.000 ratio = 13.143 hyp_len = 276 ref_len = 21)\n","BLEU = 0.31 8.7/0.2/0.1/0.1 (BP = 1.000 ratio = 13.714 hyp_len = 288 ref_len = 21)\n","BLEU = 0.30 8.7/0.2/0.1/0.1 (BP = 1.000 ratio = 14.190 hyp_len = 298 ref_len = 21)\n","BLEU = 0.32 9.6/0.2/0.1/0.1 (BP = 1.000 ratio = 13.429 hyp_len = 282 ref_len = 21)\n"]}],"source":["test_iter = Multi30k(split='test', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n","print(compute_bleu(transformer, test_iter))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 5, \"temperature\": 1.0}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 5, \"temperature\": 0.5}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 5, \"temperature\": 0.1}))\n","\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_p, params={\"p\": 0.9, \"temperature\": 1.0}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_p, params={\"p\": 0.9, \"temperature\": 0.5}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_p, params={\"p\": 0.9, \"temperature\": 0.1}))\n","\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 10, \"temperature\": 1.0}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 10, \"temperature\": 0.5}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 10, \"temperature\": 0.1}))\n","\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_p, params={\"p\": 0.5, \"temperature\": 1.0}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_p, params={\"p\": 0.5, \"temperature\": 0.5}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_p, params={\"p\": 0.5, \"temperature\": 0.1}))\n","\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 50, \"temperature\": 1.0}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 50, \"temperature\": 0.5}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 50, \"temperature\": 0.1}))\n","\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 100, \"temperature\": 1.0}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 100, \"temperature\": 0.5}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 100, \"temperature\": 0.1}))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let's take the first BLEU score computation to see what the different parameters mean (greedy decoder):\n","\n","We obtain the following BLEU score: 0.32 9.3/0.2/0.1/0.1 (BP = 1.000 ratio = 13.286 hyp_len = 279 ref_len = 21)\n","Here is a description of the different scores:\n","* BLEU score: 0.32\n","* BLEU score for 1-grams: 9.3\n","* BLEU score for 2-grams: 0.2\n","* BLEU score for 3-grams: 0.1\n","* BLEU score for 4-grams: 0.1\n","* BP: 1.000 (Brevity Penalty)\n","* ratio: 13.286 (ratio of the length of the candidate translation to the length of the reference translation)\n","* hyp_len: 279 (total length of the candidate translation)\n","* ref_len: 21 (total length of the reference translation)"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
