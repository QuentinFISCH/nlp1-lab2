{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## PyTorch tutorial copy paste"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","from torchtext.datasets import multi30k, Multi30k\n","from typing import Iterable, List\n","\n","\n","# We need to modify the URLs for the dataset since the links to the original dataset are broken\n","# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n","multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n","multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n","\n","SRC_LANGUAGE = 'de'\n","TGT_LANGUAGE = 'en'\n","\n","# Place-holders\n","token_transform = {}\n","vocab_transform = {}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%pip install spacy sacrebleu torchdata -U\n","!python -m spacy download en_core_web_sm\n","!python -m spacy download de_core_news_sm"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n","token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n","\n","\n","# helper function to yield list of tokens\n","def yield_tokens(data_iter: Iterable, language: str) -> Iterable[str]:\n","    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n","\n","    for data_sample in data_iter:\n","        yield token_transform[language](data_sample[language_index[language]])\n","\n","# Define special symbols and indices\n","UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n","# Make sure the tokens are in order of their indices to properly insert them in vocab\n","special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n","\n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","    # Training data Iterator\n","    train_iter = list(Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE)))\n","    # Create torchtext's Vocab object\n","    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n","                                                    min_freq=1,\n","                                                    specials=special_symbols,\n","                                                    special_first=True)\n","\n","# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n","# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","  vocab_transform[ln].set_default_index(UNK_IDX)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from torch import Tensor\n","import torch\n","import torch.nn as nn\n","from torch.nn import Transformer\n","import math\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n","class PositionalEncoding(nn.Module):\n","    def __init__(self,\n","                 emb_size: int,\n","                 dropout: float,\n","                 maxlen: int = 5000):\n","        super(PositionalEncoding, self).__init__()\n","        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n","        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n","        pos_embedding = torch.zeros((maxlen, emb_size))\n","        pos_embedding[:, 0::2] = torch.sin(pos * den)\n","        pos_embedding[:, 1::2] = torch.cos(pos * den)\n","        pos_embedding = pos_embedding.unsqueeze(-2)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer('pos_embedding', pos_embedding)\n","\n","    def forward(self, token_embedding: Tensor):\n","        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n","\n","# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n","class TokenEmbedding(nn.Module):\n","    def __init__(self, vocab_size: int, emb_size):\n","        super(TokenEmbedding, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, emb_size)\n","        self.emb_size = emb_size\n","\n","    def forward(self, tokens: Tensor):\n","        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n","\n","# Seq2Seq Network\n","class Seq2SeqTransformer(nn.Module):\n","    def __init__(self,\n","                 num_encoder_layers: int,\n","                 num_decoder_layers: int,\n","                 emb_size: int,\n","                 nhead: int,\n","                 src_vocab_size: int,\n","                 tgt_vocab_size: int,\n","                 dim_feedforward: int = 512,\n","                 dropout: float = 0.1):\n","        super(Seq2SeqTransformer, self).__init__()\n","        self.transformer = Transformer(d_model=emb_size,\n","                                       nhead=nhead,\n","                                       num_encoder_layers=num_encoder_layers,\n","                                       num_decoder_layers=num_decoder_layers,\n","                                       dim_feedforward=dim_feedforward,\n","                                       dropout=dropout)\n","        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n","        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n","        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n","        self.positional_encoding = PositionalEncoding(\n","            emb_size, dropout=dropout)\n","\n","    def forward(self,\n","                src: Tensor,\n","                trg: Tensor,\n","                src_mask: Tensor,\n","                tgt_mask: Tensor,\n","                src_padding_mask: Tensor,\n","                tgt_padding_mask: Tensor,\n","                memory_key_padding_mask: Tensor):\n","        src_emb = self.positional_encoding(self.src_tok_emb(src))\n","        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n","        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n","                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n","        return self.generator(outs)\n","\n","    def encode(self, src: Tensor, src_mask: Tensor):\n","        return self.transformer.encoder(self.positional_encoding(\n","                            self.src_tok_emb(src)), src_mask)\n","\n","    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n","        return self.transformer.decoder(self.positional_encoding(\n","                          self.tgt_tok_emb(tgt)), memory,\n","                          tgt_mask)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def generate_square_subsequent_mask(sz):\n","    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n","    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","    return mask\n","\n","\n","def create_mask(src, tgt):\n","    src_seq_len = src.shape[0]\n","    tgt_seq_len = tgt.shape[0]\n","\n","    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n","    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n","\n","    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n","    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n","    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["torch.manual_seed(0)\n","\n","SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n","TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n","EMB_SIZE = 512\n","NHEAD = 8\n","FFN_HID_DIM = 512\n","BATCH_SIZE = 128\n","NUM_ENCODER_LAYERS = 3\n","NUM_DECODER_LAYERS = 3\n","\n","transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n","                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n","\n","for p in transformer.parameters():\n","    if p.dim() > 1:\n","        nn.init.xavier_uniform_(p)\n","\n","transformer = transformer.to(DEVICE)\n","\n","loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n","\n","optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["from torch.nn.utils.rnn import pad_sequence\n","\n","# helper function to club together sequential operations\n","def sequential_transforms(*transforms):\n","    def func(txt_input):\n","        for transform in transforms:\n","            txt_input = transform(txt_input)\n","        return txt_input\n","    return func\n","\n","# function to add BOS/EOS and create tensor for input sequence indices\n","def tensor_transform(token_ids: List[int]):\n","    return torch.cat((torch.tensor([BOS_IDX]),\n","                      torch.tensor(token_ids),\n","                      torch.tensor([EOS_IDX])))\n","\n","# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n","text_transform = {}\n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n","                                               vocab_transform[ln], #Numericalization\n","                                               tensor_transform) # Add BOS/EOS and create tensor\n","\n","\n","# function to collate data samples into batch tensors\n","def collate_fn(batch):\n","    src_batch, tgt_batch = [], []\n","    for src_sample, tgt_sample in batch:\n","        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n","        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n","\n","    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n","    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n","    return src_batch, tgt_batch"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","def train_epoch(model, optimizer):\n","    model.train()\n","    losses = 0\n","    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n","    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n","\n","    for src, tgt in train_dataloader:\n","        src = src.to(DEVICE)\n","        tgt = tgt.to(DEVICE)\n","\n","        tgt_input = tgt[:-1, :]\n","\n","        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n","\n","        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n","\n","        optimizer.zero_grad()\n","\n","        tgt_out = tgt[1:, :]\n","        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","        loss.backward()\n","\n","        optimizer.step()\n","        losses += loss.item()\n","\n","    return losses / len(list(train_dataloader))\n","\n","\n","def evaluate(model):\n","    model.eval()\n","    losses = 0\n","\n","    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n","    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n","\n","    for src, tgt in val_dataloader:\n","        src = src.to(DEVICE)\n","        tgt = tgt.to(DEVICE)\n","\n","        tgt_input = tgt[:-1, :]\n","\n","        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n","\n","        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n","\n","        tgt_out = tgt[1:, :]\n","        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","        losses += loss.item()\n","\n","    return losses / len(list(val_dataloader))"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","/Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n","  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1, Train loss: 5.342, Val loss: 4.107, Epoch time = 343.495s\n","Epoch: 2, Train loss: 3.760, Val loss: 3.308, Epoch time = 319.152s\n","Epoch: 3, Train loss: 3.156, Val loss: 2.893, Epoch time = 323.218s\n","Epoch: 4, Train loss: 2.765, Val loss: 2.631, Epoch time = 328.611s\n","Epoch: 5, Train loss: 2.478, Val loss: 2.436, Epoch time = 329.120s\n","Epoch: 6, Train loss: 2.249, Val loss: 2.300, Epoch time = 338.191s\n","Epoch: 7, Train loss: 2.057, Val loss: 2.189, Epoch time = 339.081s\n","Epoch: 8, Train loss: 1.893, Val loss: 2.123, Epoch time = 335.729s\n","Epoch: 9, Train loss: 1.754, Val loss: 2.052, Epoch time = 333.173s\n","Epoch: 10, Train loss: 1.628, Val loss: 2.005, Epoch time = 336.888s\n","Epoch: 11, Train loss: 1.519, Val loss: 1.975, Epoch time = 330.446s\n","Epoch: 12, Train loss: 1.417, Val loss: 1.956, Epoch time = 329.546s\n","Epoch: 13, Train loss: 1.331, Val loss: 1.965, Epoch time = 328.958s\n","Epoch: 14, Train loss: 1.249, Val loss: 1.960, Epoch time = 321.621s\n","Epoch: 15, Train loss: 1.171, Val loss: 1.915, Epoch time = 320.837s\n","Epoch: 16, Train loss: 1.100, Val loss: 1.916, Epoch time = 309.242s\n","Epoch: 17, Train loss: 1.036, Val loss: 1.913, Epoch time = 311.059s\n","Epoch: 18, Train loss: 0.976, Val loss: 1.922, Epoch time = 316.606s\n"]}],"source":["from timeit import default_timer as timer\n","NUM_EPOCHS = 18\n","\n","for epoch in range(1, NUM_EPOCHS+1):\n","    start_time = timer()\n","    train_loss = train_epoch(transformer, optimizer)\n","    end_time = timer()\n","    val_loss = evaluate(transformer)\n","    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save model\n","torch.save(transformer.state_dict(), 'model.pt')"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# load model.pt\n","transformer.load_state_dict(torch.load('model.pt'))"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# function to generate output sequence using greedy algorithm\n","def greedy_decode(model, src, src_mask, max_len, start_symbol):\n","    src = src.to(DEVICE)\n","    src_mask = src_mask.to(DEVICE)\n","\n","    memory = model.encode(src, src_mask)\n","    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n","    for i in range(max_len-1):\n","        memory = memory.to(DEVICE)\n","        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n","                    .type(torch.bool)).to(DEVICE)\n","        out = model.decode(ys, memory, tgt_mask)\n","        out = out.transpose(0, 1)\n","        prob = model.generator(out[:, -1])\n","        _, next_word = torch.max(prob, dim=1)\n","        next_word = next_word.item()\n","\n","        ys = torch.cat([ys,\n","                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n","        if next_word == EOS_IDX:\n","            break\n","    return ys\n","\n","\n","# actual function to translate input sentence into target language\n","def translate(model: torch.nn.Module, src_sentence: str):\n","    model.eval()\n","    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n","    num_tokens = src.shape[0]\n","    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n","    tgt_tokens = greedy_decode(\n","        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n","    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" A group of people stand in front of an igloo . \n"]}],"source":["print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Theoritical questions answer\n","\n","* In the positional encoding, why are we using a combination of sinus and cosinus?\n","* In the Seq2SeqTransformer class,\n","    * What is the parameter nhead for?\n","    * What is the point of the generator?\n","* Describe the goal of the create_mask function. Why does it handle differently the source and target masks?"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### In the positional encoding, why are we using a combination of sinus and cosinus?\n","\n","The positional encoding is a way to encode the position of the words in the sentence. We use sinus and cosinus to encode the position of the words in the sentence. These functions are bounded so the modification of the embedding is only a little, to not change the nature of it. We combine a sinus and a cosinus because they have different periods, which allows us to have a different encoding for each position, thus avoiding the problem that we could have two words with the same position because of the periodicity of the sinus and cosinus.\n","\n","#### In the Seq2SeqTransformer class, What is the parameter nhead for?\n","\n","The parameter nhead is the number of heads in the multihead attention. It is the number of parallel attention layers. It is used to increase the representational power of the model.\n","\n","#### In the Seq2SeqTransformer class, What is the point of the generator?\n","\n","The generator is a linear layer that takes the output of the decoder and returns the logits. It maps the output of the decoder to the vocabulary.\n","\n","#### Describe the goal of the create_mask function. Why does it handle differently the source and target masks?\n","\n","The goal of the create_mask function is to create a mask for the source and the target. The source mask is used to mask the padding tokens. The target mask is used to mask the padding tokens and the future tokens."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Decoding functions"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def top_k_sampling_with_temperature(model: torch.nn.Module,\n","                                    src: torch.Tensor,\n","                                    src_mask: torch.Tensor,\n","                                    max_len: int,\n","                                    start_symbol: int,\n","                                    k: int,\n","                                    temperature: float) -> torch.Tensor:\n","    \"\"\"\n","    Top K sampling algorithm with temperature\n","\n","    -------\n","    Args:\n","    model: torch.nn.Module\n","        Transformer model\n","    src: torch.Tensor\n","        Source tensor\n","    src_mask: torch.Tensor\n","        Source mask tensor\n","    max_len: int\n","        Maximum length of the output sequence\n","    start_symbol: int\n","        Start symbol\n","    k: int\n","        Top K\n","    temperature: float\n","        Temperature\n","\n","    -------\n","    Returns:\n","    ys: torch.Tensor\n","        Output tensor\n","    \"\"\"\n","    src = src.to(DEVICE)\n","    src_mask = src_mask.to(DEVICE)\n","\n","    memory = model.encode(src, src_mask)\n","    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n","    for i in range(max_len-1):\n","        memory = memory.to(DEVICE)\n","        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n","                    .type(torch.bool)).to(DEVICE)\n","        out = model.decode(ys, memory, tgt_mask)\n","        out = out.transpose(0, 1)\n","        prob = model.generator(out[:, -1])\n","        prob = torch.div(prob, temperature)\n","        prob = torch.softmax(prob, dim=1)\n","\n","        # Choose top k words from the probability distribution\n","        top_k_prob, top_k_idx = torch.topk(prob, k=k, dim=1)\n","\n","        # Choose next word from remaining words using multinomial distribution\n","        next_word = torch.multinomial(top_k_prob, num_samples=1)\n","        next_word = top_k_idx[torch.arange(top_k_idx.size(0)), next_word.squeeze(1)]\n","        next_word = next_word.item()\n","\n","        ys = torch.cat([ys,\n","                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n","        if next_word == EOS_IDX:\n","            break\n","    return ys\n","\n","def translate_top_k(model: torch.nn.Module, src_sentence: str, k: int, temperature: float) -> str:\n","    \"\"\"\n","    Translate source sentence using top k sampling algorithm with temperature\n","\n","    -------\n","    Args:\n","    model: torch.nn.Module\n","        Transformer model\n","    src_sentence: str\n","        Source sentence\n","    k: int\n","        Top K\n","    temperature: float\n","        Temperature\n","\n","    -------\n","    Returns:\n","    str\n","        The translated sentence\n","    \"\"\"\n","    model.eval()\n","    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n","    num_tokens = src.shape[0]\n","    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n","    tgt_tokens = top_k_sampling_with_temperature(\n","        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, k=k, temperature=temperature).flatten()\n","    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def top_p_sampling_with_temperature(model: torch.nn.Module,\n","                                    src: torch.Tensor,\n","                                    src_mask: torch.Tensor,\n","                                    max_len: int,\n","                                    start_symbol: int,\n","                                    p: int,\n","                                    temperature: float) -> torch.Tensor:\n","    \"\"\"\n","    Top P sampling algorithm with temperature\n","\n","    -------\n","    Args:\n","    model: torch.nn.Module\n","        Transformer model\n","    src: torch.Tensor\n","        Source tensor\n","    src_mask: torch.Tensor\n","        Source mask tensor\n","    max_len: int\n","        Maximum length of the output sequence\n","    start_symbol: int\n","        Start symbol\n","    p: int\n","        Top P\n","    temperature: float\n","        Temperature\n","\n","    -------\n","    Returns:\n","    ys: torch.Tensor\n","        Output tensor\n","    \"\"\"\n","    src = src.to(DEVICE)\n","    src_mask = src_mask.to(DEVICE)\n","\n","    memory = model.encode(src, src_mask)\n","    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n","    for i in range(max_len-1):\n","        memory = memory.to(DEVICE)\n","        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n","                    .type(torch.bool)).to(DEVICE)\n","        out = model.decode(ys, memory, tgt_mask)\n","        out = out.transpose(0, 1)\n","        prob = model.generator(out[:, -1])\n","        prob = torch.div(prob, temperature)\n","        prob = torch.softmax(prob, dim=1)\n","        sorted_prob, sorted_idx = torch.sort(prob, descending=True, dim=1)\n","\n","        # Calculate cumulative probabilities and remove tokens with cumulative probability above p\n","        cumulative_prob = torch.cumsum(sorted_prob, dim=1)\n","        sorted_idx_to_remove = cumulative_prob > p\n","        sorted_idx_to_remove[:, 1:] = sorted_idx_to_remove[:, :-1].clone()\n","        sorted_idx_to_remove[:, 0] = 0\n","        sorted_idx_to_remove = sorted_idx_to_remove.type(torch.bool)\n","        sorted_idx[sorted_idx_to_remove] = -1\n","        sorted_idx = sorted_idx[sorted_idx != -1]\n","        sorted_idx = sorted_idx.type(torch.float)\n","\n","        # Choose next word from remaining words using multinomial distribution\n","        next_word = torch.multinomial(sorted_idx, num_samples=1)\n","        next_word = sorted_idx[next_word]\n","        next_word = next_word.item()\n","\n","        ys = torch.cat([ys,\n","                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n","        if next_word == EOS_IDX:\n","            break\n","    return ys\n","\n","def translate_top_p(model: torch.nn.Module, src_sentence: str, p: int, temperature: float) -> str:\n","    \"\"\"\n","    Translate source sentence using top p sampling algorithm with temperature\n","\n","    -------\n","    Args:\n","    model: torch.nn.Module\n","        Transformer model\n","    src_sentence: str\n","        Source sentence\n","    p: int\n","        Top P\n","    temperature: float\n","        Temperature\n","\n","    -------\n","    Returns:\n","    str\n","        The translated sentence\n","    \"\"\"\n","    model.eval()\n","    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n","    num_tokens = src.shape[0]\n","    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n","    tgt_tokens = top_p_sampling_with_temperature(\n","        model, src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, p=p, temperature=temperature).flatten()\n","    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"]},{"cell_type":"code","execution_count":161,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Greedy translate:\n"," A group of people stand in front of an igloo . \n","\n","Top K sampling with temperature (k=5, temperature 1.0, 0.5 and 0.1):\n"," A group of people stand in front of an igloo \n"," A group of people stand in front of an igloo . \n"," A group of people stand in front of an igloo . \n","\n","Top P sampling with temperature (p=0.9, temperature 1.0, 0.5 and 0.1):\n"," A group of people standing in front an pledge Artists . \n"," A group of people stand in front of an overpass . \n"," A group of people stand in front of an igloo . \n","\n","Top K sampling with temperature (k=10, temperature 1.0, 0.5 and 0.1):\n"," A group of people stand in front of an igloo . \n"," A group of people stand in front of an igloo . \n"," A group of people stand in front of an igloo . \n","\n","Top P sampling with temperature (p=0.5, temperature 1.0, 0.5 and 0.1):\n"," A group of people stand in front of an Ohio calligraphy . \n"," A group of people stand in front of an igloo . \n"," A group of people stand in front of an igloo . \n","\n","Top K sampling with temperature (k=50, temperature 1.0, 0.5 and 0.1):\n"," A group of people stand in front of an auditorium . \n"," A group of people stand in front of an olive . \n"," A group of people standing in front of an igloo . \n","\n","Top K sampling with temperature (k=100, temperature 1.0, 0.5 and 0.1):\n"," A group of people stand in front of an unidentified room . \n"," A group of people stand in front of an igloo . \n"," A group of people stand in front of an igloo . \n"]}],"source":["print(\"Greedy translate:\")\n","print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))\n","\n","print(\"\\nTop K sampling with temperature (k=5, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=5, temperature=1.0))\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=5, temperature=0.5))\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=5, temperature=0.1))\n","\n","print(\"\\nTop P sampling with temperature (p=0.9, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p=0.9, temperature=1.0))\n","print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p=0.9, temperature=0.5))\n","print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p=0.9, temperature=0.1))\n","\n","print(\"\\nTop K sampling with temperature (k=10, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=10, temperature=1.0))\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=10, temperature=0.5))\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=10, temperature=0.1))\n","\n","print(\"\\nTop P sampling with temperature (p=0.5, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p=0.5, temperature=1.0))\n","print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p=0.5, temperature=0.5))\n","print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p=0.5, temperature=0.1))\n","\n","print(\"\\nTop K sampling with temperature (k=50, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=50, temperature=1.0))\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=50, temperature=0.5))\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=50, temperature=0.1))\n","\n","print(\"\\nTop K sampling with temperature (k=100, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=100, temperature=1.0))\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=100, temperature=0.5))\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=100, temperature=0.1))"]},{"cell_type":"code","execution_count":162,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Greedy translate:\n"," While the man in a kitchen is sitting in the living room . \n","\n","Top K sampling with temperature (k=5, temperature 1.0, 0.5 and 0.1):\n"," While sitting in the kitchen in a kitchen , the woman in a living room . \n"," The man in the kitchen is sitting in the living room . \n"," While sitting in the kitchen in a kitchen with the woman in her living room . \n","\n","Top P sampling with temperature (p=0.9, temperature 1.0, 0.5 and 0.1):\n"," Skiers in the kitchen in the kitchen sitting in a warehouse . \n"," While seated in the kitchen , the man sits in a living room . \n"," While sitting in the kitchen in a living room . \n","\n","Top K sampling with temperature (k=10, temperature 1.0, 0.5 and 0.1):\n"," The man in the kitchen is sitting in a room . \n"," A man in a kitchen is sitting in the living room . \n"," The man in the kitchen is sitting in the living room . \n","\n","Top P sampling with temperature (p=0.5, temperature 1.0, 0.5 and 0.1):\n"," While sitting in the kitchen in a living room . \n"," While sitting in the kitchen in a living room . \n"," While the man in a kitchen is sitting in the living room . \n","\n","Top K sampling with temperature (k=50, temperature 1.0, 0.5 and 0.1):\n"," The Man in the kitchen is sitting in a living room . \n"," While sitting in the kitchen in a kitchen with the woman in his living room . \n"," While the man in a kitchen is sitting in the living room . \n","\n","Top K sampling with temperature (k=100, temperature 1.0, 0.5 and 0.1):\n"," On the black kitchen , the man in a kitchen in the living room . \n"," While seated in the kitchen in a kitchen with the woman in his living room . \n"," The man in the kitchen is sitting in the living room . \n"]}],"source":["print(\"Greedy translate:\")\n","print(translate(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\"))\n","\n","print(\"\\nTop K sampling with temperature (k=5, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_k(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", k=5, temperature=1.0))\n","print(translate_top_k(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", k=5, temperature=0.5))\n","print(translate_top_k(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", k=5, temperature=0.1))\n","\n","print(\"\\nTop P sampling with temperature (p=0.9, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_p(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", p=0.9, temperature=1.0))\n","print(translate_top_p(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", p=0.9, temperature=0.5))\n","print(translate_top_p(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", p=0.9, temperature=0.1))\n","\n","print(\"\\nTop K sampling with temperature (k=10, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_k(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", k=10, temperature=1.0))\n","print(translate_top_k(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", k=10, temperature=0.5))\n","print(translate_top_k(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", k=10, temperature=0.1))\n","\n","print(\"\\nTop P sampling with temperature (p=0.5, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_p(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", p=0.5, temperature=1.0))\n","print(translate_top_p(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", p=0.5, temperature=0.5))\n","print(translate_top_p(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", p=0.5, temperature=0.1))\n","\n","print(\"\\nTop K sampling with temperature (k=50, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_k(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", k=50, temperature=1.0))\n","print(translate_top_k(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", k=50, temperature=0.5))\n","print(translate_top_k(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", k=50, temperature=0.1))\n","\n","print(\"\\nTop K sampling with temperature (k=100, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_k(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", k=100, temperature=1))\n","print(translate_top_k(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", k=10, temperature=0.5))\n","print(translate_top_k(transformer, \"Während der Mann in der Küche ist, sitzt die Frau im Wohnzimmer .\", k=100, temperature=0.1))"]},{"cell_type":"code","execution_count":163,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Greedy translate:\n"," The woman walks in the woods and picking up books . \n","\n","Top K sampling with temperature (k=5, temperature 1.0, 0.5 and 0.1):\n"," The woman is walking in the forest and gathering . \n"," The lady walks in the forest and picking up lunch . \n"," The woman walks in the woods and picking up books . \n","\n","Top P sampling with temperature (p=0.9, temperature 1.0, 0.5 and 0.1):\n"," The lady walks in the woods strums everywhere . \n"," The woman walks in the forest examining properly time . \n"," The woman walks in the forest and picking up books . \n","\n","Top K sampling with temperature (k=10, temperature 1.0, 0.5 and 0.1):\n"," The woman walks in the woods while picking up him . \n"," The woman is walking in the woods while picking time . \n"," The woman walks in the woods and picking up sale . \n","\n","Top P sampling with temperature (p=0.5, temperature 1.0, 0.5 and 0.1):\n"," The woman walks in the woods gestures and wonderful . \n"," The woman walks in the woods and picking up customers . \n"," The woman walks in the woods and picking up books . \n","\n","Top K sampling with temperature (k=50, temperature 1.0, 0.5 and 0.1):\n"," The woman walks in the forest as he picks books . \n"," The woman walks in the woods as if picking up . \n"," The woman walks in the woods and picking up books . \n","\n","Top K sampling with temperature (k=100, temperature 1.0, 0.5 and 0.1):\n"," The lady walks in the woods with steam and figures . \n"," The woman is walking in the forest and picking up books . \n"," The woman walks in the woods and picking up books . \n"]}],"source":["print(\"Greedy translate:\")\n","print(translate(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\"))\n","\n","print(\"\\nTop K sampling with temperature (k=5, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_k(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", k=5, temperature=1.0))\n","print(translate_top_k(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", k=5, temperature=0.5))\n","print(translate_top_k(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", k=5, temperature=0.1))\n","\n","print(\"\\nTop P sampling with temperature (p=0.9, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_p(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", p=0.9, temperature=1.0))\n","print(translate_top_p(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", p=0.9, temperature=0.5))\n","print(translate_top_p(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", p=0.9, temperature=0.1))\n","\n","print(\"\\nTop K sampling with temperature (k=10, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_k(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", k=10, temperature=1.0))\n","print(translate_top_k(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", k=10, temperature=0.5))\n","print(translate_top_k(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", k=10, temperature=0.1))\n","\n","print(\"\\nTop P sampling with temperature (p=0.5, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_p(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", p=0.5, temperature=1.0))\n","print(translate_top_p(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", p=0.5, temperature=0.5))\n","print(translate_top_p(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", p=0.5, temperature=0.1))\n","\n","print(\"\\nTop K sampling with temperature (k=50, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_k(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", k=50, temperature=1.0))\n","print(translate_top_k(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", k=50, temperature=0.5))\n","print(translate_top_k(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", k=50, temperature=0.1))\n","\n","print(\"\\nTop K sampling with temperature (k=100, temperature 1.0, 0.5 and 0.1):\")\n","print(translate_top_k(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", k=100, temperature=1))\n","print(translate_top_k(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", k=10, temperature=0.5))\n","print(translate_top_k(transformer, \"Die Frau geht in den Wald und sammelt Pilze .\", k=100, temperature=0.1))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["When looking at these 3 examples, we note some interesting behavior.\n","\n","The first example always gives the same translation for the main part of the sentence (\"A group of people standing in front an...\" or \"A group of people stand in front an...\", which have the same meaning in this case). However, the nomial group at the end of the sentence varies depending on the decoding method and parameters we use. Most of the time, the translation is correct and ends with \"igloo\", but we also observe funny endings such as \"Ohio calligraphy\". \n","\n","The other two examples are also interesting, because none of the configurations manage to translate the sentence correctly. In the second example, the \"woman\" seems to be an issue for the model. Everytime it manages to translate it and adds it to the sentence, the \"man\" at the beginning disappears and some words are duplicated. Other translations that don't include the \"woman\" are correct about the \"man\", but the end of the sentence doesn't make sense... The third example always give a pretty accurate translation of the first part of the sentence, but the end is always wrong. None of the configuration get \"mushrooms\" right.\n","\n","In general, it seems like a smaller temperature gives better results, but it's hard to see shapes of better success with k or p."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Compute the BLEU score of the model"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["from sacrebleu import corpus_bleu, BLEU\n","\n","def compute_bleu(model: torch.nn.Module,\n","                 data_loader: torch.utils.data.DataLoader,\n","                 translate_method=translate,\n","                 params = {}) -> BLEU:\n","    \"\"\"\n","    Compute the sentence-level BLEU score\n","\n","    -------\n","    Args:\n","    model: torch.nn.Module\n","        Transformer model\n","    data_loader: torch.utils.data.DataLoader\n","        DataLoader for the dataset\n","    translate_method: Callable\n","        Translation function\n","    params: dict\n","        Parameters for the translation function\n","\n","    -------\n","    Returns:\n","    BLEUScore\n","        Sentence-level BLEU score\n","    \"\"\"\n","    model.eval()\n","    refs = []\n","    preds = []\n","    with torch.no_grad():\n","        for src, tgt in data_loader:\n","            translated_sentence = translate_method(model, src, **params)\n","            refs.append(tgt)\n","            preds.append(translated_sentence)\n","    return corpus_bleu(preds, refs)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["BLEU = 0.32 9.3/0.2/0.1/0.1 (BP = 1.000 ratio = 13.286 hyp_len = 279 ref_len = 21)\n"]},{"name":"stderr","output_type":"stream","text":["/Users/quentinfisch/Documents/EPITA/ING2/SCIA/S8/NLP1/.venv/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n","  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"]},{"name":"stdout","output_type":"stream","text":["BLEU = 0.31 8.4/0.2/0.1/0.1 (BP = 1.000 ratio = 13.667 hyp_len = 287 ref_len = 21)\n","BLEU = 0.32 9.1/0.2/0.1/0.1 (BP = 1.000 ratio = 13.571 hyp_len = 285 ref_len = 21)\n","BLEU = 0.33 9.4/0.2/0.1/0.1 (BP = 1.000 ratio = 13.190 hyp_len = 277 ref_len = 21)\n"]}],"source":["test_iter = Multi30k(split='test', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n","print(compute_bleu(transformer, test_iter))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 5, \"temperature\": 1.0}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 5, \"temperature\": 0.5}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 5, \"temperature\": 0.1}))"]},{"cell_type":"code","execution_count":174,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["BLEU = 0.32 9.3/0.2/0.1/0.1 (BP = 1.000 ratio = 13.286 hyp_len = 279 ref_len = 21)\n","BLEU = 0.32 8.8/0.2/0.1/0.1 (BP = 1.000 ratio = 13.476 hyp_len = 283 ref_len = 21)\n","BLEU = 0.33 10.1/0.2/0.1/0.1 (BP = 1.000 ratio = 13.143 hyp_len = 276 ref_len = 21)\n","BLEU = 0.32 9.2/0.2/0.1/0.1 (BP = 1.000 ratio = 13.476 hyp_len = 283 ref_len = 21)\n","BLEU = 0.33 7.3/0.2/0.1/0.1 (BP = 1.000 ratio = 12.333 hyp_len = 259 ref_len = 21)\n","BLEU = 0.31 8.7/0.2/0.1/0.1 (BP = 1.000 ratio = 13.667 hyp_len = 287 ref_len = 21)\n","BLEU = 0.33 9.4/0.2/0.1/0.1 (BP = 1.000 ratio = 13.190 hyp_len = 277 ref_len = 21)\n","BLEU = 0.32 9.8/0.2/0.1/0.1 (BP = 1.000 ratio = 13.667 hyp_len = 287 ref_len = 21)\n","BLEU = 0.32 9.1/0.2/0.1/0.1 (BP = 1.000 ratio = 13.571 hyp_len = 285 ref_len = 21)\n","BLEU = 0.32 9.2/0.2/0.1/0.1 (BP = 1.000 ratio = 13.429 hyp_len = 282 ref_len = 21)\n","BLEU = 0.31 8.8/0.2/0.1/0.1 (BP = 1.000 ratio = 13.571 hyp_len = 285 ref_len = 21)\n","BLEU = 0.32 9.2/0.2/0.1/0.1 (BP = 1.000 ratio = 13.429 hyp_len = 282 ref_len = 21)\n","BLEU = 0.32 9.3/0.2/0.1/0.1 (BP = 1.000 ratio = 13.333 hyp_len = 280 ref_len = 21)\n","BLEU = 0.31 8.8/0.2/0.1/0.1 (BP = 1.000 ratio = 13.524 hyp_len = 284 ref_len = 21)\n","BLEU = 0.31 9.1/0.2/0.1/0.1 (BP = 1.000 ratio = 13.667 hyp_len = 287 ref_len = 21)\n","BLEU = 0.33 9.4/0.2/0.1/0.1 (BP = 1.000 ratio = 13.143 hyp_len = 276 ref_len = 21)\n","BLEU = 0.31 8.7/0.2/0.1/0.1 (BP = 1.000 ratio = 13.714 hyp_len = 288 ref_len = 21)\n","BLEU = 0.30 8.7/0.2/0.1/0.1 (BP = 1.000 ratio = 14.190 hyp_len = 298 ref_len = 21)\n","BLEU = 0.32 9.6/0.2/0.1/0.1 (BP = 1.000 ratio = 13.429 hyp_len = 282 ref_len = 21)\n"]}],"source":["test_iter = Multi30k(split='test', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n","print(compute_bleu(transformer, test_iter))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 5, \"temperature\": 1.0}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 5, \"temperature\": 0.5}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 5, \"temperature\": 0.1}))\n","\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_p, params={\"p\": 0.9, \"temperature\": 1.0}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_p, params={\"p\": 0.9, \"temperature\": 0.5}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_p, params={\"p\": 0.9, \"temperature\": 0.1}))\n","\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 10, \"temperature\": 1.0}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 10, \"temperature\": 0.5}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 10, \"temperature\": 0.1}))\n","\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_p, params={\"p\": 0.5, \"temperature\": 1.0}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_p, params={\"p\": 0.5, \"temperature\": 0.5}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_p, params={\"p\": 0.5, \"temperature\": 0.1}))\n","\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 50, \"temperature\": 1.0}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 50, \"temperature\": 0.5}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 50, \"temperature\": 0.1}))\n","\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 100, \"temperature\": 1.0}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 100, \"temperature\": 0.5}))\n","print(compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": 100, \"temperature\": 0.1}))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let's take the first BLEU score computation to see what the different parameters mean (greedy decoder):\n","\n","We obtain the following BLEU score: 0.32 9.3/0.2/0.1/0.1 (BP = 1.000 ratio = 13.286 hyp_len = 279 ref_len = 21)\n","Here is a description of the different scores:\n","* BLEU score: 0.32\n","* BLEU score for 1-grams: 9.3\n","* BLEU score for 2-grams: 0.2\n","* BLEU score for 3-grams: 0.1\n","* BLEU score for 4-grams: 0.1\n","* BP: 1.000 (Brevity Penalty)\n","* ratio: 13.286 (ratio of the length of the candidate translation to the length of the reference translation)\n","* hyp_len: 279 (total length of the candidate translation)\n","* ref_len: 21 (total length of the reference translation)\n","\n","\n","#### Use part of the test set to perform an hyperparameters search on the value of temperature, k, and p. Note that, normally, this should be done on a validation set, not the test set."]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["BLEU score: 0.26, temperature: 0.1, k: 5\n","BLEU score: 0.26, temperature: 0.1, k: 10\n","BLEU score: 0.26, temperature: 0.1, k: 50\n","BLEU score: 0.26, temperature: 0.1, k: 100\n","BLEU score: 0.26, temperature: 0.2, k: 5\n","BLEU score: 0.26, temperature: 0.2, k: 10\n","BLEU score: 0.25, temperature: 0.2, k: 50\n","BLEU score: 0.26, temperature: 0.2, k: 100\n","BLEU score: 0.26, temperature: 0.3, k: 5\n","BLEU score: 0.26, temperature: 0.3, k: 10\n","BLEU score: 0.25, temperature: 0.3, k: 50\n","BLEU score: 0.26, temperature: 0.3, k: 100\n","BLEU score: 0.25, temperature: 0.4, k: 5\n","BLEU score: 0.25, temperature: 0.4, k: 10\n","BLEU score: 0.26, temperature: 0.4, k: 50\n","BLEU score: 0.26, temperature: 0.4, k: 100\n","BLEU score: 0.25, temperature: 0.5, k: 5\n","BLEU score: 0.25, temperature: 0.5, k: 10\n","BLEU score: 0.25, temperature: 0.5, k: 50\n","BLEU score: 0.25, temperature: 0.5, k: 100\n","BLEU score: 0.25, temperature: 0.6, k: 5\n","BLEU score: 0.26, temperature: 0.6, k: 10\n","BLEU score: 0.24, temperature: 0.6, k: 50\n","BLEU score: 0.26, temperature: 0.6, k: 100\n","BLEU score: 0.24, temperature: 0.7, k: 5\n","BLEU score: 0.25, temperature: 0.7, k: 10\n","BLEU score: 0.25, temperature: 0.7, k: 50\n","BLEU score: 0.25, temperature: 0.7, k: 100\n","BLEU score: 0.26, temperature: 0.8, k: 5\n","BLEU score: 0.26, temperature: 0.8, k: 10\n","BLEU score: 0.26, temperature: 0.8, k: 50\n","BLEU score: 0.25, temperature: 0.8, k: 100\n","BLEU score: 0.24, temperature: 0.9, k: 5\n","BLEU score: 0.26, temperature: 0.9, k: 10\n","BLEU score: 0.26, temperature: 0.9, k: 50\n","BLEU score: 0.25, temperature: 0.9, k: 100\n","BLEU score: 0.26, temperature: 1.0, k: 5\n","BLEU score: 0.25, temperature: 1.0, k: 10\n","BLEU score: 0.25, temperature: 1.0, k: 50\n","BLEU score: 0.25, temperature: 1.0, k: 100\n","Best BLEU score: 0.26, temperature: 0.1, k: 5\n"]}],"source":["temperatures = [i / 10 for i in range(1, 11)]\n","ks = [5, 10, 50, 100]\n","\n","# use a small subset of the test set for hyperparameter search\n","test_iter = Multi30k(split='test', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n","test_iter = list(test_iter)[:100]\n","\n","best_bleu_score = 0\n","best_temperature = 0\n","best_k = 0\n","\n","for temperature in temperatures:\n","    for k in ks:\n","        bleu_score = compute_bleu(transformer, test_iter, translate_method=translate_top_k, params={\"k\": k, \"temperature\": temperature})\n","        bleu_score = float(str(bleu_score).split()[2])\n","        if bleu_score > best_bleu_score:\n","            best_bleu_score = bleu_score\n","            best_temperature = temperature\n","            best_k = k\n","        print(f\"BLEU score: {bleu_score}, temperature: {temperature}, k: {k}\")\n","\n","print(f\"Best BLEU score: {best_bleu_score}, temperature: {best_temperature}, k: {best_k}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["It looks like the best BLEU score (0.26) is obtained multiple times with different configurations, especially with temperature=0.1 and k=5\n","\n","Let's do the same for p"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["BLEU score: 0.26, temperature: 0.1, p: 0.8\n","BLEU score: 0.26, temperature: 0.1, p: 0.81\n","BLEU score: 0.26, temperature: 0.1, p: 0.82\n","BLEU score: 0.26, temperature: 0.1, p: 0.83\n","BLEU score: 0.26, temperature: 0.1, p: 0.84\n","BLEU score: 0.26, temperature: 0.1, p: 0.85\n","BLEU score: 0.26, temperature: 0.1, p: 0.86\n","BLEU score: 0.26, temperature: 0.1, p: 0.87\n","BLEU score: 0.26, temperature: 0.1, p: 0.88\n","BLEU score: 0.26, temperature: 0.1, p: 0.89\n","BLEU score: 0.26, temperature: 0.1, p: 0.9\n","BLEU score: 0.25, temperature: 0.1, p: 0.91\n","BLEU score: 0.26, temperature: 0.1, p: 0.92\n","BLEU score: 0.26, temperature: 0.1, p: 0.93\n","BLEU score: 0.25, temperature: 0.1, p: 0.94\n","BLEU score: 0.26, temperature: 0.1, p: 0.95\n","BLEU score: 0.25, temperature: 0.1, p: 0.96\n","BLEU score: 0.25, temperature: 0.1, p: 0.97\n","BLEU score: 0.26, temperature: 0.1, p: 0.98\n","BLEU score: 0.26, temperature: 0.1, p: 0.99\n","BLEU score: 0.26, temperature: 0.2, p: 0.8\n","BLEU score: 0.26, temperature: 0.2, p: 0.81\n","BLEU score: 0.26, temperature: 0.2, p: 0.82\n","BLEU score: 0.25, temperature: 0.2, p: 0.83\n","BLEU score: 0.26, temperature: 0.2, p: 0.84\n","BLEU score: 0.26, temperature: 0.2, p: 0.85\n","BLEU score: 0.26, temperature: 0.2, p: 0.86\n","BLEU score: 0.25, temperature: 0.2, p: 0.87\n","BLEU score: 0.25, temperature: 0.2, p: 0.88\n","BLEU score: 0.26, temperature: 0.2, p: 0.89\n","BLEU score: 0.25, temperature: 0.2, p: 0.9\n","BLEU score: 0.25, temperature: 0.2, p: 0.91\n","BLEU score: 0.26, temperature: 0.2, p: 0.92\n","BLEU score: 0.27, temperature: 0.2, p: 0.93\n","BLEU score: 0.25, temperature: 0.2, p: 0.94\n","BLEU score: 0.26, temperature: 0.2, p: 0.95\n","BLEU score: 0.26, temperature: 0.2, p: 0.96\n","BLEU score: 0.26, temperature: 0.2, p: 0.97\n","BLEU score: 0.25, temperature: 0.2, p: 0.98\n","BLEU score: 0.25, temperature: 0.2, p: 0.99\n","BLEU score: 0.26, temperature: 0.3, p: 0.8\n","BLEU score: 0.26, temperature: 0.3, p: 0.81\n","BLEU score: 0.25, temperature: 0.3, p: 0.82\n","BLEU score: 0.25, temperature: 0.3, p: 0.83\n","BLEU score: 0.25, temperature: 0.3, p: 0.84\n","BLEU score: 0.25, temperature: 0.3, p: 0.85\n","BLEU score: 0.26, temperature: 0.3, p: 0.86\n","BLEU score: 0.26, temperature: 0.3, p: 0.87\n","BLEU score: 0.26, temperature: 0.3, p: 0.88\n","BLEU score: 0.26, temperature: 0.3, p: 0.89\n","BLEU score: 0.26, temperature: 0.3, p: 0.9\n","BLEU score: 0.25, temperature: 0.3, p: 0.91\n","BLEU score: 0.26, temperature: 0.3, p: 0.92\n","BLEU score: 0.26, temperature: 0.3, p: 0.93\n","BLEU score: 0.26, temperature: 0.3, p: 0.94\n","BLEU score: 0.26, temperature: 0.3, p: 0.95\n","BLEU score: 0.25, temperature: 0.3, p: 0.96\n","BLEU score: 0.26, temperature: 0.3, p: 0.97\n","BLEU score: 0.25, temperature: 0.3, p: 0.98\n","BLEU score: 0.26, temperature: 0.3, p: 0.99\n","BLEU score: 0.26, temperature: 0.4, p: 0.8\n","BLEU score: 0.25, temperature: 0.4, p: 0.81\n","BLEU score: 0.25, temperature: 0.4, p: 0.82\n","BLEU score: 0.26, temperature: 0.4, p: 0.83\n","BLEU score: 0.26, temperature: 0.4, p: 0.84\n","BLEU score: 0.25, temperature: 0.4, p: 0.85\n","BLEU score: 0.26, temperature: 0.4, p: 0.86\n","BLEU score: 0.25, temperature: 0.4, p: 0.87\n","BLEU score: 0.25, temperature: 0.4, p: 0.88\n","BLEU score: 0.25, temperature: 0.4, p: 0.89\n","BLEU score: 0.26, temperature: 0.4, p: 0.9\n","BLEU score: 0.25, temperature: 0.4, p: 0.91\n","BLEU score: 0.25, temperature: 0.4, p: 0.92\n","BLEU score: 0.25, temperature: 0.4, p: 0.93\n","BLEU score: 0.26, temperature: 0.4, p: 0.94\n","BLEU score: 0.25, temperature: 0.4, p: 0.95\n","BLEU score: 0.26, temperature: 0.4, p: 0.96\n","BLEU score: 0.25, temperature: 0.4, p: 0.97\n","BLEU score: 0.26, temperature: 0.4, p: 0.98\n","BLEU score: 0.25, temperature: 0.4, p: 0.99\n","BLEU score: 0.26, temperature: 0.5, p: 0.8\n","BLEU score: 0.25, temperature: 0.5, p: 0.81\n","BLEU score: 0.25, temperature: 0.5, p: 0.82\n","BLEU score: 0.25, temperature: 0.5, p: 0.83\n","BLEU score: 0.25, temperature: 0.5, p: 0.84\n","BLEU score: 0.25, temperature: 0.5, p: 0.85\n","BLEU score: 0.25, temperature: 0.5, p: 0.86\n","BLEU score: 0.26, temperature: 0.5, p: 0.87\n","BLEU score: 0.25, temperature: 0.5, p: 0.88\n","BLEU score: 0.25, temperature: 0.5, p: 0.89\n","BLEU score: 0.26, temperature: 0.5, p: 0.9\n","BLEU score: 0.25, temperature: 0.5, p: 0.91\n","BLEU score: 0.25, temperature: 0.5, p: 0.92\n","BLEU score: 0.25, temperature: 0.5, p: 0.93\n","BLEU score: 0.26, temperature: 0.5, p: 0.94\n","BLEU score: 0.26, temperature: 0.5, p: 0.95\n","BLEU score: 0.26, temperature: 0.5, p: 0.96\n","BLEU score: 0.24, temperature: 0.5, p: 0.97\n","BLEU score: 0.26, temperature: 0.5, p: 0.98\n","BLEU score: 0.26, temperature: 0.5, p: 0.99\n","BLEU score: 0.26, temperature: 0.6, p: 0.8\n","BLEU score: 0.25, temperature: 0.6, p: 0.81\n","BLEU score: 0.26, temperature: 0.6, p: 0.82\n","BLEU score: 0.25, temperature: 0.6, p: 0.83\n","BLEU score: 0.25, temperature: 0.6, p: 0.84\n","BLEU score: 0.26, temperature: 0.6, p: 0.85\n","BLEU score: 0.25, temperature: 0.6, p: 0.86\n","BLEU score: 0.25, temperature: 0.6, p: 0.87\n","BLEU score: 0.25, temperature: 0.6, p: 0.88\n","BLEU score: 0.25, temperature: 0.6, p: 0.89\n","BLEU score: 0.26, temperature: 0.6, p: 0.9\n","BLEU score: 0.25, temperature: 0.6, p: 0.91\n","BLEU score: 0.26, temperature: 0.6, p: 0.92\n","BLEU score: 0.27, temperature: 0.6, p: 0.93\n","BLEU score: 0.26, temperature: 0.6, p: 0.94\n","BLEU score: 0.25, temperature: 0.6, p: 0.95\n","BLEU score: 0.26, temperature: 0.6, p: 0.96\n","BLEU score: 0.24, temperature: 0.6, p: 0.97\n","BLEU score: 0.26, temperature: 0.6, p: 0.98\n","BLEU score: 0.25, temperature: 0.6, p: 0.99\n","BLEU score: 0.26, temperature: 0.7, p: 0.8\n","BLEU score: 0.25, temperature: 0.7, p: 0.81\n","BLEU score: 0.26, temperature: 0.7, p: 0.82\n","BLEU score: 0.26, temperature: 0.7, p: 0.83\n","BLEU score: 0.26, temperature: 0.7, p: 0.84\n","BLEU score: 0.25, temperature: 0.7, p: 0.85\n","BLEU score: 0.26, temperature: 0.7, p: 0.86\n","BLEU score: 0.28, temperature: 0.7, p: 0.87\n","BLEU score: 0.26, temperature: 0.7, p: 0.88\n","BLEU score: 0.26, temperature: 0.7, p: 0.89\n","BLEU score: 0.26, temperature: 0.7, p: 0.9\n","BLEU score: 0.25, temperature: 0.7, p: 0.91\n","BLEU score: 0.26, temperature: 0.7, p: 0.92\n","BLEU score: 0.26, temperature: 0.7, p: 0.93\n","BLEU score: 0.25, temperature: 0.7, p: 0.94\n","BLEU score: 0.25, temperature: 0.7, p: 0.95\n","BLEU score: 0.25, temperature: 0.7, p: 0.96\n","BLEU score: 0.25, temperature: 0.7, p: 0.97\n","BLEU score: 0.25, temperature: 0.7, p: 0.98\n","BLEU score: 0.25, temperature: 0.7, p: 0.99\n","BLEU score: 0.26, temperature: 0.8, p: 0.8\n","BLEU score: 0.27, temperature: 0.8, p: 0.81\n","BLEU score: 0.26, temperature: 0.8, p: 0.82\n","BLEU score: 0.26, temperature: 0.8, p: 0.83\n","BLEU score: 0.26, temperature: 0.8, p: 0.84\n","BLEU score: 0.27, temperature: 0.8, p: 0.85\n","BLEU score: 0.25, temperature: 0.8, p: 0.86\n","BLEU score: 0.26, temperature: 0.8, p: 0.87\n","BLEU score: 0.26, temperature: 0.8, p: 0.88\n","BLEU score: 0.26, temperature: 0.8, p: 0.89\n","BLEU score: 0.26, temperature: 0.8, p: 0.9\n","BLEU score: 0.25, temperature: 0.8, p: 0.91\n","BLEU score: 0.25, temperature: 0.8, p: 0.92\n","BLEU score: 0.25, temperature: 0.8, p: 0.93\n","BLEU score: 0.25, temperature: 0.8, p: 0.94\n","BLEU score: 0.27, temperature: 0.8, p: 0.95\n","BLEU score: 0.26, temperature: 0.8, p: 0.96\n","BLEU score: 0.26, temperature: 0.8, p: 0.97\n","BLEU score: 0.24, temperature: 0.8, p: 0.98\n","BLEU score: 0.24, temperature: 0.8, p: 0.99\n","BLEU score: 0.27, temperature: 0.9, p: 0.8\n","BLEU score: 0.26, temperature: 0.9, p: 0.81\n","BLEU score: 0.26, temperature: 0.9, p: 0.82\n","BLEU score: 0.26, temperature: 0.9, p: 0.83\n","BLEU score: 0.26, temperature: 0.9, p: 0.84\n","BLEU score: 0.26, temperature: 0.9, p: 0.85\n","BLEU score: 0.25, temperature: 0.9, p: 0.86\n","BLEU score: 0.27, temperature: 0.9, p: 0.87\n","BLEU score: 0.26, temperature: 0.9, p: 0.88\n","BLEU score: 0.26, temperature: 0.9, p: 0.89\n","BLEU score: 0.26, temperature: 0.9, p: 0.9\n","BLEU score: 0.27, temperature: 0.9, p: 0.91\n","BLEU score: 0.27, temperature: 0.9, p: 0.92\n","BLEU score: 0.25, temperature: 0.9, p: 0.93\n","BLEU score: 0.26, temperature: 0.9, p: 0.94\n","BLEU score: 0.24, temperature: 0.9, p: 0.95\n","BLEU score: 0.25, temperature: 0.9, p: 0.96\n","BLEU score: 0.24, temperature: 0.9, p: 0.97\n","BLEU score: 0.25, temperature: 0.9, p: 0.98\n","BLEU score: 0.23, temperature: 0.9, p: 0.99\n","BLEU score: 0.27, temperature: 1.0, p: 0.8\n","BLEU score: 0.26, temperature: 1.0, p: 0.81\n","BLEU score: 0.25, temperature: 1.0, p: 0.82\n","BLEU score: 0.25, temperature: 1.0, p: 0.83\n","BLEU score: 0.26, temperature: 1.0, p: 0.84\n","BLEU score: 0.26, temperature: 1.0, p: 0.85\n","BLEU score: 0.25, temperature: 1.0, p: 0.86\n","BLEU score: 0.26, temperature: 1.0, p: 0.87\n","BLEU score: 0.24, temperature: 1.0, p: 0.88\n","BLEU score: 0.26, temperature: 1.0, p: 0.89\n","BLEU score: 0.24, temperature: 1.0, p: 0.9\n","BLEU score: 0.25, temperature: 1.0, p: 0.91\n","BLEU score: 0.26, temperature: 1.0, p: 0.92\n","BLEU score: 0.24, temperature: 1.0, p: 0.93\n","BLEU score: 0.24, temperature: 1.0, p: 0.94\n","BLEU score: 0.26, temperature: 1.0, p: 0.95\n","BLEU score: 0.24, temperature: 1.0, p: 0.96\n","BLEU score: 0.23, temperature: 1.0, p: 0.97\n","BLEU score: 0.22, temperature: 1.0, p: 0.98\n","BLEU score: 0.21, temperature: 1.0, p: 0.99\n","Best BLEU score: 0.28, temperature: 0.7, p: 0.87\n"]}],"source":["ps = [i / 100 for i in range(80, 100)]\n","\n","best_bleu_score = 0\n","best_temperature = 0\n","best_p = 0\n","\n","for temperature in temperatures:\n","    for p in ps:\n","        bleu_score = compute_bleu(transformer, test_iter, translate_method=translate_top_p, params={\"p\": p, \"temperature\": temperature})\n","        bleu_score = float(str(bleu_score).split()[2])\n","        if bleu_score > best_bleu_score:\n","            best_bleu_score = bleu_score\n","            best_temperature = temperature\n","            best_p = p\n","        print(f\"BLEU score: {bleu_score}, temperature: {temperature}, p: {p}\")\n","\n","print(f\"Best BLEU score: {best_bleu_score}, temperature: {best_temperature}, p: {best_p}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["For the top P decoding, the best BLEU score we have is 0.28 with a temperature of 0.7 and a p of 0.87"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
